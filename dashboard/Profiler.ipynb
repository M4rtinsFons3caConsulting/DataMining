{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a090b2-c600-4b59-9459-ec3f46c2dddd",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Imports for this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3f85e7a0-f4af-4a45-b8ea-3f04aaf453b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# PROCESSING\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# MODELS\n",
    "from gower import gower_matrix\n",
    "from scipy.linalg import svd\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0b59c8-be03-4d56-98e1-c2f3c8e0d2ee",
   "metadata": {},
   "source": [
    "### Globals\n",
    "\n",
    "List of global variables used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a5d7f495-1526-47cf-ad02-693e0f4663c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTROL\n",
    "DATA = pd.read_csv('data/regulars.csv')\n",
    "\n",
    "# PROCESSING PICKLES\n",
    "HOT_ENCODER = joblib.load('models/hot_encoder.pkl')\n",
    "SCALER = joblib.load('../dashboard/models/std_scaler.pkl')\n",
    "MINMAXSCALER = joblib.load('../dashboard/models/minmax_scaler.pkl')\n",
    "PCA = joblib.load('../dashboard/models/pca_components.pkl')\n",
    "\n",
    "# INTERMEDIATE MODEL PICKLES\n",
    "MINISOM = joblib.load('../dashboard/models/minisom.pkl')\n",
    "NMF = joblib.load('../dashboard/models/nmf.pkl')\n",
    "SPCA = joblib.load('../dashboard/models/spca.pkl')\n",
    "\n",
    "# MODEL PICKLES\n",
    "SPENDING = joblib.load('../dashboard/models/spending_clustering.pkl')\n",
    "# CITY = joblib.load('../dashboard/models/city_clustering.pkl') implemented manually\n",
    "CUISINE = joblib.load('../dashboard/models/cuisine_clustering.pkl')\n",
    "TIME = joblib.load('../dashboard/models/hour_clustering.pkl')\n",
    "\n",
    "CUISINE_CLUSTER_TO_CITY_DICT = {\n",
    "    0 : 4,\n",
    "    1 : 8,\n",
    "    2 : 2\n",
    "}\n",
    "\n",
    "# VARIABLE LISTS\n",
    "FULl_VAR_LIST = \\\n",
    "[\n",
    "    'customer_id',\n",
    "    'cust_age',\n",
    "    'n_vendor',\n",
    "    'n_product',\n",
    "    'n_chain',\n",
    "    'first_order',\n",
    "    'last_order',\n",
    "    'american',\n",
    "    'asian',\n",
    "    'beverages',\n",
    "    'cafe',\n",
    "    'chicken_dishes',\n",
    "    'chinese',\n",
    "    'desserts',\n",
    "    'healthy',\n",
    "    'indian',\n",
    "    'italian',\n",
    "    'japanese',\n",
    "    'noodle_dishes',\n",
    "    'other',\n",
    "    'street_food_snacks',\n",
    "    'thai',\n",
    "    'DOW_0',\n",
    "    'DOW_1',\n",
    "    'DOW_2',\n",
    "    'DOW_3',\n",
    "    'DOW_4',\n",
    "    'DOW_5',\n",
    "    'DOW_6',\n",
    "    'HR_0',\n",
    "    'HR_1',\n",
    "    'HR_2',\n",
    "    'HR_3',\n",
    "    'HR_4',\n",
    "    'HR_5',\n",
    "    'HR_6',\n",
    "    'HR_7',\n",
    "    'HR_8',\n",
    "    'HR_9',\n",
    "    'HR_10',\n",
    "    'HR_11',\n",
    "    'HR_12',\n",
    "    'HR_13',\n",
    "    'HR_14',\n",
    "    'HR_15',\n",
    "    'HR_16',\n",
    "    'HR_17',\n",
    "    'HR_18',\n",
    "    'HR_19',\n",
    "    'HR_20',\n",
    "    'HR_21',\n",
    "    'HR_22',\n",
    "    'HR_23',\n",
    "    'total_amt',\n",
    "    'n_order',\n",
    "    'avg_amt_per_product',\n",
    "    'avg_amt_per_order',\n",
    "    'avg_amt_per_vendor',\n",
    "    'days_cust',\n",
    "    'avg_days_to_order',\n",
    "    'days_due',\n",
    "    'per_chain_order',\n",
    "    'n_days_week',\n",
    "    'n_times_day',\n",
    "    'n_cuisines',\n",
    "    'log_n_vendor',\n",
    "    'log_n_product',\n",
    "    'log_n_chain',\n",
    "    'log_american',\n",
    "    'log_asian',\n",
    "    'log_beverages',\n",
    "    'log_cafe',\n",
    "    'log_chicken_dishes',\n",
    "    'log_chinese',\n",
    "    'log_desserts',\n",
    "    'log_healthy',\n",
    "    'log_indian',\n",
    "    'log_italian',\n",
    "    'log_japanese',\n",
    "    'log_noodle_dishes',\n",
    "    'log_other',\n",
    "    'log_street_food_snacks',\n",
    "    'log_thai',\n",
    "    'log_total_amt',\n",
    "    'log_n_order',\n",
    "    'log_avg_amt_per_product',\n",
    "    'log_avg_amt_per_order',\n",
    "    'log_avg_amt_per_vendor',\n",
    "    'log_n_days_week',\n",
    "    'log_n_times_day',\n",
    "    'avg_amt_per_day',\n",
    "    'avg_product_per_day',\n",
    "    'avg_order_per_day',\n",
    "    'cust_region_2360.0',\n",
    "    'cust_region_2400.0',\n",
    "    'cust_region_4140.0',\n",
    "    'cust_region_4660.0',\n",
    "    'cust_region_8370.0',\n",
    "    'cust_region_8550.0',\n",
    "    'cust_region_8670.0',\n",
    "    'last_promo_DELIVERY',\n",
    "    'last_promo_DISCOUNT',\n",
    "    'last_promo_FREEBIE',\n",
    "    'last_promo_NO_PROMO',\n",
    "    'pay_method_CARD',\n",
    "    'pay_method_CASH',\n",
    "    'pay_method_DIGI',\n",
    "    'cust_city_2.0',\n",
    "    'cust_city_4.0',\n",
    "    'cust_city_8.0',\n",
    "    'age_bucket_15-24',\n",
    "    'age_bucket_25-34',\n",
    "    'age_bucket_35-44',\n",
    "    'age_bucket_45-54',\n",
    "    'age_bucket_55-64',\n",
    "    'age_bucket_65+',\n",
    "    'regular',\n",
    "    'foodie_flag',\n",
    "    'gluttonous_flag',\n",
    "    'loyal_flag',\n",
    "    'transaction_volume',\n",
    "    'interaction_rate'\n",
    "]\n",
    "\n",
    "OG_LIST = [\n",
    "    'cust_region',\n",
    "    'last_promo',\n",
    "    'pay_method',\n",
    "    'cust_age',   \n",
    "    'n_vendor',\n",
    "    'n_product',\n",
    "    'n_chain',\n",
    "    'first_order',\n",
    "    'last_order',\n",
    "    'american',\n",
    "    'asian',\n",
    "    'beverages',\n",
    "    'cafe',\n",
    "    'chicken_dishes',\n",
    "    'chinese',\n",
    "    'desserts',\n",
    "    'healthy',\n",
    "    'indian',\n",
    "    'italian',\n",
    "    'japanese',\n",
    "    'noodle_dishes',\n",
    "    'other',\n",
    "    'street_food_snacks',\n",
    "    'thai',\n",
    "    'DOW_0',\n",
    "    'DOW_1',\n",
    "    'DOW_2',\n",
    "    'DOW_3',\n",
    "    'DOW_4',\n",
    "    'DOW_5',\n",
    "    'DOW_6',\n",
    "    'HR_0',\n",
    "    'HR_1',\n",
    "    'HR_2',\n",
    "    'HR_3',\n",
    "    'HR_4',\n",
    "    'HR_5',\n",
    "    'HR_6',\n",
    "    'HR_7',\n",
    "    'HR_8',\n",
    "    'HR_9',\n",
    "    'HR_10',\n",
    "    'HR_11',\n",
    "    'HR_12',\n",
    "    'HR_13',\n",
    "    'HR_14',\n",
    "    'HR_15',\n",
    "    'HR_16',\n",
    "    'HR_17',\n",
    "    'HR_18',\n",
    "    'HR_19',\n",
    "    'HR_20',\n",
    "    'HR_21',\n",
    "    'HR_22',\n",
    "    'HR_23'   \n",
    "]\n",
    "\n",
    "NON_METRIC_KEYS = OG_LIST[:3]  \n",
    "METRIC_KEYS = OG_LIST[3:]      \n",
    "CUISINE_KEYS = OG_LIST[9:22]\n",
    "\n",
    "LOGS = \\\n",
    "[\n",
    "   'n_vendor',\n",
    "    'n_product',\n",
    "    'n_chain',\n",
    "    'american',\n",
    "    'asian',\n",
    "    'beverages',\n",
    "    'cafe',\n",
    "    'chicken_dishes',\n",
    "    'chinese',\n",
    "    'desserts',\n",
    "    'healthy',\n",
    "    'indian',\n",
    "    'italian',\n",
    "    'japanese',\n",
    "    'noodle_dishes',\n",
    "    'other',\n",
    "    'street_food_snacks',\n",
    "    'thai',\n",
    "    'total_amt',\n",
    "    'n_order',\n",
    "    'avg_amt_per_product',\n",
    "    'avg_amt_per_order',\n",
    "    'avg_amt_per_vendor',\n",
    "    'n_days_week',\n",
    "    'n_times_day'\n",
    "]\n",
    "\n",
    "PCA_FEATURES = \\\n",
    "[\n",
    "    'avg_amt_per_day', \n",
    "    'avg_product_per_day', \n",
    "    'avg_order_per_day', \n",
    "    'n_product', \n",
    "    'n_order'\n",
    "]\n",
    "\n",
    "SCALER_FEATURES = [\n",
    "    'cust_age', 'n_vendor', 'n_product', 'n_chain', 'first_order', 'last_order', \n",
    "    'american', 'asian', 'beverages', 'cafe', 'chicken_dishes', 'chinese', \n",
    "    'desserts', 'healthy', 'indian', 'italian', 'japanese', 'noodle_dishes', \n",
    "    'other', 'street_food_snacks', 'thai',\n",
    "    'DOW_0', 'DOW_1', 'DOW_2', 'DOW_3', 'DOW_4', 'DOW_5', 'DOW_6', \n",
    "    'HR_0', 'HR_1', 'HR_2', 'HR_3', 'HR_4', 'HR_5', 'HR_6', 'HR_7', 'HR_8', \n",
    "    'HR_9', 'HR_10', 'HR_11', 'HR_12', 'HR_13', 'HR_14', 'HR_15', 'HR_16', \n",
    "    'HR_17', 'HR_18', 'HR_19', 'HR_20', 'HR_21', 'HR_22', 'HR_23',\n",
    "    'total_amt', 'n_order', 'avg_amt_per_product', 'avg_amt_per_order', \n",
    "    'avg_amt_per_vendor', 'days_cust', 'avg_days_to_order', 'days_due', \n",
    "    'per_chain_order', 'n_days_week', 'n_times_day', 'n_cuisines', \n",
    "    'log_n_vendor', 'log_n_product', 'log_n_chain', 'log_american', \n",
    "    'log_asian', 'log_beverages', 'log_cafe', 'log_chicken_dishes', \n",
    "    'log_chinese', 'log_desserts', 'log_healthy', 'log_indian', 'log_italian', \n",
    "    'log_japanese', 'log_noodle_dishes', 'log_other', 'log_street_food_snacks', \n",
    "    'log_thai', 'log_total_amt', 'log_n_order', 'log_avg_amt_per_product', \n",
    "    'log_avg_amt_per_order', 'log_avg_amt_per_vendor', 'log_n_days_week', \n",
    "    'log_n_times_day', 'avg_amt_per_day', 'avg_product_per_day', \n",
    "    'avg_order_per_day'\n",
    "]\n",
    "\n",
    "# FEATURE DICTIONARIES\n",
    "MEANS = \\\n",
    "{\n",
    "    'cust_age': 27.505,\n",
    "    'first_order': 23.081,\n",
    "    'last_order': 68.927,\n",
    "}\n",
    "\n",
    "TIME_LIKELYHOODS = \\\n",
    "{ \n",
    "    'DAY':\n",
    "    {\n",
    "        'DOW_0': 0.638,\n",
    "        'DOW_1': 0.65,\n",
    "        'DOW_2': 0.679,\n",
    "        'DOW_3': 0.71,\n",
    "        'DOW_4': 0.777,\n",
    "        'DOW_5': 0.746,\n",
    "        'DOW_6': 0.808        \n",
    "    },\n",
    "\n",
    "    'HOUR':\n",
    "    {\n",
    "        'HR_0': 0.053,\n",
    "        'HR_1': 0.06,\n",
    "        'HR_2': 0.07,\n",
    "        'HR_3': 0.136,\n",
    "        'HR_4': 0.114,\n",
    "        'HR_5': 0.094,\n",
    "        'HR_6': 0.078,\n",
    "        'HR_7': 0.084,\n",
    "        'HR_8': 0.142,\n",
    "        'HR_9': 0.263,\n",
    "        'HR_10': 0.374,\n",
    "        'HR_11': 0.436,\n",
    "        'HR_12': 0.369,\n",
    "        'HR_13': 0.276,\n",
    "        'HR_14': 0.247,\n",
    "        'HR_15': 0.318,\n",
    "        'HR_16': 0.414,\n",
    "        'HR_17': 0.452,\n",
    "        'HR_18': 0.391,\n",
    "        'HR_19': 0.287,\n",
    "        'HR_20': 0.166,\n",
    "        'HR_21': 0.083,\n",
    "        'HR_22': 0.053,\n",
    "        'HR_23': 0.051   \n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize dictionaries for feature groups with flags and relevant columns\n",
    "FEATURE_GROUPS = {\n",
    "    'foodie': ['n_vendor', 'n_product', 'n_order', 'n_cuisines'],\n",
    "    'gluttonous': ['avg_amt_per_order', 'total_amt', 'n_chain'],\n",
    "    'loyal': ['avg_amt_per_vendor'] + CUISINE_KEYS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "8aadbaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = {\n",
    "    'cust_region': 'Region',\n",
    "    'cust_age': 'Age',\n",
    "    'n_vendor': 'Vendor Count',\n",
    "    'n_product': 'Product Count',\n",
    "    'n_chain': 'Chain Restaurant Order Count',\n",
    "    'first_order': 'First Order Date',\n",
    "    'last_order': 'Last Order Date',\n",
    "    'last_promo': 'Promotion',\n",
    "    'pay_method': 'Payment Method',\n",
    "    'american': 'American',\n",
    "    'asian': 'Asian',\n",
    "    'beverages': 'Beverages',\n",
    "    'cafe': 'Cafe',\n",
    "    'chicken_dishes': 'Chicken Dishes',\n",
    "    'chinese': 'Chinese',\n",
    "    'desserts': 'Desserts',\n",
    "    'healthy': 'Healthy',\n",
    "    'indian': 'Indian',\n",
    "    'italian': 'Italian',\n",
    "    'japanese': 'Japanese',\n",
    "    'noodle_dishes': 'Noodle Dishes',\n",
    "    'other': 'Other Cuisines',\n",
    "    'street_food_snacks': 'Street Food & Snacks',\n",
    "    'thai': 'Thai',\n",
    "    'DOW_0': 'Sunday',\n",
    "    'DOW_1': 'Monday',\n",
    "    'DOW_2': 'Tuesday',\n",
    "    'DOW_3': 'Wednesday',\n",
    "    'DOW_4': 'Thursday',\n",
    "    'DOW_5': 'Friday',\n",
    "    'DOW_6': 'Saturday',\n",
    "    'HR_0': '12AM',\n",
    "    'HR_1': '1AM',\n",
    "    'HR_2': '2AM',\n",
    "    'HR_3': '3AM',\n",
    "    'HR_4': '4AM',\n",
    "    'HR_5': '5AM',\n",
    "    'HR_6': '6AM',\n",
    "    'HR_7': '7AM',\n",
    "    'HR_8': '8AM',\n",
    "    'HR_9': '9AM',\n",
    "    'HR_10': '10AM',\n",
    "    'HR_11': '11AM',\n",
    "    'HR_12': '12PM',\n",
    "    'HR_13': '1PM',\n",
    "    'HR_14': '2PM',\n",
    "    'HR_15': '3PM',\n",
    "    'HR_16': '4PM',\n",
    "    'HR_17': '5PM',\n",
    "    'HR_18': '6PM',\n",
    "    'HR_19': '7PM',\n",
    "    'HR_20': '8PM',\n",
    "    'HR_21': '9PM',\n",
    "    'HR_22': '10PM',\n",
    "    'HR_23': '11PM',\n",
    "    'cust_city': 'City',\n",
    "    'total_amt': 'Total Amount',\n",
    "    'n_order': 'Order Count',\n",
    "    'avg_amt_per_product': 'Avg Amount per Product',\n",
    "    'avg_amt_per_order': 'Avg Amount per Order',\n",
    "    'avg_amt_per_vendor': 'Avg Amount per Vendor',\n",
    "    'days_cust': 'Days as Customer',\n",
    "    'avg_days_to_order': 'Avg Days to Order',\n",
    "    'days_due': 'Order Days Due',\n",
    "    'per_chain_order': '% Orders in Chain Restaurant',\n",
    "    'n_days_week': 'Days of Week Ordered Count',\n",
    "    'n_times_day': 'Hours Ordered Count',\n",
    "    'regular': 'Is Regular',\n",
    "    'n_cuisines': 'Cuisines Count',\n",
    "    'log_n_vendor': 'Log Vendor Count',\n",
    "    'log_n_product': 'Log Product Count',\n",
    "    'log_n_chain': 'Log Chain Restaurant Order Count',\n",
    "    'log_american': 'Log American',\n",
    "    'log_asian': 'Log Asian',\n",
    "    'log_beverages': 'Log Beverages',\n",
    "    'log_cafe': 'Log Cafe',\n",
    "    'log_chicken_dishes': 'Log Chicken Dishes',\n",
    "    'log_chinese': 'Log Chinese',\n",
    "    'log_desserts': 'Log Desserts',\n",
    "    'log_healthy': 'Log Healthy',\n",
    "    'log_indian': 'Log Indian',\n",
    "    'log_italian': 'Log Italian',\n",
    "    'log_japanese': 'Log Japanese',\n",
    "    'log_noodle_dishes': 'Log Noodle Dishes',\n",
    "    'log_other': 'Log Other Cuisines',\n",
    "    'log_street_food_snacks': 'Log Street Food & Snacks',\n",
    "    'log_thai': 'Log Thai',\n",
    "    'log_total_amt': 'Log Total Amount',\n",
    "    'log_n_order': 'Log Order Count',\n",
    "    'log_avg_amt_per_product': 'Log Avg Amount per Product',\n",
    "    'log_avg_amt_per_order': 'Log Avg Amount per Order',\n",
    "    'log_avg_amt_per_vendor': 'Log Avg Amount per Vendor',\n",
    "    'log_n_days_week': 'Log Days of Week Ordered Count',\n",
    "    'log_n_times_day': 'Log Hours Ordered Count',\n",
    "    'foodie_flag': 'Is Foodie',\n",
    "    'gluttonous_flag': 'Is Gluttonous',\n",
    "    'loyal_flag': 'Is Loyal',\n",
    "    'top_cuisine': 'Top Cuisine',\n",
    "    'avg_amt_per_day': 'Avg Amount Spent per Day',\n",
    "    'avg_product_per_day': 'Avg Products Ordered per Day',\n",
    "    'avg_order_per_day': 'Avg Orders Placed per Day',\n",
    "    'age_bucket': 'Age Bucket',\n",
    "    'transaction_volume': 'Transaction Volume',\n",
    "    'interaction_rate': 'Interaction Rate'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8545301e-71c7-4c39-85a9-f5a0cbb43b65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Helpers\n",
    "\n",
    "Helpers for some of the routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ae213986-df0a-4c70-a88e-c71c255f953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the largest value in a dataframe\n",
    "\n",
    "def top_n(row, col_list, n):\n",
    "    # Sort the specified columns in descending order\n",
    "    sorted_row = row[col_list].sort_values(ascending=False)\n",
    "\n",
    "    # Get the unique sorted values\n",
    "    unique_sorted_values = sorted_row.unique()\n",
    "\n",
    "    # Ensure there are enough unique values to determine the n-th largest\n",
    "    if len(unique_sorted_values) >= n:\n",
    "        nth_value = unique_sorted_values[n - 1]  # Get the n-th largest unique value\n",
    "\n",
    "        # If the n-th value is 0, return None\n",
    "        if nth_value == 0:\n",
    "            return None\n",
    "        \n",
    "        # If n > 1, check for uniqueness against the (n-1)-th largest\n",
    "        if n > 1:\n",
    "            prev_value = unique_sorted_values[n - 2]  # (n-1)-th largest unique value\n",
    "            # If nth_value is equal to the (n-1)-th value, we don't want to return it\n",
    "            if nth_value == prev_value:\n",
    "                return None\n",
    "        \n",
    "        # Return the index of the n-th largest value\n",
    "        return sorted_row[sorted_row == nth_value].index[0]\n",
    "\n",
    "    # Return None if conditions are not met\n",
    "    return None\n",
    "\n",
    "# Throws a dice based on given n_choices and probabilities.\n",
    "def throw_dice(likelihood_dict):\n",
    "    # Unzip the dictionary into choices and probabilities\n",
    "    choices, probabilities = zip(*likelihood_dict.items())\n",
    "    \n",
    "    # Normalize probabilities\n",
    "    probabilities = np.array(probabilities) / np.sum(probabilities)\n",
    "    \n",
    "    # Return a random choice based on the probabilities\n",
    "    return np.random.choice(choices, p=probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0430fa0e-9970-40ca-b09e-dfb3d577a086",
   "metadata": {},
   "source": [
    "### Preprocess\n",
    "\n",
    "Wrangle the user input data, before it is ready for variable engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ca4882de-1683-4d7a-b2eb-fe7fc01c6080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(raw_data_point):\n",
    "    # Make a copy of the data point\n",
    "    data = raw_data_point.copy()\n",
    "    \n",
    "    # Enforce datatypes for metric elements\n",
    "    for key in METRIC_KEYS:\n",
    "        try:\n",
    "            data[key] = np.float64(data[key])\n",
    "            \n",
    "            # Check if the value is less than 0\n",
    "            if data[key] < 0:\n",
    "                raise ValueError\n",
    "        except (ValueError, KeyError) as e:\n",
    "            data[key] = np.nan\n",
    "    \n",
    "    # Enforce string types for non-metric elements\n",
    "    for key in NON_METRIC_KEYS:\n",
    "        try:\n",
    "            if not isinstance(data[key], str):\n",
    "                data[key] = np.nan\n",
    "        except KeyError:\n",
    "            data[key] = np.nan\n",
    "            \n",
    "    # Initialize sums\n",
    "    n_week = 0\n",
    "    n_day = 0\n",
    "    \n",
    "    # Calculate n_week and n_day directly, filling missing values with 0\n",
    "    for n in range(7):\n",
    "        dow_key = f\"DOW_{n}\"\n",
    "        if pd.isna(data[dow_key]):\n",
    "            data[dow_key] = 0\n",
    "        n_week += data[dow_key]\n",
    "    \n",
    "    for n in range(24):\n",
    "        hr_key = f\"HR_{n}\"\n",
    "        if pd.isna(data[hr_key]):\n",
    "            data[hr_key] = 0\n",
    "        n_day += data[hr_key]\n",
    "\n",
    "    diff = int(np.ceil(n_week - n_day))\n",
    "\n",
    "    # Correct if not equal\n",
    "    likelyhood = None\n",
    "    var = None\n",
    "    if diff < 0:\n",
    "        likelyhood = TIME_LIKELYHOODS['DAY']\n",
    "        n_day += diff\n",
    "    elif diff > 0:\n",
    "        likelyhood = TIME_LIKELYHOODS['HOUR']\n",
    "        n_week += diff\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if diff != 0:\n",
    "        for _ in range(diff):\n",
    "            data[throw_dice(likelyhood)] += 1\n",
    "    \n",
    "    # Finally, set n_order equal to the sum of either \n",
    "    n_order = n_week \n",
    "\n",
    "    # Fill missing amounts in cuisines with 0\n",
    "    for key in CUISINE_KEYS:\n",
    "        if pd.isna(data[key]):\n",
    "            data[key] = 0\n",
    "            \n",
    "    # Calculate number of cuisines ordered\n",
    "    n_cuisines = sum(\n",
    "        (data[key] > 0) \n",
    "        for key in CUISINE_KEYS \n",
    "    )\n",
    "\n",
    "    # Check if customer to segment has spent any money\n",
    "    if n_cuisines == 0:\n",
    "        return \"Error: Invalid customer. Specify customer ammounts spent per cuisine.\"        \n",
    "       \n",
    "    # Check if number of cuisines ordered is larger than the number of orders made\n",
    "    diff = int(np.ceil(n_cuisines > n_order))\n",
    "    \n",
    "    if diff > 0:\n",
    "        for _ in range(diff):\n",
    "            # Assign another HR stochastically\n",
    "            data[throw_dice(TIME_LIKELYHOODS['HOUR'])] += 1\n",
    "            n_day += 1\n",
    "            \n",
    "            # Assign another DAY stochastically\n",
    "            data[throw_dice(TIME_LIKELYHOODS['DAY'])] += 1\n",
    "            n_order += 1\n",
    "            n_week += 1\n",
    "    # Sum total cuisines to obtain the total amount spent\n",
    "    total_amt = sum(\n",
    "        data[key] for key in CUISINE_KEYS\n",
    "    )\n",
    "\n",
    "    data['n_order'] = n_order\n",
    "    data['n_times_day'] = n_day\n",
    "    data['n_days_week'] = n_week\n",
    "    data['n_cuisines'] = n_cuisines\n",
    "    data['total_amt'] = total_amt\n",
    "    \n",
    "    try:\n",
    "        # Assuming each vendor only serves one type of cuisine, which might introduce some bias, but how else to solve?\n",
    "        if data['n_vendor'] < n_cuisines:\n",
    "            data['n_vendor'] = n_cuisines\n",
    "    except ValueError:\n",
    "        data['n_vendor'] = n_cuisines\n",
    "\n",
    "    try:\n",
    "        if pd.isna(data['n_chain']): \n",
    "            raise ValueError\n",
    "        # Check if the customer made a consistent number of purchases from chained restaurants \n",
    "        if data['n_chain'] > data['n_vendor']:\n",
    "            data['n_chain'] = data['n_vendor']\n",
    "\n",
    "        # Check if the customer made an illegal number of purchases from chained restaurants\n",
    "        if data['n_chain'] < 0: \n",
    "            data['n_chain'] = 0\n",
    "            \n",
    "    except ValueError:\n",
    "        # If here then definitely illegal\n",
    "        data['n_chain'] = 0\n",
    "    \n",
    "    try:\n",
    "        # Check if the number of products is at least equal to the number of orders made\n",
    "        if data['n_product'] < data['n_order']:\n",
    "            data['n_product'] = data['n_order']\n",
    "    except ValueError:\n",
    "        data['n_product'] = data['n_order']\n",
    "\n",
    "    # Fill in low risk values with known dataset means.\n",
    "    if pd.isna(data['first_order']):\n",
    "        data['first_order'] = MEANS['first_order']\n",
    "        \n",
    "    if pd.isna(data['last_order']):\n",
    "        data['last_order'] = MEANS['last_order']\n",
    "\n",
    "    if pd.isna(data['last_order']):\n",
    "        data['cust_age'] = MEANS['cust_age']\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacf6348-8529-4105-a7e9-e1dee0dd4e5c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Process\n",
    "\n",
    "Process the data so that it follows the same data structure as the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "08e8f675-a938-4915-b001-6fe458ced15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(preprocessed_data):\n",
    "    # Make a copy of the data point\n",
    "    data = preprocessed_data.copy()\n",
    "\n",
    "    # Amount spent on average per product\n",
    "    data['avg_amt_per_product'] = data['total_amt'] / data['n_product'] if data['n_product'] > 0 else 0\n",
    "\n",
    "    # Amount spent on average per order\n",
    "    data['avg_amt_per_order'] = data['total_amt'] / data['n_order'] if data['n_order'] > 0 else 0\n",
    "\n",
    "    # Amount spent on average per vendor\n",
    "    data['avg_amt_per_vendor'] = data['total_amt'] / data['n_vendor'] if data['n_vendor'] > 0 else 0\n",
    "\n",
    "    # Total days as customer\n",
    "    data['days_cust'] = data['last_order'] - data['first_order']\n",
    "\n",
    "    # Average days between orders\n",
    "    data['avg_days_to_order'] = data['days_cust'] / data['n_order'] if data['n_order'] > 0 else 0\n",
    "\n",
    "    # Days the customer is due, according to their average days between orders\n",
    "    data['days_due'] = 90 - data['last_order'] + data['avg_days_to_order']\n",
    "\n",
    "    # Percentage of orders placed to restaurants that are part of a chain\n",
    "    data['per_chain_order'] = data['n_chain'] / data['n_order'] if data['n_order'] > 0 else 0\n",
    "\n",
    "    # Average amount spent per day as customer\n",
    "    data['avg_amt_per_day'] = round(data['total_amt'] / data['days_cust'], 4) if data['days_cust'] > 0 else 0\n",
    "\n",
    "    # Average number of products ordered per day as customer\n",
    "    data['avg_product_per_day'] = round(data['n_product'] / data['days_cust'], 4) if data['days_cust'] > 0 else 0\n",
    "\n",
    "    # Average number of orders per day as customer\n",
    "    data['avg_order_per_day'] = round(data['n_order'] / data['days_cust'], 4) if data['days_cust'] > 0 else 0\n",
    "\n",
    "    # Apply log1p to each column in LOGS and add it to log_transformed with the prefix 'log_'\n",
    "    for col in LOGS:\n",
    "        if col in data and data[col] > 0:\n",
    "            data[f\"log_{col}\"] = np.log1p(data[col])\n",
    "        else:\n",
    "            data[f\"log_{col}\"] = 0\n",
    "\n",
    "    # Creating age buckets\n",
    "    data['age_bucket'] = (\n",
    "        '15-24' if data['cust_age'] < 25 else\n",
    "        '25-34' if data['cust_age'] < 35 else\n",
    "        '35-44' if data['cust_age'] < 45 else\n",
    "        '45-54' if data['cust_age'] < 55 else\n",
    "        '55-64' if data['cust_age'] < 65 else\n",
    "        '65+'\n",
    "    )\n",
    "\n",
    "    # Flag customers who have purchased in more than one day\n",
    "    data['regular'] = data['days_cust'] > 1\n",
    "\n",
    "    # Create columns to hold the flags for each feature group\n",
    "    data['foodie_flag'] = 0\n",
    "    data['gluttonous_flag'] = 0\n",
    "    data['loyal_flag'] = 0\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb893d86-b202-4d1f-84dc-50c432ecc539",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Scaling\n",
    "\n",
    "Scalling the data, so that it is on the same scaling assd the models, and can be used in predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "8b35d8c7-66d6-452c-b166-fdfe414032d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(processed_data):\n",
    "    # Create a copy of the input data to avoid modifying the original dataset\n",
    "    data = processed_data.copy()\n",
    "\n",
    "    # Convert data to a DataFrame\n",
    "    data = pd.DataFrame(data, index=['007'])\n",
    "\n",
    "    # Identify features not in SCALER_FEATURES\n",
    "    other_features = [col for col in data.columns if col not in SCALER_FEATURES]\n",
    "    \n",
    "    # Step 1: Scale the SCALER_FEATURES\n",
    "    scaled_data = SCALER.transform(data[SCALER_FEATURES])\n",
    "    \n",
    "    # Create a DataFrame for the scaled features\n",
    "    scaled_df = pd.DataFrame(scaled_data, columns=SCALER_FEATURES, index=data.index)\n",
    "    \n",
    "    # Concatenate the scaled features with the unscaled features\n",
    "    data = pd.concat([scaled_df, data[other_features]], axis=1)\n",
    "\n",
    "    # Step 2: Apply PCA transformation to the specified features\n",
    "    pca_data = PCA.transform(data[PCA_FEATURES])\n",
    "    \n",
    "    # Create a DataFrame for the PCA-transformed features\n",
    "    pca_df = pd.DataFrame(pca_data, columns=[f'PC{i}' for i in range(pca_data.shape[1])], index=data.index)\n",
    "    \n",
    "    # Step 3: Drop 'PC2' and rename 'PC0' and 'PC1'\n",
    "    pca_df.drop(columns='PC2', inplace=True, errors='ignore')  # Ignore if 'PC2' is not found\n",
    "    pca_df.rename(columns={'PC0': 'transaction_volume', 'PC1': 'interaction_rate'}, inplace=True)\n",
    "    \n",
    "    # Step 4: Concatenate the PCA-transformed features with the rest of the data\n",
    "    data = pd.concat([pca_df, data], axis=1)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a824586-217f-49a5-ae7a-ff7fe055b206",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b546be61-a778-41cb-ab25-8f6178576256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFM KMEANS\n",
    "def spending_clustering(data_input):\n",
    "    spending_features = \\\n",
    "        ['total_amt', 'n_cuisines', 'n_vendor', 'n_product']\n",
    "    \n",
    "    data = data_input[spending_features]\n",
    "    spending_label = SPENDING.predict(data)\n",
    "\n",
    "    return spending_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "43c2f563-9da3-4162-8066-b63359359ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse Principal Components with Gaussian Mixture Models\n",
    "def cuisine_clustering(data_input):\n",
    "    cuisines_features = [\n",
    "        'log_american', 'log_asian', 'log_beverages', 'log_cafe', 'log_chinese', 'log_desserts', 'log_healthy', 'log_indian',\n",
    "        'log_italian', 'log_japanese', 'log_noodle_dishes', 'log_other', 'log_street_food_snacks', 'log_thai', 'log_chicken_dishes'\n",
    "    ]\n",
    "    columns_to_add = ['log_total_amt', 'log_avg_amt_per_product']\n",
    "    \n",
    "    # Select the cuisine factores\n",
    "    cuisines_factors = data_input[cuisines_features].copy()\n",
    "    \n",
    "    # Load the SPCA model\n",
    "    spca = joblib.load('models/spca.pkl')\n",
    "\n",
    "    # Transform the dataframe, returns an array\n",
    "    spca_array = spca.transform(cuisines_factors)\n",
    "\n",
    "    # Cast as dataframe with Component names\n",
    "    spca_df = pd.DataFrame(\n",
    "        spca_array,\n",
    "        columns=[f\"Component_{i+1}\" for i in range(2)],\n",
    "        index=cuisines_factors.index\n",
    "    )\n",
    "\n",
    "    # Build the final clustering dataframe, by adding critical columns\n",
    "    cuisines_df = pd.concat([\n",
    "        spca_df,\n",
    "        data_input[columns_to_add]\n",
    "    ], axis=1)\n",
    "\n",
    "    # Load the Gaussian Mixture Model\n",
    "    cuisines_algorithm = joblib.load('models/cuisine_clustering.pkl')\n",
    "\n",
    "    # Obtain labels for the input\n",
    "    labels = cuisines_algorithm.predict(cuisines_df)\n",
    "    if data_input['cust_region'].isna().any():\n",
    "        city = CUISINE_CLUSTER_TO_CITY_DICT[labels[0]]\n",
    "    else:\n",
    "        city = data_input['cust_region'].apply(lambda x: x[0])\n",
    "\n",
    "    return labels, city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c5bfb1b8-cc48-4691-8850-2e9446b37764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Negative Matrix Factorization with Self-Organizing Maps, projected with KMeans\n",
    "def time_clustering(data_input):\n",
    "    \n",
    "    time_features = [\n",
    "        *DATA.columns[29: 53].tolist(),\n",
    "        *DATA.columns[22: 29].tolist()\n",
    "    ]\n",
    "    \n",
    "    columns_to_add = ['total_amt', 'avg_amt_per_product', 'n_chain', 'n_cuisines']\n",
    "\n",
    "    data_input = pd.concat([\n",
    "        pd.DataFrame(SCALER.inverse_transform(data_input[SCALER_FEATURES]), columns=SCALER_FEATURES, index=data_input.index),\n",
    "        data_input.drop(columns=SCALER_FEATURES)\n",
    "    ], axis=1)\n",
    "    \n",
    "    time_input = MINMAXSCALER.transform(data_input[time_features])\n",
    "\n",
    "    for col in time_input.columns:\n",
    "        if np.isclose(time_input[col], 0):\n",
    "            time_input[col] = 0\n",
    "    \n",
    "    W = NMF.transform(time_input).round(decimals=3)\n",
    "    W_df = pd.DataFrame(W, columns=[f\"Factor_{i+1}\" for i in range(W.shape[1])], index=data_input.index)\n",
    "    \n",
    "    data_input = MINMAXSCALER.fit_transform(data_input[columns_to_add])\n",
    "    \n",
    "    # Concatenate the transformed data with the additional columns\n",
    "    time_df = pd.concat([W_df, data_input], axis=1)\n",
    "    weights_flat = MINISOM.get_weights().reshape((5 * 5), len(time_df.columns))\n",
    "    labels = TIME.predict(weights_flat)\n",
    "    kmeans_matrix = labels.reshape((5, 5))\n",
    "    bmu_index = np.array([MINISOM.winner(x) for x in time_df.values])\n",
    "    labels = [kmeans_matrix[i[0]][i[1]] for i in bmu_index]\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "28cba420-7b96-4916-b990-f67a8914a8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral Clustering, projected with KMeans\n",
    "def geography_clustering(data_input):\n",
    "    geography_features = [\n",
    "        'per_chain_order', 'log_total_amt', 'avg_amt_per_product', \n",
    "        'n_cuisines', 'cust_city_2.0', 'cust_city_4.0', 'cust_city_8.0'\n",
    "    ]\n",
    "    \n",
    "    # One-hot encode `cust_city`\n",
    "    data_input = data_input.copy()\n",
    "    if data_input.loc['007', 'cust_city'] == 2:\n",
    "        data_input['cust_city_2.0'] = 1\n",
    "        data_input['cust_city_4.0'] = 0\n",
    "        data_input['cust_city_8.0'] = 0\n",
    "    elif data_input.loc['007', 'cust_city'] == 4:\n",
    "        data_input['cust_city_2.0'] = 0\n",
    "        data_input['cust_city_4.0'] = 1\n",
    "        data_input['cust_city_8.0'] = 0\n",
    "    elif data_input.loc['007', 'cust_city'] == 8:\n",
    "        data_input['cust_city_2.0'] = 0\n",
    "        data_input['cust_city_4.0'] = 0\n",
    "        data_input['cust_city_8.0'] = 1\n",
    "    else:\n",
    "       pass\n",
    "\n",
    "    # Filter features for clustering\n",
    "    geography_df = DATA[geography_features].copy()\n",
    "\n",
    "    # Select only relevant features from `data_input` and set a unique index\n",
    "    data_input = data_input[geography_features].copy()\n",
    "    data_input.index = ['007']  # Assign a unique index for tracking    \n",
    "\n",
    "    # Setting best-found Clustering hyperparameters\n",
    "    n_clusters = 3\n",
    "    rbf_param = 3.141542\n",
    "\n",
    "    # Sampling the dataset for efficient computation\n",
    "    arr_spectral_df = geography_df.sample(n=3000, random_state=1)\n",
    "\n",
    "    arr_spectral_df = pd.concat([arr_spectral_df, data_input], axis=0)\n",
    "    \n",
    "    spectral_array = arr_spectral_df.values\n",
    "\n",
    "    # Calculating the Gower distance matrix\n",
    "    gower_dist = gower_matrix(spectral_array)\n",
    "\n",
    "    # Applying the RBF transform\n",
    "    K = np.exp(-rbf_param * gower_dist)\n",
    "    \n",
    "    # Computing the similarity matrix and obtaining eigenpair solutions\n",
    "    D = K.sum(axis=1)\n",
    "    D = np.sqrt(1 / D)\n",
    "    M = np.multiply(D[np.newaxis, :], np.multiply(K, D[np.newaxis, :]))\n",
    "    \n",
    "    U, Sigma, _ = svd(M, full_matrices=False, lapack_driver='gesvd')\n",
    "\n",
    "    # Points map to the eigens\n",
    "    Usubset = U[:, :n_clusters]\n",
    "\n",
    "    # Apply KMeans to the spectral embedding\n",
    "    geography_algorithm = joblib.load('models/spectral_clustering.pkl')\n",
    "    geography_labels = geography_algorithm.predict(normalize(Usubset))\n",
    "\n",
    "    geography_df = pd.concat([\n",
    "        arr_spectral_df,\n",
    "        pd.Series(geography_labels, name='labels', index=arr_spectral_df.index)\n",
    "    ], axis=1)\n",
    "\n",
    "    # Extract the label for the row with index '007'\n",
    "    geography_label = geography_df.loc['007']['labels'].astype('int')\n",
    "\n",
    "    return geography_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "aff41986-2a7c-464e-8b68-0ed88a70e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_clusters(scaled_point):\n",
    "    \n",
    "    # Create a copy of the input data\n",
    "    data_input = scaled_point.copy()\n",
    "\n",
    "    # Perform clustering for cuisines\n",
    "    cuisine_label, city = cuisine_clustering(data_input)\n",
    "\n",
    "    data_input['cust_city'] = int(city)\n",
    "    \n",
    "    # Perform clustering for spending behavior\n",
    "    spending_label = spending_clustering(data_input)\n",
    "    \n",
    "    # Perform clustering for time\n",
    "    time_label = time_clustering(data_input)\n",
    "    \n",
    "    # Perform clustering for geography\n",
    "    geography_label = geography_clustering(data_input)\n",
    "\n",
    "    # Add labels and city (if applicable) to data_input\n",
    "    data_input['cuisine_label'] = cuisine_label\n",
    "    data_input['spending_label'] = spending_label\n",
    "    data_input['time_label'] = time_label\n",
    "    data_input['geography_label'] = geography_label\n",
    "\n",
    "    return data_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "852b9d3c-5317-4156-bb31-95a6232fe7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicts profilling variables, via mode for customers in same cust_city (most discriminating variable)\n",
    "\n",
    "def predict_profile(predicted_point):\n",
    "    data = DATA[DATA['cust_city'] == predicted_point['cust_city']]\n",
    "\n",
    "    # For each feature in NON_METRIC_KEYS, check if it's NaN in predicted_point\n",
    "    for feature in NON_METRIC_KEYS:\n",
    "        if pd.isna(predicted_point[feature]):  # If the feature is NaN in predicted_point\n",
    "            # Get the mode of that feature in the relevant data\n",
    "            mode_value = data[feature].mode()[0]  # Get the first mode\n",
    "            predicted_point[feature] = mode_value  # Fill NaN with mode value\n",
    "    \n",
    "    return predicted_point\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d832dfb-2486-4b68-8ef6-a4ffe0f30691",
   "metadata": {},
   "source": [
    "## Segmenting\n",
    "\n",
    "Finds the given customers best segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ed64eef1-68a3-4e4b-8960-481de93752f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_segmentation(data_point):\n",
    "    # Workflow\n",
    "    \n",
    "    # Preprocess\n",
    "    preproc_point = preproc(data_point)\n",
    "    # Process\n",
    "    processed_point = process(preproc_point)\n",
    "    # Scale\n",
    "    scaled_point = scale(processed_point)\n",
    "    # Predict\n",
    "    predicted_point = predict_clusters(scaled_point)\n",
    "\n",
    "    DATA[DATA['cust_city'] == predicted_point['cust_city']][NON_METRIC_KEYS]\n",
    "    \n",
    "    return predicted_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "24270709",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = {'Region': '4660', 'Age': '20', 'Vendor Count': '2', 'Product Count': '2', 'Chain Restaurant Order Count': '2', 'First Order Date': '2', 'Last Order Date': '20', 'Promotion': 'DELIVERY', 'Payment Method': 'CARD', 'American': '20', 'Asian': None, 'Beverages': None, 'Cafe': None, 'Chicken Dishes': '20', 'Chinese': '20', 'Desserts': None, 'Healthy': None, 'Indian': None, 'Italian': None, 'Japanese': None, 'Noodle Dishes': None, 'Other Cuisines': None, 'Street Food & Snacks': None, 'Thai': None, 'Sunday': None, 'Monday': '20', 'Tuesday': None, 'Wednesday': None, 'Thursday': '20', 'Friday': None, 'Saturday': None, '12AM': None, '1AM': None, '2AM': None, '3AM': '20', '4AM': None, '5AM': None, '6AM': '20', '7AM': None, '8AM': None, '9AM': None, '10AM': None, '11AM': None, '12PM': None, '1PM': None, '2PM': None, '3PM': None, '4PM': None, '5PM': '20', '6PM': None, '7PM': None, '8PM': None, '9PM': None, '10PM': None, '11PM': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "5ccae66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_mapping = {v: k for k, v in mapping_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d47b2f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_ = {reversed_mapping[k]: v for k, v in example.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
