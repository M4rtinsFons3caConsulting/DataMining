{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f85e7a0-f4af-4a45-b8ea-3f04aaf453b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "764ec4a9-5726-4382-b7c4-73bdb678ba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "PATH = '../dashboard/data/'\n",
    "\n",
    "FILENAME = 'regulars.csv'\n",
    "DATA = pd.read_csv(f'{PATH}{FILENAME}')\n",
    "\n",
    "SCALER = joblib.load('models/std_scaler.pkl')\n",
    "ENCODER = joblib.load('models/hot_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5d7f495-1526-47cf-ad02-693e0f4663c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OG_DICT = {\n",
    "        'customer_region' : 'cust_region'\n",
    "        , 'payment_method' : 'pay_method'\n",
    "        , 'customer_age' : 'cust_age'\n",
    "        , 'vendor_count' : 'n_vendor'\n",
    "        , 'product_count' : 'n_product'\n",
    "        , 'n_order' : 'n_order'\n",
    "        , 'is_chain' : 'n_chain'\n",
    "        , 'CUI_American' : 'american'\n",
    "        , 'CUI_Asian' : 'asian'\n",
    "        , 'CUI_Beverages' : 'beverages'\n",
    "        , 'CUI_Cafe' : 'cafe'\n",
    "        , 'CUI_Chicken Dishes' : 'chicken_dishes'\n",
    "        , 'CUI_Chinese' : 'chinese'\n",
    "        , 'CUI_Desserts' : 'desserts'\n",
    "        , 'CUI_Healthy' : 'healthy'\n",
    "        , 'CUI_Indian' : 'indian'\n",
    "        , 'CUI_Italian' : 'italian'\n",
    "        , 'CUI_Japanese' : 'japanese'\n",
    "        , 'CUI_Noodle Dishes' : 'noodle_dishes'\n",
    "        , 'CUI_OTHER' : 'other'\n",
    "        , 'CUI_Street Food / Snacks' : 'street_food_snacks'\n",
    "        , 'CUI_Thai' : 'thai'\n",
    "}\n",
    "\n",
    "METRIC = \\\n",
    "{\n",
    "    'cust_age': 27.505,\n",
    "    'n_vendor': 3.626,\n",
    "    'n_product': 6.392,\n",
    "    'n_chain': 3.267,\n",
    "    'first_order': 23.081,\n",
    "    'last_order': 68.927,\n",
    "    'american': 5.574,\n",
    "    'asian': 11.208,\n",
    "    'beverages': 2.489,\n",
    "    'cafe': 0.778,\n",
    "    'chicken_dishes': 0.86,\n",
    "    'chinese': 1.509,\n",
    "    'desserts': 0.902,\n",
    "    'healthy': 0.97,\n",
    "    'indian': 1.677,\n",
    "    'italian': 3.586,\n",
    "    'japanese': 3.241,\n",
    "    'noodle_dishes': 0.739,\n",
    "    'other': 3.317,\n",
    "    'street_food_snacks': 3.965,\n",
    "    'thai': 0.907,\n",
    "    'total_amt': 41.723,\n",
    "    'n_order': 5.008,\n",
    "    'avg_amt_per_product': 6.879,\n",
    "    'avg_amt_per_order': 9.066,\n",
    "    'avg_amt_per_vendor': 12.67,\n",
    "    'days_cust': 45.846,\n",
    "    'avg_days_to_order': 11.276,\n",
    "    'days_due': 32.349,\n",
    "    'per_chain_order': 0.637,\n",
    "    'n_days_week': 3.185,\n",
    "    'n_times_day': 3.42,\n",
    "    'n_cuisines': 2.628,\n",
    "    'log_n_vendor': 1.418,\n",
    "    'log_n_product': 1.808,\n",
    "    'log_n_chain': 1.203,\n",
    "    'log_american': 0.94,\n",
    "    'log_asian': 1.223,\n",
    "    'log_beverages': 0.447,\n",
    "    'log_cafe': 0.124,\n",
    "    'log_chicken_dishes': 0.227,\n",
    "    'log_chinese': 0.287,\n",
    "    'log_desserts': 0.167,\n",
    "    'log_healthy': 0.18,\n",
    "    'log_indian': 0.289,\n",
    "    'log_italian': 0.556,\n",
    "    'log_japanese': 0.552,\n",
    "    'log_noodle_dishes': 0.162,\n",
    "    'log_other': 0.592,\n",
    "    'log_street_food_snacks': 0.425,\n",
    "    'log_thai': 0.192,\n",
    "    'log_total_amt': 3.436,\n",
    "    'log_n_order': 1.634,\n",
    "    'log_avg_amt_per_product': 1.97,\n",
    "    'log_avg_amt_per_order': 2.157,\n",
    "    'log_avg_amt_per_vendor': 2.41,\n",
    "    'log_n_days_week': 1.372,\n",
    "    'log_n_times_day': 1.403,\n",
    "    'avg_amt_per_day': 1.345,\n",
    "    'avg_product_per_day': 0.189,\n",
    "    'avg_order_per_day': 0.149\n",
    "}\n",
    "\n",
    "TIME_LIKELYHOODS = \\\n",
    "{ \n",
    "    'DAY':\n",
    "    {\n",
    "        'DOW_0': 0.638,\n",
    "        'DOW_1': 0.65,\n",
    "        'DOW_2': 0.679,\n",
    "        'DOW_3': 0.71,\n",
    "        'DOW_4': 0.777,\n",
    "        'DOW_5': 0.746,\n",
    "        'DOW_6': 0.808        \n",
    "    },\n",
    "\n",
    "    'HOUR':\n",
    "    {\n",
    "        'HR_0': 0.053,\n",
    "        'HR_1': 0.06,\n",
    "        'HR_2': 0.07,\n",
    "        'HR_3': 0.136,\n",
    "        'HR_4': 0.114,\n",
    "        'HR_5': 0.094,\n",
    "        'HR_6': 0.078,\n",
    "        'HR_7': 0.084,\n",
    "        'HR_8': 0.142,\n",
    "        'HR_9': 0.263,\n",
    "        'HR_10': 0.374,\n",
    "        'HR_11': 0.436,\n",
    "        'HR_12': 0.369,\n",
    "        'HR_13': 0.276,\n",
    "        'HR_14': 0.247,\n",
    "        'HR_15': 0.318,\n",
    "        'HR_16': 0.414,\n",
    "        'HR_17': 0.452,\n",
    "        'HR_18': 0.391,\n",
    "        'HR_19': 0.287,\n",
    "        'HR_20': 0.166,\n",
    "        'HR_21': 0.083,\n",
    "        'HR_22': 0.053,\n",
    "        'HR_23': 0.051   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae213986-df0a-4c70-a88e-c71c255f953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "def top_n(row, col_list, n):\n",
    "    # Sort the specified columns in descending order\n",
    "    sorted_row = row[col_list].sort_values(ascending=False)\n",
    "\n",
    "    # Get the unique sorted values\n",
    "    unique_sorted_values = sorted_row.unique()\n",
    "\n",
    "    # Ensure there are enough unique values to determine the n-th largest\n",
    "    if len(unique_sorted_values) >= n:\n",
    "        nth_value = unique_sorted_values[n - 1]  # Get the n-th largest unique value\n",
    "\n",
    "        # If the n-th value is 0, return None\n",
    "        if nth_value == 0:\n",
    "            return None\n",
    "        \n",
    "        # If n > 1, check for uniqueness against the (n-1)-th largest\n",
    "        if n > 1:\n",
    "            prev_value = unique_sorted_values[n - 2]  # (n-1)-th largest unique value\n",
    "            # If nth_value is equal to the (n-1)-th value, we don't want to return it\n",
    "            if nth_value == prev_value:\n",
    "                return None\n",
    "        \n",
    "        # Return the index of the n-th largest value\n",
    "        return sorted_row[sorted_row == nth_value].index[0]\n",
    "\n",
    "    # Return None if conditions are not met\n",
    "    return None\n",
    "\n",
    "def throw_dice(colnames, likelyhoods: list) -> str:\n",
    "    return np.random.choice(colnames, p=np.array(likelyhoods)/np.sum(likelyhoods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80d1350f-6435-4d24-953b-e2f197e45a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(data_point: dict) -> dict:\n",
    "    if not isinstance(data_point, dict):\n",
    "        raise TypeError(\"The input must be a dictionary.\")\n",
    "\n",
    "    # Workflow\n",
    "    # Apply preprocessment to datapoint\n",
    "    point = preproc(data_point)\n",
    "    # call scaler\n",
    "    # call models\n",
    "    \n",
    "    return data_point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca4882de-1683-4d7a-b2eb-fe7fc01c6080",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "illegal target for annotation (379136628.py, line 52)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 52\u001b[0;36m\u001b[0m\n\u001b[0;31m    'customer_region' : 'cust_region'\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m illegal target for annotation\n"
     ]
    }
   ],
   "source": [
    "def preproc(raw_data_point: dict) -> dict: \n",
    "    # 1. Enforce datatypes\n",
    "    for key in raw_data_point:\n",
    "        if key in METRIC_FEATURE_LIST:\n",
    "            if type(raw_data_point[key]) not in 'Int64':\n",
    "                try:\n",
    "                    raw_data_point[key] = raw_data_point[key].astype('Int64')\n",
    "                except ValueError:\n",
    "                    raw_data_point[key] = np.nan\n",
    "        else:\n",
    "            if type(raw_data_point[key]) not in 'str':\n",
    "                raw_data_point[key] = raw_data_point[key].astype('Object')\n",
    "\n",
    "    # Assume user is leaving blank non-natural primary key items in the table.\n",
    "    \n",
    "    # Fill missing HR, and DAY with 0\n",
    "    raw_data_point[[f\"DOW_{n}\" for n in range(7)]].fillna(0, inplace=True)\n",
    "    raw_data_point[[f\"HR_{n}\" for n in range(7)]].fillna(0, inplace=True)\n",
    "    \n",
    "    # But correct the number of WEEK and Day purchases to match\n",
    "    sum_week = raw_data_point[[f\"DOW_{n}\" for n in range(7)]].sum(axis=1)\n",
    "    sum_day = raw_data_point[[f\"HR_{n}\" for n in range(24)]].sum(axis=1)\n",
    "    diff = sum_week - sum_day\n",
    "    \n",
    "    # Correct if not equal\n",
    "    likelyhood = None\n",
    "    if diff < 0:\n",
    "        likelyhood = TIME_LIKELYHOODS['DAY']\n",
    "    elif diff > 0:\n",
    "        likelyhood = TIME_LIKELYHOODS['WEEK']\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if likelyhoods:\n",
    "        for _ in range(diff):\n",
    "            raw_data_point[throw_dice(**likelyhoods)] += 1\n",
    "\n",
    "    # Finally, set n_order equal to either sum of either \n",
    "    n_order = sum_week \n",
    "    \n",
    "    # Fill missing ammounts in cuisines with 0\n",
    "    raw_data_point[list({v for k, v in OG_DICT.items() if k.startswith('CUI')})].fillna(0, inplace=True)\n",
    "    \n",
    "    columns_to_add = ['log_total_amt', 'log_avg_amt_per_product']\n",
    "    columns_to_add = ['total_amt', 'avg_amt_per_product', 'n_chain', 'n_cuisines']\n",
    "    \n",
    "    columns_to_add = ['total_amt', 'n_cuisines', 'n_vendor', 'n_product']\n",
    "    \n",
    "    , 'vendor_count' : 'n_vendor'\n",
    "    , 'is_chain' : 'n_chain'\n",
    "    , 'product_count' : 'n_product'\n",
    "\n",
    "    'customer_region' : 'cust_region'\n",
    "    , 'payment_method' : 'pay_method'\n",
    "    , 'customer_age' : 'cust_age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885c5d0d-5d0c-4c5f-88d2-143c2a71d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# has at least one vendor\n",
    "has_vendor = data['n_vendor'] != 0 \n",
    "\n",
    "# has at least one product\n",
    "has_product = data['n_product'] != 0 \n",
    "\n",
    "# purchase must have been made on a valid dow\n",
    "some_day = (data[[f\"DOW_{n}\" for n in range(7)]] != 0).any(axis = 1) \n",
    "\n",
    "# purchase must have been made at a valid hour\n",
    "some_hour = (data[[f\"HR_{n}\" for n in range(24)]] != 0).any(axis = 1)  \n",
    "\n",
    "# some type of cuisine must have been ordered\n",
    "some_food = (data[data.columns[9:24]] != 0).any(axis = 1) \n",
    "\n",
    "data = data[(has_vendor & has_product & some_day & some_hour & some_food)]  # And we drop these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f87ec88-1457-4865-ba1c-9089fe2ecc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer Region\n",
    "data.loc[data['cust_region'] == '-', 'cust_region'] = '8670'\n",
    "data.loc[data['cust_region'].isin(['2440', '2490']), 'cust_region'] = '2400'\n",
    "\n",
    "# Add the feature Customer CIty\n",
    "data['cust_city'] = data['cust_region'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a3c1e4-c253-40eb-84dc-b39c7d1cb561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last Promo\n",
    "data.loc[data['last_promo'] == '-', 'last_promo'] = 'NO_PROMO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03520739-ebc3-43a4-b861-ebec6b2aa8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidying up datatypes\n",
    "for col in data.iloc[:, 0:9]:\n",
    "    if col in ['last_promo', 'pay_method']:\n",
    "        data[col] = data[col].astype(object)\n",
    "    else:\n",
    "        data[col] = data[col].astype('Int64')\n",
    "\n",
    "for col in data.iloc[:, 9:24]:\n",
    "    data[col] = data[col].astype(float)\n",
    "\n",
    "for col in data.iloc[:, 24:]:\n",
    "    data[col] = data[col].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c94e94-3302-4a2a-a612-441e7e3946b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables\n",
    "non_metric_features = []\n",
    "\n",
    "# Hour of day variables\n",
    "hour_features = data.columns[31:55].to_list()\n",
    "\n",
    "# Day of week variables\n",
    "day_features = data.columns[24:31].to_list()\n",
    "\n",
    "# Cuisine features\n",
    "cuisine_features = data.columns[9:24].to_list()\n",
    "\n",
    "# Metric variables, that are not above\n",
    "metric_features = data.columns.drop(non_metric_features).drop(hour_features).drop(day_features).drop(cuisine_features).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e8f675-a938-4915-b001-6fe458ced15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total amount spent by customer on all types of cuisine\n",
    "data['total_amt'] = data[cuisine_features].sum(axis=1)\n",
    "\n",
    "# Number of orders made by the customer\n",
    "data['n_order'] = data[day_features].sum(axis=1)\n",
    "\n",
    "# Amount spent on average per product\n",
    "data['avg_amt_per_product'] = data['total_amt'] / data['n_product']\n",
    "\n",
    "# Amount spent on average per order\n",
    "data['avg_amt_per_order'] = data['total_amt'] / data['n_order']\n",
    "\n",
    "# Amount spent on average per vendor\n",
    "data['avg_amt_per_vendor'] = data['total_amt'] / data['n_vendor']\n",
    "\n",
    "# Total days as customer\n",
    "data['days_cust'] = data['last_order'] - data['first_order']\n",
    "\n",
    "# Average days between orders\n",
    "data['avg_days_to_order'] = data['days_cust'] / data['n_order']\n",
    "\n",
    "# Days the customer is due, according to their average days between orders\n",
    "data['days_due'] = 90 - data['last_order'] + data['avg_days_to_order']\n",
    "\n",
    "# Percentage of orders placed to restaurants that are part of a chain\n",
    "data['per_chain_order'] = data['n_chain'] / data['n_order']\n",
    "\n",
    "# And we add these tese features to the metric features list.\n",
    "metric_features.extend([\n",
    "    'n_order'\n",
    "    ,'per_chain_order'\n",
    "    ,'total_amt'\n",
    "    ,'avg_amt_per_order'\n",
    "    ,'avg_amt_per_product'\n",
    "    ,'avg_amt_per_vendor'\n",
    "    ,'days_cust'\n",
    "    ,'avg_days_to_order'\n",
    "    ,'days_due'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc43983-1ca9-470c-91b4-08f1c05b65bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask to check if each day column is populated\n",
    "mask = data[[f'DOW_{i}' for i in range(7)]] > 0\n",
    "\n",
    "# Sum over the mask to get the count of days with purchases for each row\n",
    "data.loc[:, 'n_days_week'] = mask.sum(axis=1)\n",
    "\n",
    "# Updating the list of metric features\n",
    "metric_features.append('n_days_week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ae784-31d6-43e7-a7fa-fff6e792ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask to check if each hour column is populated\n",
    "mask = data[hour_features] > 0\n",
    "\n",
    "# Sum over the mask to get the count of hours with purchases for each row\n",
    "data.loc[:, 'n_times_day'] = mask.sum(axis=1)\n",
    "\n",
    "# Updating the list of metric features\n",
    "metric_features.append('n_times_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a112e658-1c42-47bb-a0f1-910c10f5f3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag customers who have purchased in more than one day\n",
    "data['regular'] = (data['days_cust'] > 1)\n",
    "\n",
    "non_metric_features.append('regular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4979cf38-f857-4971-ac93-719c810da62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask where values are greater than zero (indicating an order)\n",
    "mask = data[cuisine_features] > 0\n",
    "\n",
    "# Use mask to get the number of cuisines for each row\n",
    "data.loc[:, 'n_cuisines'] = mask.sum(axis=1)\n",
    "\n",
    "# Updating the metric_features_list\n",
    "metric_features.append('n_cuisines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee45ab09-e666-454b-8666-1544fa80822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping specified columns and getting remaining columns as a list\n",
    "targets = data.drop(columns=[\n",
    "    'cust_age'\n",
    "    , 'first_order'\n",
    "    , 'last_order'\n",
    "    , 'days_cust'\n",
    "    , 'days_due'\n",
    "    , 'avg_days_to_order'\n",
    "    , 'per_chain_order'\n",
    "    , 'cust_region'\n",
    "    , 'cust_city'\n",
    "    , 'last_promo'\n",
    "    , 'pay_method'\n",
    "    , 'n_cuisines'\n",
    "    , 'regular'\n",
    "] + hour_features + day_features).columns.tolist()\n",
    "\n",
    "# Initialize an empty dfFrame to store log-transformed columns\n",
    "log_transformed = pd.DataFrame()\n",
    "\n",
    "# Apply log1p to each column in targets and add it to log_transformed with the prefix 'log_'\n",
    "for col in targets:\n",
    "    log_transformed[f\"log_{col}\"] = np.log1p(data[col])\n",
    "\n",
    "# We create a list of log_features to assist us in our exploration\n",
    "log_features = log_transformed.columns.tolist()\n",
    "\n",
    "# Concatenate the original dfFrame with the new log-transformed dfFrame\n",
    "data = pd.concat([data, log_transformed], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24985f23-4590-4ea4-83e6-f53eedd893c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries for feature groups with flags and relevant columns\n",
    "feature_groups = {\n",
    "    'foodie': ['n_vendor', 'n_product', 'n_order', 'n_cuisines'],\n",
    "    'gluttonous': ['avg_amt_per_order', 'total_amt', 'n_chain'],\n",
    "    'loyal': ['avg_amt_per_vendor'] + cuisine_features\n",
    "}\n",
    "\n",
    "# Create columns to hold the flags for each feature group\n",
    "data['foodie_flag'] = 0\n",
    "data['gluttonous_flag'] = 0\n",
    "data['loyal_flag'] = 0\n",
    "\n",
    "# Function to calculate IQR bounds\n",
    "def calculate_bounds(series):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# Assign flags for each feature group\n",
    "for group, features in feature_groups.items():\n",
    "    for feature in features:\n",
    "        log_feature = f\"log_{feature}\"\n",
    "        \n",
    "        if feature == 'n_cuisines':\n",
    "            log_feature = feature\n",
    "        \n",
    "        lower_bound, upper_bound = calculate_bounds(data.loc[(data['regular'] == 1) & (data[feature] > 0), log_feature])\n",
    "        \n",
    "        # Mark outliers for each group\n",
    "        if group == 'foodie':\n",
    "            data.loc[data['regular'] == 1, 'foodie_flag'] |= (\n",
    "                data.loc[data['regular'] == 1, log_feature] > upper_bound\n",
    "            ).astype(int)\n",
    "        elif group == 'gluttonous':\n",
    "            data.loc[data['regular'] == 1, 'gluttonous_flag'] |= (\n",
    "                data.loc[data['regular'] == 1, log_feature] > upper_bound\n",
    "            ).astype(int)\n",
    "        elif group == 'loyal':\n",
    "            data.loc[data['regular'] == 1, 'loyal_flag'] |= (\n",
    "                data.loc[data['regular'] == 1, log_feature] > upper_bound\n",
    "            ).astype(int)\n",
    "\n",
    "# Display results\n",
    "for group in ['foodie_flag', 'gluttonous_flag', 'loyal_flag']:\n",
    "    print(f\"Number of customers flagged as {group.split('_')[0]}:\", data[group].sum())\n",
    "\n",
    "non_metric_features.extend([\n",
    "    'foodie_flag'\n",
    "    ,'gluttonous_flag'\n",
    "    ,'loyal_flag'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d389edfd-4bed-4788-a0a8-e3c8b0f2209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['top_cuisine'] = data.apply(top_n, col_list=cuisine_features, n=1, axis=1)\n",
    "\n",
    "non_metric_features.append('top_cuisine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8f2f5-f552-4def-b438-6c86ea18b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average amount spent per day as customer\n",
    "data['avg_amt_per_day'] = np.round(data['total_amt'] / data['days_cust'], 4)\n",
    "\n",
    "# Average number of products ordered per day as customer\n",
    "data['avg_product_per_day'] = np.round(data['n_product'] / data['days_cust'], 4)\n",
    "\n",
    "# Average number of orders per day as customer\n",
    "data['avg_order_per_day'] = np.round(data['n_order'] / data['days_cust'], 4)\n",
    "\n",
    "metric_features.extend([\n",
    "    'avg_amt_per_day'\n",
    "    ,'avg_product_per_day'\n",
    "    ,'avg_order_per_day'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f96466-3c89-4523-abc0-6a6ff6a1a27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['cust_age'].isna(), 'cust_age'] = data['cust_age'].mean().astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b41bcb-1d58-4b2a-9c3a-133d068db828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating age buckets\n",
    "data['age_bucket'] = np.where(\n",
    "    data['cust_age'] < 25, '15-24', np.where(\n",
    "        data['cust_age'] < 35, '25-34', np.where(\n",
    "            data['cust_age'] < 45, '35-44', np.where(\n",
    "                data['cust_age'] < 55, '45-54', np.where(\n",
    "                    data['cust_age'] < 65, '55-64', '65+'\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "non_metric_features.insert(4, 'age_bucket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a006109-bf03-4781-9725-c1e1d471ec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['cust_age'].isna(), 'cust_age'] = np.ceil(data['cust_age'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c4aa4c-419c-444a-8c6c-a7f9335aa882",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.topcs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
