{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a090b2-c600-4b59-9459-ec3f46c2dddd",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Imports for this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3f85e7a0-f4af-4a45-b8ea-3f04aaf453b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# PROCESSING\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0b59c8-be03-4d56-98e1-c2f3c8e0d2ee",
   "metadata": {},
   "source": [
    "### Globals\n",
    "\n",
    "List of global variables used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a5d7f495-1526-47cf-ad02-693e0f4663c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'SpectralCluster' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[154], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# MODEL PICKLES\u001b[39;00m\n\u001b[1;32m     10\u001b[0m SPENDING \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../dashboard/models/spending_clustering.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m CITY \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../dashboard/models/city_clustering.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m CUISINE \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../dashboard/models/cuisine_clustering.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m TIME \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../dashboard/models/hour_clustering.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/DataMining/lib/python3.12/site-packages/joblib/numpy_pickle.py:658\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[1;32m    656\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[0;32m--> 658\u001b[0m             obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj, filename, mmap_mode)\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/anaconda3/envs/DataMining/lib/python3.12/site-packages/joblib/numpy_pickle.py:577\u001b[0m, in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    575\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m     obj \u001b[38;5;241m=\u001b[39m unpickler\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unpickler\u001b[38;5;241m.\u001b[39mcompat_mode:\n\u001b[1;32m    579\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been generated with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblib version less than 0.10. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease regenerate this pickle file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m                       \u001b[38;5;241m%\u001b[39m filename,\n\u001b[1;32m    583\u001b[0m                       \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/DataMining/lib/python3.12/pickle.py:1248\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[0;32m-> 1248\u001b[0m         dispatch[key[\u001b[38;5;241m0\u001b[39m]](\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/anaconda3/envs/DataMining/lib/python3.12/pickle.py:1573\u001b[0m, in \u001b[0;36m_Unpickler.load_stack_global\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(name) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(module) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m   1572\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTACK_GLOBAL requires str\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1573\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind_class(module, name))\n",
      "File \u001b[0;32m~/anaconda3/envs/DataMining/lib/python3.12/pickle.py:1617\u001b[0m, in \u001b[0;36m_Unpickler.find_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28m__import__\u001b[39m(module, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m-> 1617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _getattribute(sys\u001b[38;5;241m.\u001b[39mmodules[module], name)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1618\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1619\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(sys\u001b[38;5;241m.\u001b[39mmodules[module], name)\n",
      "File \u001b[0;32m~/anaconda3/envs/DataMining/lib/python3.12/pickle.py:326\u001b[0m, in \u001b[0;36m_getattribute\u001b[0;34m(obj, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, subpath)\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m--> 326\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt get attribute \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    327\u001b[0m                              \u001b[38;5;241m.\u001b[39mformat(name, top)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, parent\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'SpectralCluster' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "# PROCESSING PICKLES\n",
    "SCALER = joblib.load('../dashboard/models/std_scaler.pkl')\n",
    "PCA = joblib.load('../dashboard/models/pca_components.pkl')\n",
    "\n",
    "# INTERMEDIATE MODEL PICKLES\n",
    "MINISOM = joblib.load('../dashboard/models/minisom.pkl')\n",
    "NMF = joblib.load('../dashboard/models/nmf.pkl')\n",
    "\n",
    "# MODEL PICKLES\n",
    "SPENDING = joblib.load('../dashboard/models/spending_clustering.pkl')\n",
    "CITY = joblib.load('../dashboard/models/city_clustering.pkl')\n",
    "CUISINE = joblib.load('../dashboard/models/cuisine_clustering.pkl')\n",
    "TIME = joblib.load('../dashboard/models/hour_clustering.pkl')\n",
    "\n",
    "# VARIABLE LISTS\n",
    "FULl_VAR_LIST = \\\n",
    "[\n",
    "    'customer_id',\n",
    "    'cust_age',\n",
    "    'n_vendor',\n",
    "    'n_product',\n",
    "    'n_chain',\n",
    "    'first_order',\n",
    "    'last_order',\n",
    "    'american',\n",
    "    'asian',\n",
    "    'beverages',\n",
    "    'cafe',\n",
    "    'chicken_dishes',\n",
    "    'chinese',\n",
    "    'desserts',\n",
    "    'healthy',\n",
    "    'indian',\n",
    "    'italian',\n",
    "    'japanese',\n",
    "    'noodle_dishes',\n",
    "    'other',\n",
    "    'street_food_snacks',\n",
    "    'thai',\n",
    "    'DOW_0',\n",
    "    'DOW_1',\n",
    "    'DOW_2',\n",
    "    'DOW_3',\n",
    "    'DOW_4',\n",
    "    'DOW_5',\n",
    "    'DOW_6',\n",
    "    'HR_0',\n",
    "    'HR_1',\n",
    "    'HR_2',\n",
    "    'HR_3',\n",
    "    'HR_4',\n",
    "    'HR_5',\n",
    "    'HR_6',\n",
    "    'HR_7',\n",
    "    'HR_8',\n",
    "    'HR_9',\n",
    "    'HR_10',\n",
    "    'HR_11',\n",
    "    'HR_12',\n",
    "    'HR_13',\n",
    "    'HR_14',\n",
    "    'HR_15',\n",
    "    'HR_16',\n",
    "    'HR_17',\n",
    "    'HR_18',\n",
    "    'HR_19',\n",
    "    'HR_20',\n",
    "    'HR_21',\n",
    "    'HR_22',\n",
    "    'HR_23',\n",
    "    'total_amt',\n",
    "    'n_order',\n",
    "    'avg_amt_per_product',\n",
    "    'avg_amt_per_order',\n",
    "    'avg_amt_per_vendor',\n",
    "    'days_cust',\n",
    "    'avg_days_to_order',\n",
    "    'days_due',\n",
    "    'per_chain_order',\n",
    "    'n_days_week',\n",
    "    'n_times_day',\n",
    "    'n_cuisines',\n",
    "    'log_n_vendor',\n",
    "    'log_n_product',\n",
    "    'log_n_chain',\n",
    "    'log_american',\n",
    "    'log_asian',\n",
    "    'log_beverages',\n",
    "    'log_cafe',\n",
    "    'log_chicken_dishes',\n",
    "    'log_chinese',\n",
    "    'log_desserts',\n",
    "    'log_healthy',\n",
    "    'log_indian',\n",
    "    'log_italian',\n",
    "    'log_japanese',\n",
    "    'log_noodle_dishes',\n",
    "    'log_other',\n",
    "    'log_street_food_snacks',\n",
    "    'log_thai',\n",
    "    'log_total_amt',\n",
    "    'log_n_order',\n",
    "    'log_avg_amt_per_product',\n",
    "    'log_avg_amt_per_order',\n",
    "    'log_avg_amt_per_vendor',\n",
    "    'log_n_days_week',\n",
    "    'log_n_times_day',\n",
    "    'avg_amt_per_day',\n",
    "    'avg_product_per_day',\n",
    "    'avg_order_per_day',\n",
    "    'cust_region_2360.0',\n",
    "    'cust_region_2400.0',\n",
    "    'cust_region_4140.0',\n",
    "    'cust_region_4660.0',\n",
    "    'cust_region_8370.0',\n",
    "    'cust_region_8550.0',\n",
    "    'cust_region_8670.0',\n",
    "    'last_promo_DELIVERY',\n",
    "    'last_promo_DISCOUNT',\n",
    "    'last_promo_FREEBIE',\n",
    "    'last_promo_NO_PROMO',\n",
    "    'pay_method_CARD',\n",
    "    'pay_method_CASH',\n",
    "    'pay_method_DIGI',\n",
    "    'cust_city_2.0',\n",
    "    'cust_city_4.0',\n",
    "    'cust_city_8.0',\n",
    "    'age_bucket_15-24',\n",
    "    'age_bucket_25-34',\n",
    "    'age_bucket_35-44',\n",
    "    'age_bucket_45-54',\n",
    "    'age_bucket_55-64',\n",
    "    'age_bucket_65+',\n",
    "    'regular',\n",
    "    'foodie_flag',\n",
    "    'gluttonous_flag',\n",
    "    'loyal_flag',\n",
    "    'transaction_volume',\n",
    "    'interaction_rate'\n",
    "]\n",
    "\n",
    "OG_LIST = [\n",
    "    'cust_region',\n",
    "    'last_promo',\n",
    "    'pay_method',\n",
    "    'cust_age',   \n",
    "    'n_vendor',\n",
    "    'n_product',\n",
    "    'n_chain',\n",
    "    'first_order',\n",
    "    'last_order',\n",
    "    'american',\n",
    "    'asian',\n",
    "    'beverages'\n",
    "    'cafe',\n",
    "    'chicken_dishes',\n",
    "    'chinese',\n",
    "    'desserts'\n",
    "    'healthy',\n",
    "    'indian',\n",
    "    'italian',\n",
    "    'japanese',\n",
    "    'noodle_dishes',\n",
    "    'other',\n",
    "    'street_food_snacks',\n",
    "    'thai',\n",
    "    'DOW_0',\n",
    "    'DOW_1',\n",
    "    'DOW_2',\n",
    "    'DOW_3',\n",
    "    'DOW_4',\n",
    "    'DOW_5',\n",
    "    'DOW_6',\n",
    "    'HR_0',\n",
    "    'HR_1',\n",
    "    'HR_2',\n",
    "    'HR_3',\n",
    "    'HR_4',\n",
    "    'HR_5',\n",
    "    'HR_6',\n",
    "    'HR_7',\n",
    "    'HR_8',\n",
    "    'HR_9',\n",
    "    'HR_10',\n",
    "    'HR_11',\n",
    "    'HR_12',\n",
    "    'HR_13',\n",
    "    'HR_14',\n",
    "    'HR_15',\n",
    "    'HR_16',\n",
    "    'HR_17',\n",
    "    'HR_18',\n",
    "    'HR_19',\n",
    "    'HR_20',\n",
    "    'HR_21',\n",
    "    'HR_22',\n",
    "    'HR_23'   \n",
    "]\n",
    "\n",
    "NON_METRIC_KEYS = OG_LIST[:3]  \n",
    "METRIC_KEYS = OG_LIST[3:]      \n",
    "CUISINE_KEYS = OG_LIST[9:22]\n",
    "\n",
    "LOGS = \\\n",
    "[\n",
    "   'n_vendor',\n",
    "    'n_product',\n",
    "    'n_chain',\n",
    "    'american',\n",
    "    'asian',\n",
    "    'beverages',\n",
    "    'cafe',\n",
    "    'chicken_dishes',\n",
    "    'chinese',\n",
    "    'desserts',\n",
    "    'healthy',\n",
    "    'indian',\n",
    "    'italian',\n",
    "    'japanese',\n",
    "    'noodle_dishes',\n",
    "    'other',\n",
    "    'street_food_snacks',\n",
    "    'thai',\n",
    "    'total_amt',\n",
    "    'n_order',\n",
    "    'avg_amt_per_product',\n",
    "    'avg_amt_per_order',\n",
    "    'avg_amt_per_vendor',\n",
    "    'n_days_week',\n",
    "    'n_times_day'\n",
    "]\n",
    "\n",
    "PCA_FEATURES = \\\n",
    "[\n",
    "    'avg_amt_per_day', \n",
    "    'avg_product_per_day', \n",
    "    'avg_order_per_day', \n",
    "    'n_product', \n",
    "    'n_order'\n",
    "]\n",
    "\n",
    "SCALER_FEATURES = [\n",
    "    'cust_age', 'n_vendor', 'n_product', 'n_chain', 'first_order', 'last_order', \n",
    "    'american', 'asian', 'beverages', 'cafe', 'chicken_dishes', 'chinese', \n",
    "    'desserts', 'healthy', 'indian', 'italian', 'japanese', 'noodle_dishes', \n",
    "    'other', 'street_food_snacks', 'thai',\n",
    "    'DOW_0', 'DOW_1', 'DOW_2', 'DOW_3', 'DOW_4', 'DOW_5', 'DOW_6', \n",
    "    'HR_0', 'HR_1', 'HR_2', 'HR_3', 'HR_4', 'HR_5', 'HR_6', 'HR_7', 'HR_8', \n",
    "    'HR_9', 'HR_10', 'HR_11', 'HR_12', 'HR_13', 'HR_14', 'HR_15', 'HR_16', \n",
    "    'HR_17', 'HR_18', 'HR_19', 'HR_20', 'HR_21', 'HR_22', 'HR_23',\n",
    "    'total_amt', 'n_order', 'avg_amt_per_product', 'avg_amt_per_order', \n",
    "    'avg_amt_per_vendor', 'days_cust', 'avg_days_to_order', 'days_due', \n",
    "    'per_chain_order', 'n_days_week', 'n_times_day', 'n_cuisines', \n",
    "    'log_n_vendor', 'log_n_product', 'log_n_chain', 'log_american', \n",
    "    'log_asian', 'log_beverages', 'log_cafe', 'log_chicken_dishes', \n",
    "    'log_chinese', 'log_desserts', 'log_healthy', 'log_indian', 'log_italian', \n",
    "    'log_japanese', 'log_noodle_dishes', 'log_other', 'log_street_food_snacks', \n",
    "    'log_thai', 'log_total_amt', 'log_n_order', 'log_avg_amt_per_product', \n",
    "    'log_avg_amt_per_order', 'log_avg_amt_per_vendor', 'log_n_days_week', \n",
    "    'log_n_times_day', 'avg_amt_per_day', 'avg_product_per_day', \n",
    "    'avg_order_per_day'\n",
    "]\n",
    "\n",
    "# FEATURE DICTIONARIES\n",
    "MEANS = \\\n",
    "{\n",
    "    'cust_age': 27.505,\n",
    "    'first_order': 23.081,\n",
    "    'last_order': 68.927,\n",
    "}\n",
    "\n",
    "TIME_LIKELYHOODS = \\\n",
    "{ \n",
    "    'DAY':\n",
    "    {\n",
    "        'DOW_0': 0.638,\n",
    "        'DOW_1': 0.65,\n",
    "        'DOW_2': 0.679,\n",
    "        'DOW_3': 0.71,\n",
    "        'DOW_4': 0.777,\n",
    "        'DOW_5': 0.746,\n",
    "        'DOW_6': 0.808        \n",
    "    },\n",
    "\n",
    "    'HOUR':\n",
    "    {\n",
    "        'HR_0': 0.053,\n",
    "        'HR_1': 0.06,\n",
    "        'HR_2': 0.07,\n",
    "        'HR_3': 0.136,\n",
    "        'HR_4': 0.114,\n",
    "        'HR_5': 0.094,\n",
    "        'HR_6': 0.078,\n",
    "        'HR_7': 0.084,\n",
    "        'HR_8': 0.142,\n",
    "        'HR_9': 0.263,\n",
    "        'HR_10': 0.374,\n",
    "        'HR_11': 0.436,\n",
    "        'HR_12': 0.369,\n",
    "        'HR_13': 0.276,\n",
    "        'HR_14': 0.247,\n",
    "        'HR_15': 0.318,\n",
    "        'HR_16': 0.414,\n",
    "        'HR_17': 0.452,\n",
    "        'HR_18': 0.391,\n",
    "        'HR_19': 0.287,\n",
    "        'HR_20': 0.166,\n",
    "        'HR_21': 0.083,\n",
    "        'HR_22': 0.053,\n",
    "        'HR_23': 0.051   \n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize dictionaries for feature groups with flags and relevant columns\n",
    "FEATURE_GROUPS = {\n",
    "    'foodie': ['n_vendor', 'n_product', 'n_order', 'n_cuisines'],\n",
    "    'gluttonous': ['avg_amt_per_order', 'total_amt', 'n_chain'],\n",
    "    'loyal': ['avg_amt_per_vendor'] + CUISINE_KEYS\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8545301e-71c7-4c39-85a9-f5a0cbb43b65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Helpers\n",
    "\n",
    "Helpers for some of the routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ae213986-df0a-4c70-a88e-c71c255f953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the largest value in a dataframe\n",
    "\n",
    "def top_n(row, col_list, n):\n",
    "    # Sort the specified columns in descending order\n",
    "    sorted_row = row[col_list].sort_values(ascending=False)\n",
    "\n",
    "    # Get the unique sorted values\n",
    "    unique_sorted_values = sorted_row.unique()\n",
    "\n",
    "    # Ensure there are enough unique values to determine the n-th largest\n",
    "    if len(unique_sorted_values) >= n:\n",
    "        nth_value = unique_sorted_values[n - 1]  # Get the n-th largest unique value\n",
    "\n",
    "        # If the n-th value is 0, return None\n",
    "        if nth_value == 0:\n",
    "            return None\n",
    "        \n",
    "        # If n > 1, check for uniqueness against the (n-1)-th largest\n",
    "        if n > 1:\n",
    "            prev_value = unique_sorted_values[n - 2]  # (n-1)-th largest unique value\n",
    "            # If nth_value is equal to the (n-1)-th value, we don't want to return it\n",
    "            if nth_value == prev_value:\n",
    "                return None\n",
    "        \n",
    "        # Return the index of the n-th largest value\n",
    "        return sorted_row[sorted_row == nth_value].index[0]\n",
    "\n",
    "    # Return None if conditions are not met\n",
    "    return None\n",
    "\n",
    "# Throws a dice based on given n_choices and probabilities.\n",
    "def throw_dice(likelihood_dict):\n",
    "    # Unzip the dictionary into choices and probabilities\n",
    "    choices, probabilities = zip(*likelihood_dict.items())\n",
    "    \n",
    "    # Normalize probabilities\n",
    "    probabilities = np.array(probabilities) / np.sum(probabilities)\n",
    "    \n",
    "    # Return a random choice based on the probabilities\n",
    "    return np.random.choice(choices, p=probabilities)\n",
    "\n",
    "# Function to calculate IQR bounds\n",
    "def calculate_bounds(series):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0430fa0e-9970-40ca-b09e-dfb3d577a086",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Preprocess\n",
    "\n",
    "Wrangle the user input data, before it is ready for variable engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ca4882de-1683-4d7a-b2eb-fe7fc01c6080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(raw_data_point):\n",
    "    # Make a copy of the data point\n",
    "    data = raw_data_point.copy()\n",
    "    \n",
    "    # Enforce datatypes for metric elements\n",
    "    for key in METRIC_KEYS:\n",
    "        try:\n",
    "            data[key] = np.float64(data[key])\n",
    "            \n",
    "            # Check if the value is less than 0\n",
    "            if data[key] < 0:\n",
    "                raise ValueError\n",
    "        except (ValueError, KeyError) as e:\n",
    "            data[key] = np.nan\n",
    "    \n",
    "    # Enforce string types for non-metric elements\n",
    "    for key in NON_METRIC_KEYS:\n",
    "        try:\n",
    "            if not isinstance(data[key], str):\n",
    "                data[key] = np.nan\n",
    "        except KeyError:\n",
    "            data[key] = np.nan\n",
    "            \n",
    "    # Initialize sums\n",
    "    n_week = 0\n",
    "    n_day = 0\n",
    "    \n",
    "    # Calculate n_week and n_day directly, filling missing values with 0\n",
    "    for n in range(7):\n",
    "        dow_key = f\"DOW_{n}\"\n",
    "        if pd.isna(data[dow_key]):\n",
    "            data[dow_key] = 0\n",
    "        n_week += data[dow_key]\n",
    "    \n",
    "    for n in range(24):\n",
    "        hr_key = f\"HR_{n}\"\n",
    "        if pd.isna(data[hr_key]):\n",
    "            data[hr_key] = 0\n",
    "        n_day += data[hr_key]\n",
    "\n",
    "    diff = int(np.ceil(n_week - n_day))\n",
    "\n",
    "    # Correct if not equal\n",
    "    likelyhood = None\n",
    "    var = None\n",
    "    if diff < 0:\n",
    "        likelyhood = TIME_LIKELYHOODS['DAY']\n",
    "        n_day += diff\n",
    "    elif diff > 0:\n",
    "        likelyhood = TIME_LIKELYHOODS['HOUR']\n",
    "        n_week += diff\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if diff != 0:\n",
    "        for _ in range(diff):\n",
    "            data[throw_dice(likelyhood)] += 1\n",
    "    \n",
    "    # Finally, set n_order equal to the sum of either \n",
    "    n_order = n_week \n",
    "\n",
    "    # Fill missing amounts in cuisines with 0\n",
    "    for key in CUISINE_KEYS:\n",
    "        if pd.isna(data[key]):\n",
    "            data[key] = 0\n",
    "            \n",
    "    # Calculate number of cuisines ordered\n",
    "    n_cuisines = sum(\n",
    "        (data[key] > 0) \n",
    "        for key in CUISINE_KEYS \n",
    "    )\n",
    "\n",
    "    # Check if customer to segment has spent any money\n",
    "    if n_cuisines == 0:\n",
    "        return \"Error: Invalid customer. Specify customer ammounts spent per cuisine.\"        \n",
    "       \n",
    "    # Check if number of cuisines ordered is larger than the number of orders made\n",
    "    diff = int(np.ceil(n_cuisines > n_order))\n",
    "    \n",
    "    if diff > 0:\n",
    "        for _ in range(diff):\n",
    "            # Assign another HR stochastically\n",
    "            data[throw_dice(TIME_LIKELYHOODS['HOUR'])] += 1\n",
    "            n_day += 1\n",
    "            \n",
    "            # Assign another DAY stochastically\n",
    "            data[throw_dice(TIME_LIKELYHOODS['DAY'])] += 1\n",
    "            n_order += 1\n",
    "            n_week += 1\n",
    "    # Sum total cuisines to obtain the total amount spent\n",
    "    total_amt = sum(\n",
    "        data[key] for key in CUISINE_KEYS\n",
    "    )\n",
    "\n",
    "    data['n_order'] = n_order\n",
    "    data['n_times_day'] = n_day\n",
    "    data['n_days_week'] = n_week\n",
    "    data['n_cuisines'] = n_cuisines\n",
    "    data['total_amt'] = total_amt\n",
    "    \n",
    "    try:\n",
    "        # Assuming each vendor only serves one type of cuisine, which might introduce some bias, but how else to solve?\n",
    "        if data['n_vendor'] < n_cuisines:\n",
    "            data['n_vendor'] = n_cuisines\n",
    "    except ValueError:\n",
    "        data['n_vendor'] = n_cuisines\n",
    "\n",
    "    try:\n",
    "        if pd.isna(data['n_chain']): \n",
    "            raise ValueError\n",
    "        # Check if the customer made a consistent number of purchases from chained restaurants \n",
    "        if data['n_chain'] > data['n_vendor']:\n",
    "            data['n_chain'] = data['n_vendor']\n",
    "\n",
    "        # Check if the customer made an illegal number of purchases from chained restaurants\n",
    "        if data['n_chain'] < 0: \n",
    "            data['n_chain'] = 0\n",
    "            \n",
    "    except ValueError:\n",
    "        # If here then definitely illegal\n",
    "        data['n_chain'] = 0\n",
    "    \n",
    "    try:\n",
    "        # Check if the number of products is at least equal to the number of orders made\n",
    "        if data['n_product'] < data['n_order']:\n",
    "            data['n_product'] = data['n_order']\n",
    "    except ValueError:\n",
    "        data['n_product'] = data['n_order']\n",
    "\n",
    "    # Fill in low risk values with known dataset means.\n",
    "    if pd.isna(data['first_order']):\n",
    "        data['first_order'] = MEANS['first_order']\n",
    "        \n",
    "    if pd.isna(data['last_order']):\n",
    "        data['last_order'] = MEANS['last_order']\n",
    "\n",
    "    if pd.isna(data['last_order']):\n",
    "        data['cust_age'] = MEANS['cust_age']\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacf6348-8529-4105-a7e9-e1dee0dd4e5c",
   "metadata": {},
   "source": [
    "### Process\n",
    "\n",
    "Process the data so that it follows the same data structure as the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "08e8f675-a938-4915-b001-6fe458ced15a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3664856685.py, line 74)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[136], line 74\u001b[0;36m\u001b[0m\n\u001b[0;31m    PICKLE.\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def process(preprocessed_data):\n",
    "    # Make a copy of the data point\n",
    "    data = preprocessed_data.copy()\n",
    "\n",
    "    # Amount spent on average per product\n",
    "    data['avg_amt_per_product'] = data['total_amt'] / data['n_product'] if data['n_product'] > 0 else 0\n",
    "\n",
    "    # Amount spent on average per order\n",
    "    data['avg_amt_per_order'] = data['total_amt'] / data['n_order'] if data['n_order'] > 0 else 0\n",
    "\n",
    "    # Amount spent on average per vendor\n",
    "    data['avg_amt_per_vendor'] = data['total_amt'] / data['n_vendor'] if data['n_vendor'] > 0 else 0\n",
    "\n",
    "    # Total days as customer\n",
    "    data['days_cust'] = data['last_order'] - data['first_order']\n",
    "\n",
    "    # Average days between orders\n",
    "    data['avg_days_to_order'] = data['days_cust'] / data['n_order'] if data['n_order'] > 0 else 0\n",
    "\n",
    "    # Days the customer is due, according to their average days between orders\n",
    "    data['days_due'] = 90 - data['last_order'] + data['avg_days_to_order']\n",
    "\n",
    "    # Percentage of orders placed to restaurants that are part of a chain\n",
    "    data['per_chain_order'] = data['n_chain'] / data['n_order'] if data['n_order'] > 0 else 0\n",
    "\n",
    "    # Average amount spent per day as customer\n",
    "    data['avg_amt_per_day'] = round(data['total_amt'] / data['days_cust'], 4) if data['days_cust'] > 0 else 0\n",
    "\n",
    "    # Average number of products ordered per day as customer\n",
    "    data['avg_product_per_day'] = round(data['n_product'] / data['days_cust'], 4) if data['days_cust'] > 0 else 0\n",
    "\n",
    "    # Average number of orders per day as customer\n",
    "    data['avg_order_per_day'] = round(data['n_order'] / data['days_cust'], 4) if data['days_cust'] > 0 else 0\n",
    "\n",
    "    # Apply log1p to each column in LOGS and add it to log_transformed with the prefix 'log_'\n",
    "    for col in LOGS:\n",
    "        if col in data and data[col] > 0:\n",
    "            data[f\"log_{col}\"] = np.log1p(data[col])\n",
    "        else:\n",
    "            data[f\"log_{col}\"] = 0\n",
    "\n",
    "    # Creating age buckets\n",
    "    data['age_bucket'] = (\n",
    "        '15-24' if data['cust_age'] < 25 else\n",
    "        '25-34' if data['cust_age'] < 35 else\n",
    "        '35-44' if data['cust_age'] < 45 else\n",
    "        '45-54' if data['cust_age'] < 55 else\n",
    "        '55-64' if data['cust_age'] < 65 else\n",
    "        '65+'\n",
    "    )\n",
    "\n",
    "    # Flag customers who have purchased in more than one day\n",
    "    data['regular'] = data['days_cust'] > 1\n",
    "\n",
    "    # Create columns to hold the flags for each feature group\n",
    "    data['foodie_flag'] = 0\n",
    "    data['gluttonous_flag'] = 0\n",
    "    data['loyal_flag'] = 0\n",
    "\n",
    "    # Assign flags for each feature group\n",
    "    for group, features in FEATURE_GROUPS.items():\n",
    "        for feature in features:\n",
    "            log_feature = f\"log_{feature}\" if feature != 'n_cuisines' else feature\n",
    "            if data[log_feature] > 0:\n",
    "                lower_bound, upper_bound = calculate_bounds([data[log_feature]])\n",
    "                # Mark outliers for each group\n",
    "                if group == 'foodie':\n",
    "                    data['foodie_flag'] |= int(data[log_feature] > upper_bound)\n",
    "                elif group == 'gluttonous':\n",
    "                    data['gluttonous_flag'] |= int(data[log_feature] > upper_bound)\n",
    "                elif group == 'loyal':\n",
    "                    data['loyal_flag'] |= int(data[log_feature] > upper_bound)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb893d86-b202-4d1f-84dc-50c432ecc539",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "Scalling the data, so that it is on the same scaling assd the models, and can be used in predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8b35d8c7-66d6-452c-b166-fdfe414032d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(processed_data):\n",
    "    # Create a copy of the input data to avoid modifying the original dataset\n",
    "    data = processed_data.copy()\n",
    "\n",
    "    # Convert data to a DataFrame\n",
    "    data = pd.DataFrame(data, columns=processed_data.columns) \n",
    "    \n",
    "    # Identify features not in SCALER_FEATURES\n",
    "    other_features = [col for col in data.columns if col not in SCALER_FEATURES]\n",
    "    \n",
    "    # Step 1: Scale the SCALER_FEATURES\n",
    "    scaled_data = SCALER.transform(data[SCALER_FEATURES]\n",
    "    missing_columns = [col for col in SCALER_FEATURES if col not in data.columns]\n",
    "    print(\"Missing columns:\", missing_columns)\n",
    "    \n",
    "    # Create a DataFrame for the scaled features\n",
    "    scaled_df = pd.DataFrame(scaled_data, columns=SCALER_FEATURES, index=data.index)\n",
    "    \n",
    "    # Concatenate the scaled features with the unscaled features\n",
    "    data = pd.concat([scaled_df, data[other_features]], axis=1)\n",
    "\n",
    "    # Step 2: Apply PCA transformation to the specified features\n",
    "    pca_data = PCA.transform(data[PCA_FEATURES])\n",
    "    \n",
    "    # Create a DataFrame for the PCA-transformed features\n",
    "    pca_df = pd.DataFrame(pca_data, columns=[f'PC{i}' for i in range(pca_data.shape[1])], index=data.index)\n",
    "    \n",
    "    # Step 3: Drop 'PC2' and rename 'PC0' and 'PC1'\n",
    "    pca_df.drop(columns='PC2', inplace=True, errors='ignore')  # Ignore if 'PC2' is not found\n",
    "    pca_df.rename(columns={'PC0': 'transaction_volume', 'PC1': 'interaction_rate'}, inplace=True)\n",
    "    \n",
    "    # Step 4: Concatenate the PCA-transformed features with the rest of the data\n",
    "    data = pd.concat([pca_df, data.drop(columns=PCA_FEATURES)], axis=1)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a824586-217f-49a5-ae7a-ff7fe055b206",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "aff41986-2a7c-464e-8b68-0ed88a70e307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_volume</th>\n",
       "      <th>interaction_rate</th>\n",
       "      <th>cust_age</th>\n",
       "      <th>n_vendor</th>\n",
       "      <th>n_chain</th>\n",
       "      <th>first_order</th>\n",
       "      <th>last_order</th>\n",
       "      <th>american</th>\n",
       "      <th>asian</th>\n",
       "      <th>beverages</th>\n",
       "      <th>...</th>\n",
       "      <th>age_bucket_45-54</th>\n",
       "      <th>age_bucket_55-64</th>\n",
       "      <th>age_bucket_65+</th>\n",
       "      <th>regular</th>\n",
       "      <th>foodie_flag</th>\n",
       "      <th>gluttonous_flag</th>\n",
       "      <th>loyal_flag</th>\n",
       "      <th>top_cuisine</th>\n",
       "      <th>transaction_volume</th>\n",
       "      <th>interaction_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.898381</td>\n",
       "      <td>-8.932696</td>\n",
       "      <td>-3.898937</td>\n",
       "      <td>-1.629598</td>\n",
       "      <td>-1.089055</td>\n",
       "      <td>-1.187410</td>\n",
       "      <td>-3.813539</td>\n",
       "      <td>-0.529285</td>\n",
       "      <td>-0.485634</td>\n",
       "      <td>-0.358491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>indian</td>\n",
       "      <td>9.861899</td>\n",
       "      <td>-3.578223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.171226</td>\n",
       "      <td>-11.783180</td>\n",
       "      <td>-4.038629</td>\n",
       "      <td>-1.629598</td>\n",
       "      <td>-1.164803</td>\n",
       "      <td>-1.187410</td>\n",
       "      <td>-3.813539</td>\n",
       "      <td>-0.420625</td>\n",
       "      <td>-0.434944</td>\n",
       "      <td>-0.358491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>asian</td>\n",
       "      <td>15.648610</td>\n",
       "      <td>-5.018780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.600708</td>\n",
       "      <td>-7.633377</td>\n",
       "      <td>-3.639510</td>\n",
       "      <td>-1.629598</td>\n",
       "      <td>-1.164803</td>\n",
       "      <td>-1.187410</td>\n",
       "      <td>-3.813539</td>\n",
       "      <td>-0.529285</td>\n",
       "      <td>-0.464701</td>\n",
       "      <td>-0.358491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>asian</td>\n",
       "      <td>7.659009</td>\n",
       "      <td>-3.114611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.481555</td>\n",
       "      <td>-7.217356</td>\n",
       "      <td>-3.958805</td>\n",
       "      <td>-1.629598</td>\n",
       "      <td>-1.013307</td>\n",
       "      <td>-1.187410</td>\n",
       "      <td>-3.813539</td>\n",
       "      <td>-0.485433</td>\n",
       "      <td>-0.511194</td>\n",
       "      <td>-0.334214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>american</td>\n",
       "      <td>5.377867</td>\n",
       "      <td>-2.266646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.189194</td>\n",
       "      <td>-9.040799</td>\n",
       "      <td>-3.898937</td>\n",
       "      <td>-1.629598</td>\n",
       "      <td>-1.013307</td>\n",
       "      <td>-1.187410</td>\n",
       "      <td>-3.813539</td>\n",
       "      <td>-0.441954</td>\n",
       "      <td>-0.511194</td>\n",
       "      <td>0.028672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>beverages</td>\n",
       "      <td>10.454657</td>\n",
       "      <td>-3.798568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23429</th>\n",
       "      <td>32.119256</td>\n",
       "      <td>-8.643073</td>\n",
       "      <td>-3.998717</td>\n",
       "      <td>-1.629598</td>\n",
       "      <td>-1.089055</td>\n",
       "      <td>-0.976951</td>\n",
       "      <td>-3.572919</td>\n",
       "      <td>-0.501990</td>\n",
       "      <td>-0.511194</td>\n",
       "      <td>-0.358491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>other</td>\n",
       "      <td>8.273828</td>\n",
       "      <td>-2.987892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23430</th>\n",
       "      <td>26.150107</td>\n",
       "      <td>-7.465875</td>\n",
       "      <td>-3.878981</td>\n",
       "      <td>-1.774272</td>\n",
       "      <td>-1.013307</td>\n",
       "      <td>-0.976951</td>\n",
       "      <td>-3.572919</td>\n",
       "      <td>-0.529285</td>\n",
       "      <td>-0.477854</td>\n",
       "      <td>-0.358491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>asian</td>\n",
       "      <td>6.740559</td>\n",
       "      <td>-2.773197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23431</th>\n",
       "      <td>26.545741</td>\n",
       "      <td>-7.612944</td>\n",
       "      <td>-3.998717</td>\n",
       "      <td>-1.629598</td>\n",
       "      <td>-1.013307</td>\n",
       "      <td>-0.976951</td>\n",
       "      <td>-3.572919</td>\n",
       "      <td>-0.529285</td>\n",
       "      <td>-0.511194</td>\n",
       "      <td>-0.358491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>desserts</td>\n",
       "      <td>7.546971</td>\n",
       "      <td>-3.072963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23432</th>\n",
       "      <td>26.194208</td>\n",
       "      <td>-7.482269</td>\n",
       "      <td>-3.539731</td>\n",
       "      <td>-1.629598</td>\n",
       "      <td>-1.089055</td>\n",
       "      <td>-0.976951</td>\n",
       "      <td>-3.572919</td>\n",
       "      <td>-0.483494</td>\n",
       "      <td>-0.488022</td>\n",
       "      <td>-0.358491</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>asian</td>\n",
       "      <td>6.830450</td>\n",
       "      <td>-2.806612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23433</th>\n",
       "      <td>37.877341</td>\n",
       "      <td>-9.658829</td>\n",
       "      <td>-3.918893</td>\n",
       "      <td>-1.629598</td>\n",
       "      <td>-0.937559</td>\n",
       "      <td>-0.976951</td>\n",
       "      <td>-3.570153</td>\n",
       "      <td>-0.529285</td>\n",
       "      <td>-0.511194</td>\n",
       "      <td>-0.358491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>other</td>\n",
       "      <td>9.438912</td>\n",
       "      <td>-2.638248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23434 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       transaction_volume  interaction_rate  cust_age  n_vendor   n_chain  \\\n",
       "0               32.898381         -8.932696 -3.898937 -1.629598 -1.089055   \n",
       "1               46.171226        -11.783180 -4.038629 -1.629598 -1.164803   \n",
       "2               26.600708         -7.633377 -3.639510 -1.629598 -1.164803   \n",
       "3               25.481555         -7.217356 -3.958805 -1.629598 -1.013307   \n",
       "4               33.189194         -9.040799 -3.898937 -1.629598 -1.013307   \n",
       "...                   ...               ...       ...       ...       ...   \n",
       "23429           32.119256         -8.643073 -3.998717 -1.629598 -1.089055   \n",
       "23430           26.150107         -7.465875 -3.878981 -1.774272 -1.013307   \n",
       "23431           26.545741         -7.612944 -3.998717 -1.629598 -1.013307   \n",
       "23432           26.194208         -7.482269 -3.539731 -1.629598 -1.089055   \n",
       "23433           37.877341         -9.658829 -3.918893 -1.629598 -0.937559   \n",
       "\n",
       "       first_order  last_order  american     asian  beverages  ...  \\\n",
       "0        -1.187410   -3.813539 -0.529285 -0.485634  -0.358491  ...   \n",
       "1        -1.187410   -3.813539 -0.420625 -0.434944  -0.358491  ...   \n",
       "2        -1.187410   -3.813539 -0.529285 -0.464701  -0.358491  ...   \n",
       "3        -1.187410   -3.813539 -0.485433 -0.511194  -0.334214  ...   \n",
       "4        -1.187410   -3.813539 -0.441954 -0.511194   0.028672  ...   \n",
       "...            ...         ...       ...       ...        ...  ...   \n",
       "23429    -0.976951   -3.572919 -0.501990 -0.511194  -0.358491  ...   \n",
       "23430    -0.976951   -3.572919 -0.529285 -0.477854  -0.358491  ...   \n",
       "23431    -0.976951   -3.572919 -0.529285 -0.511194  -0.358491  ...   \n",
       "23432    -0.976951   -3.572919 -0.483494 -0.488022  -0.358491  ...   \n",
       "23433    -0.976951   -3.570153 -0.529285 -0.511194  -0.358491  ...   \n",
       "\n",
       "       age_bucket_45-54  age_bucket_55-64  age_bucket_65+  regular  \\\n",
       "0                   0.0               0.0             0.0     True   \n",
       "1                   0.0               0.0             0.0     True   \n",
       "2                   0.0               0.0             0.0     True   \n",
       "3                   0.0               0.0             0.0     True   \n",
       "4                   0.0               0.0             0.0     True   \n",
       "...                 ...               ...             ...      ...   \n",
       "23429               0.0               0.0             0.0     True   \n",
       "23430               0.0               0.0             0.0     True   \n",
       "23431               0.0               0.0             0.0     True   \n",
       "23432               1.0               0.0             0.0     True   \n",
       "23433               0.0               0.0             0.0     True   \n",
       "\n",
       "       foodie_flag  gluttonous_flag  loyal_flag  top_cuisine  \\\n",
       "0                0                0           0       indian   \n",
       "1                0                0           0        asian   \n",
       "2                0                0           0        asian   \n",
       "3                0                0           0     american   \n",
       "4                0                0           0    beverages   \n",
       "...            ...              ...         ...          ...   \n",
       "23429            0                0           0        other   \n",
       "23430            0                0           0        asian   \n",
       "23431            0                0           0     desserts   \n",
       "23432            0                0           0        asian   \n",
       "23433            0                0           0        other   \n",
       "\n",
       "       transaction_volume  interaction_rate  \n",
       "0                9.861899         -3.578223  \n",
       "1               15.648610         -5.018780  \n",
       "2                7.659009         -3.114611  \n",
       "3                5.377867         -2.266646  \n",
       "4               10.454657         -3.798568  \n",
       "...                   ...               ...  \n",
       "23429            8.273828         -2.987892  \n",
       "23430            6.740559         -2.773197  \n",
       "23431            7.546971         -3.072963  \n",
       "23432            6.830450         -2.806612  \n",
       "23433            9.438912         -2.638248  \n",
       "\n",
       "[23434 rows x 120 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(scaled_point)\n",
    "    data = scaled_point.copy()\n",
    "\n",
    "    MINISOM.\n",
    "# INTERMEDIATE MODEL PICKLES\n",
    "MINISOM = joblib.load('../dashboard/models/minisom.pkl')\n",
    "NMF = joblib.load('../dashboard/models/nmf.pkl')\n",
    "\n",
    "# MODEL PICKLES\n",
    "SPENDING = joblib.load('../dashboard/models/spending_clustering.pkl')\n",
    "CITY = joblib.load('../dashboard/models/city_clustering.pkl')\n",
    "CUISINE = joblib.load('../dashboard/models/cuisine_clustering.pkl')\n",
    "TIME = joblib.load('../dashboard/models/hour_clustering.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d832dfb-2486-4b68-8ef6-a4ffe0f30691",
   "metadata": {},
   "source": [
    "## Run\n",
    "\n",
    "Self explanatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed64eef1-68a3-4e4b-8960-481de93752f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(data_point: dict) -> dict:\n",
    "    if not isinstance(data_point, dict):\n",
    "        raise TypeError(\"The input must be a dictionary.\")\n",
    "\n",
    "    # Workflow\n",
    "    \n",
    "    # Preprocess\n",
    "    preproc_point = preproc(data_point)\n",
    "    # Process\n",
    "    processed_point = process(preproc_point)\n",
    "    # Scale\n",
    "    scaled_point = scale(processed_point)\n",
    "    # Predict\n",
    "    predicted_point = predict(scaled_point)\n",
    "    \n",
    "    return data_point"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
