{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f85e7a0-f4af-4a45-b8ea-3f04aaf453b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "764ec4a9-5726-4382-b7c4-73bdb678ba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "PATH = '../dashboard/data/'\n",
    "\n",
    "FILENAME = 'raw_data.csv'\n",
    "raw_data_point = pd.read_csv(f'{PATH}{FILENAME}')\n",
    "\n",
    "SCALER = joblib.load('models/std_scaler.pkl')\n",
    "ENCODER = joblib.load('models/hot_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a5d7f495-1526-47cf-ad02-693e0f4663c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OG_LIST = [\n",
    "    'cust_region',\n",
    "    'last_promo',\n",
    "    'pay_method',\n",
    "    'cust_age',   \n",
    "    'n_vendor',\n",
    "    'n_product',\n",
    "    'n_chain',\n",
    "    'first_order',\n",
    "    'last_order',\n",
    "    'american',\n",
    "    'asian',\n",
    "    'beverages'\n",
    "    'cafe',\n",
    "    'chicken_dishes',\n",
    "    'chinese',\n",
    "    'desserts'\n",
    "    'healthy',\n",
    "    'indian',\n",
    "    'italian',\n",
    "    'japanese',\n",
    "    'noodle_dishes',\n",
    "    'other',\n",
    "    'street_food_snacks',\n",
    "    'thai',\n",
    "    'DOW_0',\n",
    "    'DOW_1',\n",
    "    'DOW_2',\n",
    "    'DOW_3',\n",
    "    'DOW_4',\n",
    "    'DOW_5',\n",
    "    'DOW_6',\n",
    "    'HR_0',\n",
    "    'HR_1',\n",
    "    'HR_2',\n",
    "    'HR_3',\n",
    "    'HR_4',\n",
    "    'HR_5',\n",
    "    'HR_6',\n",
    "    'HR_7',\n",
    "    'HR_8',\n",
    "    'HR_9',\n",
    "    'HR_10',\n",
    "    'HR_11',\n",
    "    'HR_12',\n",
    "    'HR_13',\n",
    "    'HR_14',\n",
    "    'HR_15',\n",
    "    'HR_16',\n",
    "    'HR_17',\n",
    "    'HR_18',\n",
    "    'HR_19',\n",
    "    'HR_20',\n",
    "    'HR_21',\n",
    "    'HR_22',\n",
    "    'HR_23'   \n",
    "]\n",
    "\n",
    "NON_METRIC_KEYS = OG_LIST[:3]  \n",
    "METRIC_KEYS = OG_LIST[3:]      \n",
    "CUISINE_KEYS = OG_LIST[9:22]\n",
    "\n",
    "MEANS = \\\n",
    "{\n",
    "    'cust_age': 27.505,\n",
    "    'first_order': 23.081,\n",
    "    'last_order': 68.927,\n",
    "}\n",
    "\n",
    "TIME_LIKELYHOODS = \\\n",
    "{ \n",
    "    'DAY':\n",
    "    {\n",
    "        'DOW_0': 0.638,\n",
    "        'DOW_1': 0.65,\n",
    "        'DOW_2': 0.679,\n",
    "        'DOW_3': 0.71,\n",
    "        'DOW_4': 0.777,\n",
    "        'DOW_5': 0.746,\n",
    "        'DOW_6': 0.808        \n",
    "    },\n",
    "\n",
    "    'HOUR':\n",
    "    {\n",
    "        'HR_0': 0.053,\n",
    "        'HR_1': 0.06,\n",
    "        'HR_2': 0.07,\n",
    "        'HR_3': 0.136,\n",
    "        'HR_4': 0.114,\n",
    "        'HR_5': 0.094,\n",
    "        'HR_6': 0.078,\n",
    "        'HR_7': 0.084,\n",
    "        'HR_8': 0.142,\n",
    "        'HR_9': 0.263,\n",
    "        'HR_10': 0.374,\n",
    "        'HR_11': 0.436,\n",
    "        'HR_12': 0.369,\n",
    "        'HR_13': 0.276,\n",
    "        'HR_14': 0.247,\n",
    "        'HR_15': 0.318,\n",
    "        'HR_16': 0.414,\n",
    "        'HR_17': 0.452,\n",
    "        'HR_18': 0.391,\n",
    "        'HR_19': 0.287,\n",
    "        'HR_20': 0.166,\n",
    "        'HR_21': 0.083,\n",
    "        'HR_22': 0.053,\n",
    "        'HR_23': 0.051   \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ae213986-df0a-4c70-a88e-c71c255f953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "def top_n(row, col_list, n):\n",
    "    # Sort the specified columns in descending order\n",
    "    sorted_row = row[col_list].sort_values(ascending=False)\n",
    "\n",
    "    # Get the unique sorted values\n",
    "    unique_sorted_values = sorted_row.unique()\n",
    "\n",
    "    # Ensure there are enough unique values to determine the n-th largest\n",
    "    if len(unique_sorted_values) >= n:\n",
    "        nth_value = unique_sorted_values[n - 1]  # Get the n-th largest unique value\n",
    "\n",
    "        # If the n-th value is 0, return None\n",
    "        if nth_value == 0:\n",
    "            return None\n",
    "        \n",
    "        # If n > 1, check for uniqueness against the (n-1)-th largest\n",
    "        if n > 1:\n",
    "            prev_value = unique_sorted_values[n - 2]  # (n-1)-th largest unique value\n",
    "            # If nth_value is equal to the (n-1)-th value, we don't want to return it\n",
    "            if nth_value == prev_value:\n",
    "                return None\n",
    "        \n",
    "        # Return the index of the n-th largest value\n",
    "        return sorted_row[sorted_row == nth_value].index[0]\n",
    "\n",
    "    # Return None if conditions are not met\n",
    "    return None\n",
    "\n",
    "def throw_dice(likelihood_dict):\n",
    "    # Unzip the dictionary into choices and probabilities\n",
    "    choices, probabilities = zip(*likelihood_dict.items())\n",
    "    \n",
    "    # Normalize probabilities\n",
    "    probabilities = np.array(probabilities) / np.sum(probabilities)\n",
    "    \n",
    "    # Return a random choice based on the probabilities\n",
    "    return np.random.choice(choices, p=probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "80d1350f-6435-4d24-953b-e2f197e45a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(data_point: dict) -> dict:\n",
    "    if not isinstance(data_point, dict):\n",
    "        raise TypeError(\"The input must be a dictionary.\")\n",
    "\n",
    "    # Workflow\n",
    "    # Apply preprocessment to datapoint\n",
    "    point = preproc(data_point)\n",
    "    # call scaler\n",
    "    # call models\n",
    "    \n",
    "    return data_point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "74f435b7-bfb2-4527-a615-03b885cd66ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = {\n",
    "    \"n_order\": 3,\n",
    "    \"n_cuisines\": 5,\n",
    "    \"total_amt\": 150.0,\n",
    "    \"n_vendor\": 2,\n",
    "    \"n_chain\": 1,\n",
    "    \"n_product\": 5,\n",
    "    \"first_order\": 10.5,\n",
    "    \"last_order\": 45.3,\n",
    "    \"DOW_0\": 1, \"DOW_1\": 1, \"DOW_2\": 0, \"DOW_3\": 0, \"DOW_4\": 0, \"DOW_5\": 0, \"DOW_6\": 0,\n",
    "    \"HR_0\": 0, \"HR_1\": 0, \"HR_2\": 0, \"HR_3\": 1, \"HR_4\": 0, \"HR_5\": 0, \"HR_6\": 0, \"HR_7\": 1,\n",
    "    \"HR_8\": 0, \"HR_9\": 0, \"HR_10\": 0, \"HR_11\": 0,\n",
    "    \"thai\": 2, \"italian\": 3, \"asian\": 0, 'american':100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "71d708df-0253-466f-a7aa-b5e8862b04d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_order': 3.0,\n",
       " 'n_cuisines': 3,\n",
       " 'total_amt': 105.0,\n",
       " 'n_vendor': 3,\n",
       " 'n_chain': 1.0,\n",
       " 'n_product': 5.0,\n",
       " 'first_order': 23.081,\n",
       " 'last_order': 68.927,\n",
       " 'DOW_0': 1.0,\n",
       " 'DOW_1': 1.0,\n",
       " 'DOW_2': 1.0,\n",
       " 'DOW_3': 0.0,\n",
       " 'DOW_4': 0.0,\n",
       " 'DOW_5': 0.0,\n",
       " 'DOW_6': 0.0,\n",
       " 'HR_0': 0.0,\n",
       " 'HR_1': 0.0,\n",
       " 'HR_2': 0.0,\n",
       " 'HR_3': 1.0,\n",
       " 'HR_4': 0.0,\n",
       " 'HR_5': 0.0,\n",
       " 'HR_6': 0.0,\n",
       " 'HR_7': 1.0,\n",
       " 'HR_8': 0.0,\n",
       " 'HR_9': 0.0,\n",
       " 'HR_10': 0.0,\n",
       " 'HR_11': 0.0,\n",
       " 'thai': 2.0,\n",
       " 'italian': 3.0,\n",
       " 'asian': 0.0,\n",
       " 'american': 100.0,\n",
       " 'cust_age': 27.505,\n",
       " 'beveragescafe': 0.0,\n",
       " 'chicken_dishes': 0.0,\n",
       " 'chinese': 0.0,\n",
       " 'dessertshealthy': 0.0,\n",
       " 'indian': 0.0,\n",
       " 'japanese': 0.0,\n",
       " 'noodle_dishes': 0.0,\n",
       " 'other': 0.0,\n",
       " 'street_food_snacks': 0.0,\n",
       " 'HR_12': 0.0,\n",
       " 'HR_13': 0.0,\n",
       " 'HR_14': 0.0,\n",
       " 'HR_15': 0.0,\n",
       " 'HR_16': 1.0,\n",
       " 'HR_17': 0.0,\n",
       " 'HR_18': 0.0,\n",
       " 'HR_19': 0.0,\n",
       " 'HR_20': 0.0,\n",
       " 'HR_21': 0.0,\n",
       " 'HR_22': 0.0,\n",
       " 'HR_23': 0.0,\n",
       " 'cust_region': nan,\n",
       " 'last_promo': nan,\n",
       " 'pay_method': nan}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ca4882de-1683-4d7a-b2eb-fe7fc01c6080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(raw_data_point: dict) -> dict: \n",
    "    \n",
    "    # Enforce datatypes for metric elements\n",
    "    for key in METRIC_KEYS:\n",
    "        try:\n",
    "            raw_data_point[key] = np.float64(raw_data_point[key])\n",
    "            \n",
    "            # Check if the value is less than 0\n",
    "            if raw_data_point[key] < 0:\n",
    "                raise ValueError\n",
    "        except (ValueError, KeyError) as e:\n",
    "            raw_data_point[key] = np.nan\n",
    "    \n",
    "    # Enforce string types for non-metric elements\n",
    "    for key in NON_METRIC_KEYS:\n",
    "        try:\n",
    "            if not isinstance(raw_data_point[key], str):\n",
    "                raw_data_point[key] = np.nan\n",
    "        except KeyError:\n",
    "            raw_data_point[key] = np.nan\n",
    "            \n",
    "    # Initialize sums\n",
    "    n_week = 0\n",
    "    n_day = 0\n",
    "    \n",
    "    # Calculate n_week and n_day directly, filling missing values with 0\n",
    "    for n in range(7):\n",
    "        dow_key = f\"DOW_{n}\"\n",
    "        if pd.isna(raw_data_point[dow_key]):\n",
    "            raw_data_point[dow_key] = 0\n",
    "        n_week += raw_data_point[dow_key]\n",
    "    \n",
    "    for n in range(24):\n",
    "        hr_key = f\"HR_{n}\"\n",
    "        if pd.isna(raw_data_point[hr_key]):\n",
    "            raw_data_point[hr_key] = 0\n",
    "        n_day += raw_data_point[hr_key]\n",
    "\n",
    "    diff = int(np.ceil(n_week - n_day))\n",
    "\n",
    "    # Correct if not equal\n",
    "    likelyhood = None\n",
    "    if diff < 0:\n",
    "        likelyhood = TIME_LIKELYHOODS['DAY']\n",
    "    elif diff > 0:\n",
    "        likelyhood = TIME_LIKELYHOODS['HOUR']\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if diff != 0:\n",
    "        for _ in range(diff):\n",
    "            raw_data_point[throw_dice(likelyhood)] += 1\n",
    "\n",
    "    # Finally, set n_order equal to the sum of either \n",
    "    n_order = n_week \n",
    "\n",
    "    # Fill missing amounts in cuisines with 0\n",
    "    for key in CUISINE_KEYS:\n",
    "        if pd.isna(raw_data_point[key]):\n",
    "            raw_data_point[key] = 0\n",
    "            \n",
    "    # Calculate number of cuisines ordered\n",
    "    n_cuisines = sum(\n",
    "        (raw_data_point[key] > 0) \n",
    "        for key in CUISINE_KEYS \n",
    "    )\n",
    "\n",
    "    # Check if customer to segment has spent any money\n",
    "    if n_cuisines == 0:\n",
    "        return \"Error: Invalid customer. Specify customer ammounts spent per cuisine.\"        \n",
    "       \n",
    "    # Check if number of cuisines ordered is larger than the number of orders made\n",
    "    diff = int(np.ceil(n_cuisines > n_order))\n",
    "    \n",
    "    if diff > 0:\n",
    "        for _ in range(diff):\n",
    "            # Assign another HR stochastically\n",
    "            raw_data_point[throw_dice(TIME_LIKELYHOODS['HOUR'])] += 1\n",
    "            # Assign another DAY stochastically\n",
    "            raw_data_point[throw_dice(TIME_LIKELYHOODS['DAY'])] += 1\n",
    "            n_order += 1\n",
    "            \n",
    "    # Sum total cuisines to obtain the total amount spent\n",
    "    total_amt = sum(\n",
    "        raw_data_point[key] for key in CUISINE_KEYS\n",
    "    )\n",
    "\n",
    "    raw_data_point['n_order'] = n_order\n",
    "    raw_data_point['n_cuisines'] = n_cuisines\n",
    "    raw_data_point['total_amt'] = total_amt\n",
    "        \n",
    "    try:\n",
    "        # Assuming each vendor only serves one type of cuisine, which might introduce some bias, but how else to solve?\n",
    "        if raw_data_point['n_vendor'] < n_cuisines:\n",
    "            raw_data_point['n_vendor'] = n_cuisines\n",
    "    except ValueError:\n",
    "        raw_data_point['n_vendor'] = n_cuisines\n",
    "\n",
    "    try:\n",
    "        if pd.isna(raw_data_point['n_chain']): \n",
    "            raise ValueError\n",
    "        # Check if the customer made a consistent number of purchases from chained restaurants \n",
    "        if raw_data_point['n_chain'] > raw_data_point['n_vendor']:\n",
    "            raw_data_point['n_chain'] = raw_data_point['n_vendor']\n",
    "\n",
    "        # Check if the customer made an illegal number of purchases from chained restaurants\n",
    "        if raw_data_point['n_chain'] < 0: \n",
    "            raw_data_point['n_chain'] = 0\n",
    "            \n",
    "    except ValueError:\n",
    "        # If here then definitely illegal\n",
    "        raw_data_point['n_chain'] = 0\n",
    "    \n",
    "    try:\n",
    "        # Check if the number of products is at least equal to the number of orders made\n",
    "        if raw_data_point['n_product'] < raw_data_point['n_order']:\n",
    "            raw_data_point['n_product'] = raw_data_point['n_order']\n",
    "    except ValueError:\n",
    "        raw_data_point['n_product'] = raw_data_point['n_order']\n",
    "\n",
    "    if raw_data_point['first_order']:\n",
    "        raw_data_point['first_order'] = MEANS['first_order']\n",
    "        \n",
    "    if raw_data_point['last_order']:\n",
    "        raw_data_point['last_order'] = MEANS['last_order']\n",
    "\n",
    "    if raw_data_point['cust_age']:\n",
    "        raw_data_point['cust_age'] = MEANS['cust_age']\n",
    "        \n",
    "    return raw_data_point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e8f675-a938-4915-b001-6fe458ced15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount spent on average per product\n",
    "raw_data_point['avg_amt_per_product'] = raw_data_point['total_amt'] / raw_data_point['n_product']\n",
    "\n",
    "# Amount spent on average per order\n",
    "raw_data_point['avg_amt_per_order'] = raw_data_point['total_amt'] / raw_data_point['n_order']\n",
    "\n",
    "# Amount spent on average per vendor\n",
    "raw_data_point['avg_amt_per_vendor'] = raw_data_point['total_amt'] / raw_data_point['n_vendor']\n",
    "\n",
    "# Total days as customer\n",
    "raw_data_point['days_cust'] = raw_data_point['last_order'] - raw_data_point['first_order']\n",
    "\n",
    "# Average days between orders\n",
    "raw_data_point['avg_days_to_order'] = raw_data_point['days_cust'] / raw_data_point['n_order']\n",
    "\n",
    "# Days the customer is due, according to their average days between orders\n",
    "raw_data_point['days_due'] = 90 - raw_data_point['last_order'] + raw_data_point['avg_days_to_order']\n",
    "\n",
    "# Percentage of orders placed to restaurants that are part of a chain\n",
    "raw_data_point['per_chain_order'] = raw_data_point['n_chain'] / raw_data_point['n_order']\n",
    "\n",
    "# And we add these tese features to the metric features list.\n",
    "metric_features.extend([\n",
    "    'n_order'\n",
    "    ,'per_chain_order'\n",
    "    ,'total_amt'\n",
    "    ,'avg_amt_per_order'\n",
    "    ,'avg_amt_per_product'\n",
    "    ,'avg_amt_per_vendor'\n",
    "    ,'days_cust'\n",
    "    ,'avg_days_to_order'\n",
    "    ,'days_due'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc43983-1ca9-470c-91b4-08f1c05b65bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask to check if each day column is populated\n",
    "mask = raw_data_point[[f'DOW_{i}' for i in range(7)]] > 0\n",
    "\n",
    "# Sum over the mask to get the count of days with purchases for each row\n",
    "raw_data_point.loc[:, 'n_days_week'] = mask.sum(axis=1)\n",
    "\n",
    "# Updating the list of metric features\n",
    "metric_features.append('n_days_week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ae784-31d6-43e7-a7fa-fff6e792ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask to check if each hour column is populated\n",
    "mask = raw_data_point[hour_features] > 0\n",
    "\n",
    "# Sum over the mask to get the count of hours with purchases for each row\n",
    "raw_data_point.loc[:, 'n_times_day'] = mask.sum(axis=1)\n",
    "\n",
    "# Updating the list of metric features\n",
    "metric_features.append('n_times_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a112e658-1c42-47bb-a0f1-910c10f5f3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag customers who have purchased in more than one day\n",
    "raw_data_point['regular'] = (raw_data_point['days_cust'] > 1)\n",
    "\n",
    "non_metric_features.append('regular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4979cf38-f857-4971-ac93-719c810da62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask where values are greater than zero (indicating an order)\n",
    "mask = raw_data_point[cuisine_features] > 0\n",
    "\n",
    "# Use mask to get the number of cuisines for each row\n",
    "raw_data_point.loc[:, 'n_cuisines'] = mask.sum(axis=1)\n",
    "\n",
    "# Updating the metric_features_list\n",
    "metric_features.append('n_cuisines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee45ab09-e666-454b-8666-1544fa80822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping specified columns and getting remaining columns as a list\n",
    "targets = raw_data_point.drop(columns=[\n",
    "    'cust_age'\n",
    "    , 'first_order'\n",
    "    , 'last_order'\n",
    "    , 'days_cust'\n",
    "    , 'days_due'\n",
    "    , 'avg_days_to_order'\n",
    "    , 'per_chain_order'\n",
    "    , 'cust_region'\n",
    "    , 'cust_city'\n",
    "    , 'last_promo'\n",
    "    , 'pay_method'\n",
    "    , 'n_cuisines'\n",
    "    , 'regular'\n",
    "] + hour_features + day_features).columns.tolist()\n",
    "\n",
    "# Initialize an empty dfFrame to store log-transformed columns\n",
    "log_transformed = pd.DataFrame()\n",
    "\n",
    "# Apply log1p to each column in targets and add it to log_transformed with the prefix 'log_'\n",
    "for col in targets:\n",
    "    log_transformed[f\"log_{col}\"] = np.log1p(raw_data_point[col])\n",
    "\n",
    "# We create a list of log_features to assist us in our exploration\n",
    "log_features = log_transformed.columns.tolist()\n",
    "\n",
    "# Concatenate the original dfFrame with the new log-transformed dfFrame\n",
    "raw_data_point = pd.concat([raw_data_point, log_transformed], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24985f23-4590-4ea4-83e6-f53eedd893c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries for feature groups with flags and relevant columns\n",
    "feature_groups = {\n",
    "    'foodie': ['n_vendor', 'n_product', 'n_order', 'n_cuisines'],\n",
    "    'gluttonous': ['avg_amt_per_order', 'total_amt', 'n_chain'],\n",
    "    'loyal': ['avg_amt_per_vendor'] + cuisine_features\n",
    "}\n",
    "\n",
    "# Create columns to hold the flags for each feature group\n",
    "raw_data_point['foodie_flag'] = 0\n",
    "raw_data_point['gluttonous_flag'] = 0\n",
    "raw_data_point['loyal_flag'] = 0\n",
    "\n",
    "# Function to calculate IQR bounds\n",
    "def calculate_bounds(series):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# Assign flags for each feature group\n",
    "for group, features in feature_groups.items():\n",
    "    for feature in features:\n",
    "        log_feature = f\"log_{feature}\"\n",
    "        \n",
    "        if feature == 'n_cuisines':\n",
    "            log_feature = feature\n",
    "        \n",
    "        lower_bound, upper_bound = calculate_bounds(raw_data_point.loc[(raw_data_point['regular'] == 1) & (raw_data_point[feature] > 0), log_feature])\n",
    "        \n",
    "        # Mark outliers for each group\n",
    "        if group == 'foodie':\n",
    "            raw_data_point.loc[raw_data_point['regular'] == 1, 'foodie_flag'] |= (\n",
    "                raw_data_point.loc[raw_data_point['regular'] == 1, log_feature] > upper_bound\n",
    "            ).astype(int)\n",
    "        elif group == 'gluttonous':\n",
    "            raw_data_point.loc[raw_data_point['regular'] == 1, 'gluttonous_flag'] |= (\n",
    "                raw_data_point.loc[raw_data_point['regular'] == 1, log_feature] > upper_bound\n",
    "            ).astype(int)\n",
    "        elif group == 'loyal':\n",
    "            raw_data_point.loc[raw_data_point['regular'] == 1, 'loyal_flag'] |= (\n",
    "                raw_data_point.loc[raw_data_point['regular'] == 1, log_feature] > upper_bound\n",
    "            ).astype(int)\n",
    "\n",
    "# Display results\n",
    "for group in ['foodie_flag', 'gluttonous_flag', 'loyal_flag']:\n",
    "    print(f\"Number of customers flagged as {group.split('_')[0]}:\", raw_data_point[group].sum())\n",
    "\n",
    "non_metric_features.extend([\n",
    "    'foodie_flag'\n",
    "    ,'gluttonous_flag'\n",
    "    ,'loyal_flag'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d389edfd-4bed-4788-a0a8-e3c8b0f2209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_point['top_cuisine'] = raw_data_point.apply(top_n, col_list=cuisine_features, n=1, axis=1)\n",
    "\n",
    "non_metric_features.append('top_cuisine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8f2f5-f552-4def-b438-6c86ea18b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average amount spent per day as customer\n",
    "raw_data_point['avg_amt_per_day'] = np.round(raw_data_point['total_amt'] / raw_data_point['days_cust'], 4)\n",
    "\n",
    "# Average number of products ordered per day as customer\n",
    "raw_data_point['avg_product_per_day'] = np.round(raw_data_point['n_product'] / raw_data_point['days_cust'], 4)\n",
    "\n",
    "# Average number of orders per day as customer\n",
    "raw_data_point['avg_order_per_day'] = np.round(raw_data_point['n_order'] / raw_data_point['days_cust'], 4)\n",
    "\n",
    "metric_features.extend([\n",
    "    'avg_amt_per_day'\n",
    "    ,'avg_product_per_day'\n",
    "    ,'avg_order_per_day'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f96466-3c89-4523-abc0-6a6ff6a1a27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_point.loc[raw_data_point['cust_age'].isna(), 'cust_age'] = raw_data_point['cust_age'].mean().astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b41bcb-1d58-4b2a-9c3a-133d068db828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating age buckets\n",
    "raw_data_point['age_bucket'] = np.where(\n",
    "    raw_data_point['cust_age'] < 25, '15-24', np.where(\n",
    "        raw_data_point['cust_age'] < 35, '25-34', np.where(\n",
    "            raw_data_point['cust_age'] < 45, '35-44', np.where(\n",
    "                raw_data_point['cust_age'] < 55, '45-54', np.where(\n",
    "                    raw_data_point['cust_age'] < 65, '55-64', '65+'\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "non_metric_features.insert(4, 'age_bucket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a006109-bf03-4781-9725-c1e1d471ec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_point.loc[raw_data_point['cust_age'].isna(), 'cust_age'] = np.ceil(raw_data_point['cust_age'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c4aa4c-419c-444a-8c6c-a7f9335aa882",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_point.topcs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
