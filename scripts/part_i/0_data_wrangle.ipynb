{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Wrangling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports, Options and Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports list\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style is important\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Ensuring pandas always prints all columns and rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_seq_items', None)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the csv data\n",
    "\n",
    "data = pd.read_csv('DM2425_ABCDEats_DATASET.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Wrangling the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset features has 31888 observations accross 56 variables, of which, 24 refer to hours of day, and 7 to days of week. Given the descriptions provided, we would expect all of these variables to take on integer values which is not the case. Moreover, missing values are present, specifically in 'HR_0', 'customer_age' and 'first_order'.\n",
    "\n",
    "Additionally, 'Customer_age' and 'first_order' should be integers, but are being shown in info() as floats, and the variable 'is_chain', which should be a boolean is, in fact, and integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling .tail() shows us that no aggregations are present at the bottom of the dataset, which is good. Moreover, the data itself is just as we expected from inspecting .info()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Renaming variables\n",
    "\n",
    "We rename some variables to an easier and shorter convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_rename_dict = {\n",
    "        'customer_region' : 'cust_region'\n",
    "        , 'payment_method' : 'pay_method'\n",
    "        , 'customer_age' : 'cust_age'\n",
    "        , 'vendor_count' : 'n_vendor'\n",
    "        , 'product_count' : 'n_product'\n",
    "        , 'n_order' : 'n_order'\n",
    "        , 'is_chain' : 'n_chain'\n",
    "        , 'CUI_American' : 'American'\n",
    "        , 'CUI_Asian' : 'Asian'\n",
    "        , 'CUI_Beverages' : 'Beverages'\n",
    "        , 'CUI_Cafe' : 'Cafe'\n",
    "        , 'CUI_Chicken Dishes' : 'Chicken Dishes'\n",
    "        , 'CUI_Chinese' : 'Chinese'\n",
    "        , 'CUI_Desserts' : 'Desserts'\n",
    "        , 'CUI_Healthy' : 'Healthy'\n",
    "        , 'CUI_Indian' : 'Indian'\n",
    "        , 'CUI_Italian' : 'Italian'\n",
    "        , 'CUI_Japanese' : 'Japanese'\n",
    "        , 'CUI_Noodle Dishes' : 'Noodle Dishes'\n",
    "        , 'CUI_OTHER' : 'OTHER'\n",
    "        , 'CUI_Street Food / Snacks' : 'Street Food / Snacks'\n",
    "        , 'CUI_Thai' : 'Thai'\n",
    "}\n",
    "\n",
    "# Rename the columns for easier reference\n",
    "data.rename(columns=_rename_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Investigating 'customer_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the values of customer id, by manipulating the hex/bin conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert customer_id from hexadecimal to int \n",
    "id_array = data['customer_id'].apply(lambda x: int(x, 16))\n",
    "\n",
    "# And plot its histogram\n",
    "plt.hist(id_array, bins=120, edgecolor='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conversion of the values of customer_id into hexadecimal, and the plot of their histogram shows us, what is very likely to be a uniform distribution, this tells us that these values are indeed either random, or randomly sampled, or both. We will not perform any further investigation into this hypothesis, and will use the hexadecimal form of customer_id as index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for duplicate ID's\n",
    "\n",
    "Before turning customer id into the index of the dataframe, we check if there are any repeated values, i.e., multiple entries for a customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep false, shows does not drop the duplicated values which allows visual inspection\n",
    "data[data['customer_id'].duplicated(keep=False)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detected duplicate customer entries are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the integer value of the customer hex values, the index. \n",
    "\n",
    "data['customer_id'] = id_array\n",
    "\n",
    "# Set 'customer_id' as the index\n",
    "data = data[~data['customer_id'].duplicated()].set_index('customer_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small functional choice, because we retain gain the ability to slice with integer indices, but we can always convert them back to their respective hexdecimal values for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Data Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now highlight missing values, and analyze the consistency of data with data types and business logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance it might seem odd that there are missing values for 'first_order' that do not have a corresponding last_order, but in actuality this points to a very obvious fact, that these customers had made their first order before we had begun pur data collection efforts.\n",
    "\n",
    "Defined as:\n",
    "- \"Number of days from the start of the dataset when the customer first placed an order.\"\n",
    "\n",
    "The soundest value to input is the minimum value possible i.e. 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.loc[data['first_order'].isna(), 'first_order'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HR_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"DOW_0 to DOW_6\" encode -> Number of orders placed on each day of the week. (0 = Sunday, 6 = Saturday) <br>\n",
    "\"HR_0 to HR_23\" encode -> of orders placed during each hour of the day. (0 = midnight, 23 = 11 PM)\n",
    "\n",
    "Therefore, it is simply a matter of summing the DOW values, subtracting all of the HR values, and impute the result of the operation to the value of HR_0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_week = data[[f\"DOW_{n}\" for n in range(7)]].sum(axis=1)\n",
    "sum_day = data[[f\"HR_{n}\" for n in range(24)]].sum(axis=1)\n",
    "\n",
    "data.loc[data['HR_0'].isna(), 'HR_0'] = (sum_week - sum_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by plotting the histogram for Age, making not of a heavy skew of the data towards the lower values of it's range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data['cust_age'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at pairwise plots of age with other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Specify the variable for which you want to plot against others\n",
    "target_variable = 'cust_age'\n",
    "\n",
    "# Create scatter plots\n",
    "def scatter_plots(df, target_variable):\n",
    "    \n",
    "    # Get a list of all columns except the target variable\n",
    "    other_variables = df.columns[df.columns != target_variable]\n",
    "\n",
    "    n_cols = 2\n",
    "    n_rows = math.ceil(len(df.columns) / n_cols)\n",
    "    \n",
    "    # Create scatter plots\n",
    "    plt.figure(figsize=(12, 6 * n_rows))  # Adjust height based on number of rows\n",
    "    for i, variable in enumerate(other_variables):\n",
    "        plt.subplot(n_rows, n_cols, i + 1)  # Adjust the subplot layout\n",
    "        sns.scatterplot(data=df, x=target_variable, y=variable)\n",
    "        plt.title(f'Scatter plot of {target_variable} vs {variable}')\n",
    "        plt.xlabel(target_variable)\n",
    "        plt.ylabel(variable)\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "# Call the function to create the plots\n",
    "scatter_plots(data, target_variable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inspection of the plots above shows us that there seems to be some stochastic relation between some of these variables, and customer age. However, it is not something that we want to explore, or exploit just yet, so we will refrain from treating this missing values during this exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = {}\n",
    "\n",
    "_df = data.loc[~data['cust_age'].isna(),].copy(deep=True)\n",
    "\n",
    "for _x in data.columns:\n",
    "    try: \n",
    "        coef, p_value  = pearsonr(_df[_x], _df['cust_age'])\n",
    "        correlations[_x] = {'coef' : coef, 'p_value' : p_value}\n",
    "    except Exception as e: \n",
    "        pass\n",
    "        \n",
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going one step further we can see that there is no evidence of linear association between other metric variables and customer_age, because most p-values are too large, and of those that aren't the effect is both likely spurious, as well as the actual values of the correlations insignificant. \n",
    "\n",
    "Taking all this into consideration, we might consider some kind of imputation later, that makes use of this knowledge that we have gathered, about these distributions. But for now we drop these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset='cust_age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Unique Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on to unique values, we use this point in the notebook to assess the quality, and consistency of our variables data types, with the their expected values, given the metadata and general knowledge about the features they capture.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        'feature_type': [data[column].dtype for column in data.columns],\n",
    "        'unique_values': data.apply(lambda col: sorted(pd.Series.unique(col).tolist()))\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis of the output above leads us to the following conclusions:\n",
    "\n",
    "- customer_region - value '-' possibly meaning 'unknown_region'.\n",
    "- last_promo - value '-' possibly meaning 'NOPROMO'\n",
    "- is_chain - has 60 different values, it may be storing the count of orders made to chain restaurants.\n",
    "\n",
    "Other than that, all variables, now including HR_0 seem to take on values we would expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Inconsistencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can immediatly be seen that there are no negative values in any of the series above (as we have taken care in sorting them), however, many other possible types of inconsistencies are possible, yet we are confident that there are likely mostly two kinds of inconsistencies left: \n",
    "\n",
    "- duplicate values;\n",
    "- relational impossibilities.\n",
    "\n",
    "Duplicated values are observations that share every columnwise value; while relational impossibilities are data patterns that are inconsistent with the possible business outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Indentifying and treating duplicated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[data.duplicated(keep=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[data.duplicated()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.duplicated(keep=False)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find 94 such rows, regarding 47 entries; and simply drop the repeats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Identifying inconsistent values\n",
    "\n",
    "Using DeMorgan's Law and the fact that we are interested only in situations where the conditions below are all True we \n",
    "may represent the set of all inconsistent value as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# has at least one vendor\n",
    "has_vendor = data['n_vendor'] != 0 \n",
    "\n",
    "# has at least one product\n",
    "has_product = data['n_product'] != 0 \n",
    "\n",
    "# purchase must have been made on a valid dow\n",
    "some_day = (data[[f\"DOW_{n}\" for n in range(7)]] != 0).any(axis = 1) \n",
    "\n",
    "# purchase must have been made at a valid hour\n",
    "some_hour = (data[[f\"HR_{n}\" for n in range(24)]] != 0).any(axis = 1)  \n",
    "\n",
    "# some type of cuisine must have been ordered\n",
    "some_food = (data[data.columns[9:24]] != 0).any(axis = 1) \n",
    "\n",
    "# We find 156 such values\n",
    "data[~(has_vendor & has_product & some_day & some_hour & some_food)].shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[~(has_vendor & has_product & some_day & some_hour & some_food)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(has_vendor & has_product & some_day & some_hour & some_food)] # And we drop these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check if the sum of the order counts for DOW, match the sum of the order counts per HR.\n",
    "\n",
    "sum_week = data[[f\"DOW_{n}\" for n in range(7)]].sum(axis=1)\n",
    "sum_day = data[[f\"HR_{n}\" for n in range(24)]].sum(axis=1)\n",
    "\n",
    "data[(sum_day != sum_week)].shape[0] # Bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check if there are any customers, for whom 'last order' was placed before 'first_order'.\n",
    "\n",
    "data[data['last_order'] < data['first_order']].shape[0] # Double Bam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We correct the inconsistent categorical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4. Treating Inconsistent values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now analyise the nature of the values '-' in last promo and in cust_region, and 'last_promo', to see if what type of missing information they hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some preparation\n",
    "\n",
    "selected_features = data.columns.tolist()\n",
    "selected_features.remove('pay_method')\n",
    "selected_features.remove('last_promo')\n",
    "selected_features.remove('cust_region')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cust Region\n",
    "\n",
    "Even though fancier methods exists, we apply a few simple tools learned from Statistics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We start by visually inspecting the output of describe applied to the groupby\n",
    "# cust_region\n",
    "data.groupby('cust_region').describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get unique regions\n",
    "unique_regions = data['cust_region'].unique()\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over all combinations of regions\n",
    "for region_a, region_b in itertools.combinations(unique_regions, 2):\n",
    "    # Filter data by category for each region\n",
    "    group_a = data[data['cust_region'] == region_a]\n",
    "    group_b = data[data['cust_region'] == region_b]\n",
    "    \n",
    "    # Run t-tests for each feature and store the p-values\n",
    "    p_values = {}\n",
    "    for feature in selected_features:  # list of continuous features\n",
    "        t_stat, p_val = ttest_ind(group_a[feature], group_b[feature], equal_var=False)\n",
    "        p_values[feature] = p_val\n",
    "    \n",
    "    # Apply Bonferroni correction\n",
    "    p_values_corrected = multipletests(list(p_values.values()), method='bonferroni')\n",
    "    \n",
    "    # Extract adjusted p-values\n",
    "    adjusted_p_values = dict(zip(p_values.keys(), p_values_corrected[1]))\n",
    "\n",
    "    # Calculate Bonferroni threshold\n",
    "    alpha = 0.05  # Original significance level\n",
    "    n_tests = len(p_values)  # Number of tests\n",
    "    bonferroni_threshold = alpha / n_tests\n",
    "\n",
    "    # Create a list of features where we reject the null hypothesis\n",
    "    rejected_features = [feature for feature, adjusted_p in adjusted_p_values.items() if adjusted_p < bonferroni_threshold]\n",
    "\n",
    "    # Store results in the list as a single entry per region pair\n",
    "    results.append({\n",
    "        'region_a': region_a,\n",
    "        'region_b': region_b,\n",
    "        'rejected_features': rejected_features  # List of features rejected\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results DataFrame\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we would not present these results the juri of a murder trial - as has regretebly happened before - these multiple t-tests with bonferroni correction are useful for clustering purposes, as we are just looking for sufficient confidence in our groupings to perform exploratory analysis, moreover, if perhaps, the subsequent analysis holds we might even end up retaining them. \n",
    "\n",
    "Moving on, our analysis points us to the following inferences: \n",
    "- '2440' and '2490' map to the same region, as do '-' and '8670', as we had initially conjecture, as they do not differ in a statistically significant fashion in the multiple t-tests\n",
    "- with only 13 observations, customer region '8550' is very difficult to group with any other, because we lack a statistically representative sample of that population;\n",
    "- regions starting with the same integer differ only by a small subset of features, and as we are given the information that these pertain to three cities, we are left to conclude, that these customer regions, respect a city wide macro-trend, while they show ethno-demographic differences at a more micro-level.\n",
    "\n",
    "We then decide to do as follows: \n",
    "- fill the values of '-' with 8670, and join region '2440' with '2490', as both cases fail to demonstrate statistical evidence of belonging to different populations;\n",
    "- create a variable city, with value corresponding to the first digit of each 'cust region';\n",
    "- give no focus to region '8550' in further exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make the changes official\n",
    "\n",
    "data.loc[data['cust_region'] == '-', 'cust_region'] = '8670'\n",
    "data.loc[data['cust_region'].isin(['2440', '2490']), 'cust_region'] = '2400'\n",
    "\n",
    "# And add the variable\n",
    "data['cust_city'] = data['cust_region'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Very good \n",
    "\n",
    "data['cust_city'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last Promo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get unique promos\n",
    "unique_regions = data['last_promo'].unique()\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over all combinations of regions\n",
    "for region_a, region_b in itertools.combinations(unique_regions, 2):\n",
    "    # Filter data by category for each region\n",
    "    group_a = data[data['last_promo'] == region_a]\n",
    "    group_b = data[data['last_promo'] == region_b]\n",
    "    \n",
    "    # Run t-tests for each feature and store the p-values\n",
    "    p_values = {}\n",
    "    for feature in selected_features:  # list of continuous features\n",
    "        t_stat, p_val = ttest_ind(group_a[feature], group_b[feature], equal_var=False)\n",
    "        p_values[feature] = p_val\n",
    "    \n",
    "    # Apply Bonferroni correction\n",
    "    p_values_corrected = multipletests(list(p_values.values()), method='bonferroni')\n",
    "    \n",
    "    # Extract adjusted p-values\n",
    "    adjusted_p_values = dict(zip(p_values.keys(), p_values_corrected[1]))\n",
    "\n",
    "    # Calculate Bonferroni threshold\n",
    "    alpha = 0.05  # Original significance level\n",
    "    n_tests = len(p_values)  # Number of tests\n",
    "    bonferroni_threshold = alpha / n_tests\n",
    "\n",
    "    # Create a list of features where we reject the null hypothesis\n",
    "    rejected_features = [feature for feature, adjusted_p in adjusted_p_values.items() if adjusted_p < bonferroni_threshold]\n",
    "\n",
    "    # Store results in the list as a single entry per region pair\n",
    "    results.append({\n",
    "        'promo_a': region_a,\n",
    "        'promo_b': region_b,\n",
    "        'rejected_features': rejected_features  # List of features rejected\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results DataFrame\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.groupby('last_promo').describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that these correspond to \n",
    "\n",
    "https://merchants.ubereats.com/gb/en/resources/articles/10-types-of-sales-promotions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Asserting Data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And assert the desired datatypes for each of our variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidying up datatypes\n",
    "\n",
    "for col in data.iloc[:, 0:9]:\n",
    "    if col in ['last_promo', 'pay_method', 'cust_region']:\n",
    "        data[col] = data[col].astype(object)\n",
    "    else:\n",
    "        data[col] = data[col].astype('Int64')\n",
    "\n",
    "for col in data.iloc[:, 9:24]:\n",
    "    data[col] = data[col].astype(float)\n",
    "\n",
    "for col in data.iloc[:, 24:]:\n",
    "    data[col] = data[col].astype('Int64')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the wrangling portion of our data analysis, the next portion can be found at \"initial_explore.ipyjn\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export DataFrame as a CSV file\n",
    "\n",
    "data.reset_index(drop=False).to_csv('wrangled_data.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
