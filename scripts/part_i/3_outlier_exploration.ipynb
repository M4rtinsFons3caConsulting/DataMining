{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Correlation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports, Options and Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports list\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import fastcluster\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style is important\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Ensuring pandas always prints all columns and rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_seq_items', None)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the csv data\n",
    "\n",
    "data = pd.read_csv('/home/shadybea/Documents/IMS/2024 - 2025/Projects/DataMining/initial_explore.csv', index_col='customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = data[\n",
    "    #(data['regular'] == 0) & \n",
    "    (data['loyal_flag']) & \n",
    "    (data['gluttonous_flag']) & \n",
    "    (data['foodie_flag']) &\n",
    "    (data['cust_region'].isin([6, 7, 8]))\n",
    "].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df.info() # Load OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables\n",
    "non_metric_features = ['cust_region', 'last_promo', 'pay_method', 'ordered_cuisines']\n",
    "\n",
    "# Hour of day variables\n",
    "hour_features = list(subset_df.columns[31:55]) + list(subset_df.columns[93:117])\n",
    "\n",
    "# Day of week variables\n",
    "day_features = list(subset_df.columns[24:31]) + list(subset_df.columns[86:93])\n",
    "\n",
    "# Cusine features\n",
    "cuisine_features = list(subset_df.columns[9:24]) + list(subset_df.columns[71:86])\n",
    "\n",
    "# Metric variables, that are not above\n",
    "metric_features = subset_df.columns.drop(non_metric_features).drop(hour_features).drop(day_features).drop(cuisine_features).to_list()[:18]\n",
    "\n",
    "# Hour of day variables\n",
    "hour_features = list(subset_df.columns[31:55])\n",
    "\n",
    "# # Day of week variables\n",
    "day_features = list(subset_df.columns[24:31])\n",
    "\n",
    "# # Cusine features\n",
    "cuisine_features = list(subset_df.columns[9:24])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the correlation matrix for the metric features\n",
    "corr_matrix = subset_df[metric_features].corr(method='pearson')\n",
    "\n",
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "# create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "annot = np.full(corr_matrix.shape, '', dtype=object)\n",
    "\n",
    "# Fill the annotation array with formatted values for correlations above the threshold\n",
    "for i in range(corr_matrix.shape[0]):\n",
    "    for j in range(corr_matrix.shape[1]):\n",
    "        if abs(corr_matrix.iat[i, j]) >= 0.3:\n",
    "            annot[i, j] = f\"{corr_matrix.iat[i, j]:.2f}\" \n",
    "\n",
    "# create the heatmap\n",
    "sns.heatmap(data=corr_matrix, annot=annot, cmap='vlag', \n",
    "            fmt=\"\", center=0, vmin=-1, vmax=1, square=True, \n",
    "            linewidths=.5, mask=mask, annot_kws={\"size\": 10})\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Metric Features Pearson Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the correlation matrix for the day features\n",
    "corr_matrix = subset_df[day_features].corr(method='pearson')\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "\n",
    "# create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "annot = np.full(corr_matrix.shape, '', dtype=object)\n",
    "\n",
    "# Fill the annotation array with formatted values for correlations above the threshold\n",
    "for i in range(corr_matrix.shape[0]):\n",
    "    for j in range(corr_matrix.shape[1]):\n",
    "        if abs(corr_matrix.iat[i, j]) >= 0.3:\n",
    "            annot[i, j] = f\"{corr_matrix.iat[i, j]:.2f}\" \n",
    "\n",
    "# create the heatmap\n",
    "sns.heatmap(data=corr_matrix, annot=annot, cmap='vlag',\n",
    "            fmt=\"\", center=0, vmin=-1, vmax=1, square=True, \n",
    "            linewidths=.5, mask=mask, annot_kws={\"size\": 10})\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Day Features Pearson Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't think there is anything relevant in this graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hour Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the correlation matrix for the hour features\n",
    "corr_matrix = subset_df[hour_features].corr(method='pearson')\n",
    "\n",
    "plt.figure(figsize=(25,15))\n",
    "\n",
    "# create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "annot = np.full(corr_matrix.shape, '', dtype=object)\n",
    "\n",
    "# Fill the annotation array with formatted values for correlations above the threshold\n",
    "for i in range(corr_matrix.shape[0]):\n",
    "    for j in range(corr_matrix.shape[1]):\n",
    "        if abs(corr_matrix.iat[i, j]) >= 0.3:\n",
    "            annot[i, j] = f\"{corr_matrix.iat[i, j]:.2f}\" \n",
    "\n",
    "# create the heatmap\n",
    "sns.heatmap(data=corr_matrix, annot=annot, cmap='vlag',\n",
    "            fmt=\"\", center=0, vmin=-1, vmax=1, square=True, \n",
    "            linewidths=.5, mask=mask, annot_kws={\"size\": 6.5})\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Hour Features Pearson Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is nothing relevant here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuisine Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the correlation matrix for the cuisine features\n",
    "corr_matrix = subset_df[cuisine_features].corr(method='pearson')\n",
    "\n",
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "# create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "annot = np.full(corr_matrix.shape, '', dtype=object)\n",
    "\n",
    "# Fill the annotation array with formatted values for correlations above the threshold\n",
    "for i in range(corr_matrix.shape[0]):\n",
    "    for j in range(corr_matrix.shape[1]):\n",
    "        if abs(corr_matrix.iat[i, j]) >= 0.3:\n",
    "            annot[i, j] = f\"{corr_matrix.iat[i, j]:.2f}\" \n",
    "\n",
    "# create the heatmap\n",
    "sns.heatmap(data=corr_matrix, annot=annot, cmap='vlag',\n",
    "            fmt=\"\", center=0, vmin=-1, vmax=1, square=True, \n",
    "            linewidths=.5, mask=mask, annot_kws={\"size\": 10})\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Cuisine Features Pearson Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is nothing here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Features (so far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the correlation matrix for the cuisine features\n",
    "corr_matrix = subset_df[metric_features[:15] + hour_features[:24] + day_features[:7] + cuisine_features[:15]].corr(method='pearson')\n",
    "\n",
    "plt.figure(figsize=(50,38))\n",
    "\n",
    "# create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))  # Mask only the upper triangle\n",
    "\n",
    "annot = np.full(corr_matrix.shape, '', dtype=object)\n",
    "\n",
    "# Fill the annotation array with formatted values for correlations above the threshold\n",
    "for i in range(corr_matrix.shape[0]):\n",
    "    for j in range(corr_matrix.shape[1]):\n",
    "        if abs(corr_matrix.iat[i, j]) >= 0.3:\n",
    "            annot[i, j] = f\"{corr_matrix.iat[i, j]:.2f}\" \n",
    "\n",
    "# create the heatmap\n",
    "sns.heatmap(data=corr_matrix, annot=annot, cmap='vlag',\n",
    "            fmt=\"\", center=0, vmin=-1, vmax=1, square=True, \n",
    "            linewidths=.5, mask=mask, annot_kws={\"size\": 10})\n",
    "\n",
    "# Show the plot\n",
    "plt.title('All Features Pearson Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of vendors, products and orders made in chained restaurants seem to be fairly correlated among themselves, which could mean that customers that order more products, order them from more distinct vendors and often from chained restaurants.\n",
    "\n",
    "The total amount spent and the number of orders directly correlate with the number of products, as to be expected since the more products are bought, the more money is spent, and more likely it is for the products to have been bought on different occasions.\n",
    "\n",
    "First order is inversely correlated with the number of vendors, products, purchases in chained restaurants and orders and the total amount spent - since it is a first order it makes sense that these values would be at their lowest.\n",
    "\n",
    "The longer a person has been a customer, the more orders tend to be placed and bigger the number of vendors and products. However, the average days between each order is also bigger, which could mean that the customers are not buying as regularly, i.e. this a strong indicator that propensity towards consumption tends to deacelerate with time.\n",
    "\n",
    "Regarding the hours, we can see two order spikes around lunch and dinner time, with the lunch time spike starting rather early which can indicate the orders of our workers that want to make sure their lunch is delivered in time for their lunch break. In addition, the afternoon hours seem to show autocorrelation between lagged hours, this means that if we know that someone usually buys at a certain time in the afternoon, it is likely they have made purchases at the previous hour - this likelyhood increases with the number of orders.\n",
    "\n",
    "<span style='color:yellow'>Regarding the week days, it seems the number of orders has lower correlation with Friday and Saturday and that the days as customers is also less correlated with these two week days - this could mean our customer base tends to order more on the week days.</span>\n",
    "\n",
    "Regarding the cuisines, the Asian cuisine shows high correlation with the total amount spent by a customer, implying that higher spenders tend to order chinese food; the Chicken dishes seem to be ordered a lot from chain restaurants; the OTHER cuisine gets a lot of orders with a bigger amount of products and from chain restaurants and is mostly consumed during the afternoon.\n",
    "High spenders mostly spend their money on Asian, Street Food/Snacks, American and Japanese dishes, while the overall product volume is greatest in OTHER, Chinese, American and Asian. The cuisine on which customers tend to spend more money per order is on Street Food/Snacks.\n",
    "OTHER is highly correlated with 4 days of the week - Monday to Thursday.\n",
    "\n",
    "To sum up, when looking at cuisines and time, 4 o'clock in the afternoon seems to be a great time to eat some pasta xD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding our customer base (i.e., people that have ordered more than once), we conclude we have found our 9 to 5 workers, as the days due inversely correlates with the working days of the week - this means that for those customers for which this correlation holds, we can say that they tend to order more often and below the average days to order threshold for which we would expect the average customer to place a new order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute the correlation matrix for all the features\n",
    "# corr_matrix = one_time[metric_features[:15] + hour_features[:24] + day_features[:7] + cuisine_features[:15]].corr(method='pearson')\n",
    "\n",
    "# plt.figure(figsize=(50,38))\n",
    "\n",
    "# # create a mask for the upper triangle\n",
    "# mask = np.triu(np.ones_like(corr_matrix, dtype=bool))  # Mask only the upper triangle\n",
    "\n",
    "# annot = np.full(corr_matrix.shape, '', dtype=object)\n",
    "\n",
    "# # Fill the annotation array with formatted values for correlations above the threshold\n",
    "# for i in range(corr_matrix.shape[0]):\n",
    "#     for j in range(corr_matrix.shape[1]):\n",
    "#         if abs(corr_matrix.iat[i, j]) >= 0.3:\n",
    "#             annot[i, j] = f\"{corr_matrix.iat[i, j]:.2f}\" \n",
    "\n",
    "# # create the heatmap\n",
    "# sns.heatmap(data=corr_matrix, annot=annot, cmap='vlag',\n",
    "#             fmt=\"\", center=0, vmin=-1, vmax=1, square=True, \n",
    "#             linewidths=.5, mask=mask, annot_kws={\"size\": 10})\n",
    "\n",
    "# # Show the plot\n",
    "# plt.title('All Features one_time Pearson Correlation Heatmap')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one time customers tend to spend their money on Street Food/Snacks and Asian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Create a mask where values are greater than zero (indicating an order)\n",
    "# mask = one_time[cuisine_features[:15]] > 0\n",
    "\n",
    "# #Use mask to get the ordered cuisines for each row\n",
    "# one_time['ordered_cuisines'] = mask.apply(\n",
    "#     lambda row: next((cuisine for cuisine, ordered in row.items() if ordered), None), \n",
    "#     axis=1\n",
    "# )\n",
    "# plt.figure(figsize=(15, 8))\n",
    "# #Display countplot for how many cuisines were ordered per customer\n",
    "# sns.countplot(data=one_time, x='ordered_cuisines', color='black')\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered = one_time.loc[one_time['last_promo'] != 3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.histplot(data=filtered, x='total_amt', hue='last_promo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-metric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestIndependence(df, target_var, alpha=0.05):        \n",
    "    results = {}\n",
    "\n",
    "    for col in df[non_metric_features]:\n",
    "        \n",
    "        if col != target_var:\n",
    "            contingency_table = pd.crosstab(df[target_var], df[col])\n",
    "            chi2, p, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "            results[col] = {\n",
    "                \"Chi-square statistic\": chi2\n",
    "                , \"p-value\": p\n",
    "                , \"Associated\": p < alpha\n",
    "            }\n",
    "\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in non_metric_features:\n",
    "    print(var)\n",
    "    display(TestIndependence(subset_df, var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three-way ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We filter only those with dependents not missing\n",
    "xfeat = 'pay_method'\n",
    "huefeat='last_promo'\n",
    "df_viz = subset_df.loc[~subset_df[xfeat].isna()]\n",
    "regions = df_viz.cust_region.dropna().unique()\n",
    "\n",
    "fig, axes = plt.subplots(1,len(regions), figsize=(30, 6), tight_layout=True, sharey=True)\n",
    "\n",
    "for (axi, region_i) in zip(axes, regions):\n",
    "    data_ = df_viz.loc[df_viz['cust_region']==region_i]\n",
    "    \n",
    "    sns.pointplot(data=data_, \n",
    "              y='avg_amt_per_product', \n",
    "              x=xfeat, \n",
    "              hue=huefeat, \n",
    "              errorbar='sd',\n",
    "              linestyles=[\"-\", \"--\", \":\", \"-.\"],\n",
    "              capsize=.1,\n",
    "              ax=axi\n",
    "              )\n",
    "\n",
    "    axi.legend([], frameon=False)\n",
    "    axi.set_title(region_i)\n",
    "\n",
    "axes[-1].legend(loc=(1.02,.5), title=huefeat, handlelength=6)\n",
    "\n",
    "fig.suptitle(\"Three-way ANOVA\\navg_amt_per_product, cust_region, pay_method, last_promo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df['above_log_avg_amt_per_product'] = np.where(subset_df['log_avg_amt_per_product'] > 1.824, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We filter only those with dependents not missing\n",
    "xfeat = 'above_log_avg_amt_per_product'\n",
    "huefeat='last_promo'\n",
    "df_viz = subset_df.loc[~subset_df[xfeat].isna()]\n",
    "regions = df_viz.cust_region.dropna().unique()\n",
    "\n",
    "fig, axes = plt.subplots(1,len(regions), figsize=(30, 6), tight_layout=True, sharey=True)\n",
    "\n",
    "for (axi, region_i) in zip(axes, regions):\n",
    "    data_ = df_viz.loc[df_viz['cust_region']==region_i]\n",
    "    \n",
    "    sns.pointplot(data=data_, \n",
    "              y='total_amt', \n",
    "              x=xfeat, \n",
    "              hue=huefeat, \n",
    "              errorbar='sd',\n",
    "              linestyles=[\"-\", \"--\", \":\", \"-.\"],\n",
    "              capsize=.1,\n",
    "              ax=axi\n",
    "              )\n",
    "\n",
    "    axi.legend([], frameon=False)\n",
    "    axi.set_title(region_i)\n",
    "\n",
    "axes[-1].legend(loc=(1.02,.5), title=huefeat, handlelength=6)\n",
    "\n",
    "fig.suptitle(\"Three-way ANOVA\\ntotal_amt, cust_region, pay_method, last_promo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "sns.violinplot(\n",
    "    data=subset_df,\n",
    "    x='cust_region',\n",
    "    y='total_amt',\n",
    "    hue='above_log_avg_amt_per_product',\n",
    "    split=True,\n",
    "    inner=None,\n",
    "    fill=False\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = subset_df\n",
    "x = rs.total_amt\n",
    "g = rs.cust_region\n",
    "h = rs.above_log_avg_amt_per_product\n",
    "df = pd.DataFrame(dict(x=x, g=g, h=h))\n",
    "\n",
    "# Initialize the FacetGrid object\n",
    "pal = sns.cubehelix_palette(10, rot=-.25, light=.7)\n",
    "g = sns.FacetGrid(df, row=\"g\", hue=\"h\", aspect=15, height=1.5, palette='Dark2')\n",
    "\n",
    "# Draw the densities in a few steps\n",
    "g.map(sns.kdeplot, \"x\",\n",
    "      bw_adjust=.5, clip_on=False,\n",
    "      fill=True, alpha=0.5, linewidth=1.5)\n",
    "g.map(sns.kdeplot, \"x\", clip_on=False, lw=2, bw_adjust=.5, alpha=1)\n",
    "\n",
    "# Passing color=None to refline() uses the hue mapping\n",
    "g.refline(y=0, linewidth=2, linestyle=\"-\", color=None, clip_on=False)\n",
    "\n",
    "# Define and use a simple function to label the plot in axes coordinates\n",
    "def label(ax, region):\n",
    "    ax.text(0, .2, f\"Region {region}\", fontweight=\"bold\", color='black',\n",
    "            ha=\"left\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "# Iterate over axes and unique region values\n",
    "for ax, region in zip(g.axes.flat, df['g'].unique()):\n",
    "    label(ax, region)  # Place the label in the correct subplot\n",
    "\n",
    "# Adjust labels and titles\n",
    "g.set(xlabel=\"total_amt\")\n",
    "g.set_titles(size=12)\n",
    "\n",
    "# Remove axes details that don't play well with overlap\n",
    "g.set_titles(\"\")\n",
    "g.set(yticks=[], ylabel=\"\")\n",
    "g.despine(bottom=True, left=True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PairGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = non_metric_features + metric_features + day_features + hour_features + cuisine_features\n",
    "sample_data = subset_df[features].sample(1000, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = sns.PairGrid(data[features])\n",
    "# g.map_lower(sns.scatterplot)  # or any other plot type\n",
    "# g.map_diag(sns.kdeplot, fill=True)  # or sns.kdeplot for density plots\n",
    "\n",
    "# for i in range(len(g.axes)):\n",
    "#    for j in range(len(g.axes)):\n",
    "#        if j > i:  # This identifies the upper triangle\n",
    "#            g.axes[i, j].set_visible(False)  # Hide the axes\n",
    "\n",
    "# g.fig.set_dpi(30)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df['top_cuisine'] = subset_df[cuisine_features].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n(row, col_list, n):\n",
    "    sorted_row = row[col_list].sort_values(ascending=False)\n",
    "    if len(sorted_row) > n-1 and sorted_row.iloc[n-2] != sorted_row.iloc[n-1]:\n",
    "        return sorted_row.index[n-1]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df['top2_cuisine'] = subset_df.apply(top_n, col_list=cuisine_features, n=2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df['top3_cuisine'] = subset_df.apply(top_n, col_list=cuisine_features, n=3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.PairGrid(subset_df[['avg_days_to_order', 'days_due', 'per_chain_order', 'n_order', 'days_cust']])\n",
    "g.map_lower(sns.scatterplot, hue=subset_df['n_cuisines'], alpha=0.1)\n",
    "g.map_diag(sns.kdeplot, fill=True, color='#fdb0c0')\n",
    "\n",
    "for i in range(len(g.axes)):\n",
    "   for j in range(len(g.axes)):\n",
    "       if j > i:  # This identifies the upper triangle\n",
    "           g.axes[i, j].set_visible(False)  # Hide the axes\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "columns_list = cuisine_features\n",
    "y_column = 'days_cust'\n",
    "\n",
    "# Calculate the number of rows needed to have 3 columns\n",
    "ncols = 3\n",
    "nrows = math.ceil(len(columns_list) / ncols)  # This rounds up to ensure enough rows\n",
    "\n",
    "# Set up the subplots\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(5 * ncols, 5 * nrows))\n",
    "\n",
    "# Flatten axes for easy iteration if there's more than one row\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through each column and create a regression plot on each subplot\n",
    "for i, col in enumerate(columns_list):\n",
    "    sns.regplot(x=col, y=y_column, data=subset_df[subset_df[col] > 0], ax=axes[i], scatter_kws={'alpha': 0.1})\n",
    "    axes[i].set_title(f'Regression of {y_column} on {col}')\n",
    "\n",
    "# Hide any unused axes (if the number of plots is not a multiple of ncols)\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix for the hour features\n",
    "hour_correlation = subset_df[hour_features].corr()\n",
    "\n",
    "# Create a clustermap using the correlation matrix\n",
    "sns.clustermap(hour_correlation, cmap='vlag', linewidths=0.5, figsize=(10, 8))\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Customer Activity Heatmap by Hour')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = subset_df\n",
    "x = rs.total_amt\n",
    "g = rs.cust_region\n",
    "df = pd.DataFrame(dict(x=x, g=g))\n",
    "#m = df.g.map(ord)\n",
    "df[\"x\"] += df[\"g\"]\n",
    "\n",
    "# Initialize the FacetGrid object\n",
    "pal = sns.cubehelix_palette(10, rot=-.25, light=.7)\n",
    "g = sns.FacetGrid(df, row=\"g\", hue=\"g\", aspect=15, height=1.5, palette=pal)\n",
    "\n",
    "# Draw the densities in a few steps\n",
    "g.map(sns.kdeplot, \"x\",\n",
    "      bw_adjust=.5, clip_on=False,\n",
    "      fill=True, alpha=1, linewidth=1.5)\n",
    "g.map(sns.kdeplot, \"x\", clip_on=False, color=\"w\", lw=2, bw_adjust=.5)\n",
    "\n",
    "# passing color=None to refline() uses the hue mapping\n",
    "g.refline(y=0, linewidth=2, linestyle=\"-\", color=None, clip_on=False)\n",
    "\n",
    "\n",
    "# Define and use a simple function to label the plot in axes coordinates\n",
    "def label(x, color, label):\n",
    "    ax = plt.gca()\n",
    "    ax.text(0, .2, label, fontweight=\"bold\", color=color,\n",
    "            ha=\"left\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "\n",
    "g.map(label, \"x\")\n",
    "g.set(xlabel=\"total_amt\")\n",
    "g.set_titles(size=12)\n",
    "\n",
    "# Set the subplots to overlap\n",
    "#g.figure.subplots_adjust(hspace=-.25)\n",
    "\n",
    "# Remove axes details that don't play well with overlap\n",
    "g.set_titles(\"\")\n",
    "g.set(yticks=[], ylabel=\"\")\n",
    "g.despine(bottom=True, left=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(subset_df, col='top_cuisine', col_wrap=4, height=3)\n",
    "g.map(sns.histplot, 'total_amt', kde=True)\n",
    "\n",
    "plt.subplots_adjust(top=0.9)\n",
    "g.fig.suptitle('Histograms of Total Amount by Cuisine')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_order_total_amt = subset_df.groupby('last_order')['total_amt'].sum().reset_index()\n",
    "\n",
    "# sns.kdeplot(data=last_order_total_amt, x='last_order', y='total_amt')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hour-Week Contingency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dow_sums = subset_df[[f'DOW_{i}' for i in range(7)]].sum()\n",
    "hr_sums = subset_df[[f'HR_{i}' for i in range(24)]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a new DataFrame with each unique (day, hour) combination\n",
    "# heatmap_data = pd.DataFrame(\n",
    "#     {\n",
    "#         'Day': [f'DOW_{i}' for i in range(7)] * 24,\n",
    "#         'Hour': list(range(24)) * 7,\n",
    "#         'Order_Count': [dow_sums[f'DOW_{i}'] * hr_sums[f'HR_{j}'] for i in range(7) for j in range(24)]\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# # Reshape for the heatmap plot\n",
    "# heatmap_pivot = heatmap_data.pivot(index='Day', columns='Hour', values='Order_Count')\n",
    "\n",
    "# # Plot heatmap\n",
    "# plt.figure(figsize=(20, 12))\n",
    "# sns.heatmap(heatmap_pivot, cmap=\"YlGnBu\", annot=True, fmt=\".0f\", cbar_kws={'label': 'Order Count'}, annot_kws={'size': 6}, square=True)\n",
    "# plt.xlabel(\"Hour of the Day\")\n",
    "# plt.ylabel(\"Day of the Week\")\n",
    "# plt.title(\"Order Counts by Time of Day and Day of Week\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Order Count Matrix for Hour-Day Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df['DOW_total'] = subset_df[[f'DOW_{i}' for i in range(7)]].sum(axis=1)\n",
    "for i in range(7):\n",
    "    subset_df[f'Order_DOW_{i}'] = subset_df[f'DOW_{i}'] / subset_df['DOW_total']\n",
    "\n",
    "subset_df['HR_total'] = subset_df[[f'HR_{i}' for i in range(24)]].sum(axis=1)\n",
    "for i in range(24):\n",
    "    subset_df[f'Order_HR_{i}'] = subset_df[f'HR_{i}'] / subset_df['HR_total']\n",
    "\n",
    "subset_df.drop(columns=['DOW_total', 'HR_total'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data = []\n",
    "\n",
    "for i in range(7):  # For each day of the week\n",
    "    for j in range(24):  # For each hour of the day\n",
    "        # Calculate Order Count as the sum of products of the indicators\n",
    "        order_count = (subset_df[f'Order_DOW_{i}'] * subset_df[f'Order_HR_{j}']).sum()\n",
    "        \n",
    "        heatmap_data.append({\n",
    "            'Day': f'DOW_{i}',\n",
    "            'Hour': j,\n",
    "            'Order_Count': order_count\n",
    "        })\n",
    "\n",
    "# Convert heatmap_data into a DataFrame\n",
    "heatmap_df = pd.DataFrame(heatmap_data)\n",
    "\n",
    "# Reshape for the heatmap\n",
    "heatmap_pivot = heatmap_df.pivot(index='Day', columns='Hour', values='Order_Count').fillna(0)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(20, 12))\n",
    "sns.heatmap(heatmap_pivot, cmap=\"YlGnBu\", annot=True, fmt=\".0f\", cbar_kws={'label': 'Order Count'}, annot_kws={'size': 6}, square=True)\n",
    "plt.xlabel(\"Hour of the Day\")\n",
    "plt.ylabel(\"Day of the Week\")\n",
    "plt.title(\"Order Counts by Time of Day and Day of Week\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cramer's V Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7):  # For each day of the week\n",
    "    subset_df[f'Has_DOW_{i}'] = (subset_df[f'DOW_{i}'] > 0).astype(int)\n",
    "\n",
    "for j in range(24):  # For each hour of the day\n",
    "    subset_df[f'Has_HR_{j}'] = (subset_df[f'HR_{j}'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOW_features = [f'Has_DOW_{i}' for i in range(7)]\n",
    "HR_features = [f'Has_HR_{i}' for i in range(24)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(confusion_matrix):\n",
    "    chi2 = stats.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
    "    rcorr = r - 1\n",
    "    kcorr = k - 1\n",
    "    return np.sqrt(phi2corr / min((kcorr, rcorr)))\n",
    "\n",
    "# Initialize a DataFrame to store the Cramér's V values\n",
    "cramer_v_results = pd.DataFrame(index=[f'Has_DOW_{i}' for i in range(7)], columns=[f'Has_HR_{j}' for j in range(24)])\n",
    "\n",
    "# Calculate Cramér's V for each DOW and HR combination\n",
    "for i in range(7):\n",
    "    for j in range(24):\n",
    "        # Create contingency table\n",
    "        contingency_table = pd.crosstab(subset_df[f'Has_DOW_{i}'], subset_df[f'Has_HR_{j}'])\n",
    "        # Calculate Cramér's V\n",
    "        cramer_v_value = cramers_v(contingency_table)\n",
    "        cramer_v_results.loc[f'Has_DOW_{i}', f'Has_HR_{j}'] = cramer_v_value\n",
    "\n",
    "# Optionally plot the heatmap of Cramér's V values\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(cramer_v_results.astype(float), annot=True, cmap='YlGnBu', fmt=\".2f\", annot_kws={'size': 7}, cbar_kws={'label': \"Cramér's V\"}, square=True)\n",
    "plt.title(\"Cramér's V Heatmap between Days of Week and Hours\")\n",
    "plt.xlabel(\"Hours\")\n",
    "plt.ylabel(\"Days of Week\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cramers_v(confusion_matrix):\n",
    "#     chi2 = stats.chi2_contingency(confusion_matrix)[0]\n",
    "#     n = confusion_matrix.sum().sum()\n",
    "#     phi2 = chi2 / n\n",
    "#     r, k = confusion_matrix.shape\n",
    "#     phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
    "#     rcorr = r - 1\n",
    "#     kcorr = k - 1\n",
    "#     return np.sqrt(phi2corr / min((kcorr, rcorr)))\n",
    "\n",
    "# # Initialize a DataFrame to store the Cramér's V values\n",
    "# cramer_v_results = pd.DataFrame(index=[f'Has_DOW_{i}' for i in range(7)], columns=[f'Has_HR_{j}' for j in range(24)])\n",
    "\n",
    "# # Calculate Cramér's V for each DOW and HR combination\n",
    "# for i in range(7):\n",
    "#     for j in range(24):\n",
    "#         # Create contingency table\n",
    "#         contingency_table = pd.crosstab(data[f'Has_DOW_{i}'], data[f'Has_HR_{j}'])\n",
    "#         # Calculate Cramér's V\n",
    "#         cramer_v_value = cramers_v(contingency_table)\n",
    "#         cramer_v_results.loc[f'Has_DOW_{i}', f'Has_HR_{j}'] = cramer_v_value * dow_sums[f'DOW_{i}'] * hr_sums[f'HR_{j}']\n",
    "\n",
    "# # Optionally plot the heatmap of Cramér's V values\n",
    "# plt.figure(figsize=(15, 12))\n",
    "# sns.heatmap(cramer_v_results.astype(float), cmap='YlGnBu', fmt=\".2f\", annot_kws={'size': 7}, cbar_kws={'label': \"Cramér's V\"}, square=True)\n",
    "# plt.title(\"Cramér's V Heatmap between Days of Week and Hours\")\n",
    "# plt.xlabel(\"Hours\")\n",
    "# plt.ylabel(\"Days of Week\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation Weighted Cramer's V Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(confusion_matrix):\n",
    "    chi2 = stats.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
    "    rcorr = r - 1\n",
    "    kcorr = k - 1\n",
    "    return np.sqrt(phi2corr / min((kcorr, rcorr)))\n",
    "\n",
    "# Initialize a DataFrame to store the Cramér's V values\n",
    "cramer_v_results = pd.DataFrame(index=[f'Has_DOW_{i}' for i in range(7)], columns=[f'Has_HR_{j}' for j in range(24)])\n",
    "\n",
    "# Calculate Cramér's V for each DOW and HR combination\n",
    "for i in range(7):\n",
    "    for j in range(24):\n",
    "        # Create contingency table\n",
    "        contingency_table = pd.crosstab(subset_df[f'Has_DOW_{i}'], subset_df[f'Has_HR_{j}'])\n",
    "        # Calculate Cramér's V\n",
    "        cramer_v_value = cramers_v(contingency_table)\n",
    "        cramer_v_results.loc[f'Has_DOW_{i}', f'Has_HR_{j}'] = cramer_v_value * (subset_df[f'Order_DOW_{i}'] * subset_df[f'Order_HR_{j}']).sum()\n",
    "\n",
    "# Optionally plot the heatmap of Cramér's V values\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(cramer_v_results.astype(float), cmap='YlGnBu', fmt=\".2f\", annot_kws={'size': 7}, cbar_kws={'label': \"Cramér's V\"}, square=True)\n",
    "plt.title(\"Cramér's V Heatmap between Days of Week and Hours\")\n",
    "plt.xlabel(\"Hours\")\n",
    "plt.ylabel(\"Days of Week\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Absolute) Pearson's Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_corr = subset_df[DOW_features + HR_features].corr(method='pearson')\n",
    "\n",
    "corr_subset = np.abs(pearson_corr.loc[DOW_features, HR_features])\n",
    "\n",
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "\n",
    "annot = np.full(corr_subset.shape, '', dtype=object)\n",
    "\n",
    "# Fill the annotation array with formatted values for correlations above the threshold\n",
    "for i in range(corr_subset.shape[0]):\n",
    "    for j in range(corr_subset.shape[1]):\n",
    "        if abs(corr_subset.iat[i, j]) >= 0.3:\n",
    "            annot[i, j] = f\"{corr_subset.iat[i, j]:.2f}\" \n",
    "\n",
    "# create the heatmap\n",
    "sns.heatmap(data=corr_subset, annot=True, cmap='YlGnBu', \n",
    "            fmt=\".2f\", square=True, \n",
    "            linewidths=.5, annot_kws={\"size\": 10})\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Pearson Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style='color:yellow'>Expectation Weighted (Absolute of) Pearson's Correlation Heatmap</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_corr = subset_df[DOW_features + HR_features].corr(method='pearson')\n",
    "\n",
    "corr_subset = np.abs(pearson_corr.loc[DOW_features, HR_features])\n",
    "\n",
    "for i in range(7):\n",
    "    for j in range(24):\n",
    "        corr_subset.loc[f'Has_DOW_{i}', f'Has_HR_{j}'] = corr_subset.loc[f'Has_DOW_{i}', f'Has_HR_{j}'] * (subset_df[f'Order_DOW_{i}'] * subset_df[f'Order_HR_{j}']).sum()\n",
    "\n",
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "\n",
    "annot = np.full(corr_subset.shape, '', dtype=object)\n",
    "\n",
    "# Fill the annotation array with formatted values for correlations above the threshold\n",
    "for i in range(corr_subset.shape[0]):\n",
    "    for j in range(corr_subset.shape[1]):\n",
    "        if abs(corr_subset.iat[i, j]) >= 0.3:\n",
    "            annot[i, j] = f\"{corr_subset.iat[i, j]:.2f}\" \n",
    "\n",
    "# create the heatmap\n",
    "sns.heatmap(data=corr_subset, cmap='YlGnBu', \n",
    "            fmt=\"\", square=True\n",
    "            )\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Pearson Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pearson_corr = data[DOW_features + HR_features].corr(method='pearson')\n",
    "\n",
    "# corr_subset = pearson_corr.loc[DOW_features, HR_features]\n",
    "\n",
    "# for i in range(7):\n",
    "#     for j in range(24):\n",
    "#         corr_subset.loc[f'Has_DOW_{i}', f'Has_HR_{j}'] = corr_subset.loc[f'Has_DOW_{i}', f'Has_HR_{j}'] * dow_sums[f'DOW_{i}'] * hr_sums[f'HR_{j}']\n",
    "\n",
    "# plt.figure(figsize=(15,12))\n",
    "\n",
    "\n",
    "# annot = np.full(corr_subset.shape, '', dtype=object)\n",
    "\n",
    "# # Fill the annotation array with formatted values for correlations above the threshold\n",
    "# for i in range(corr_subset.shape[0]):\n",
    "#     for j in range(corr_subset.shape[1]):\n",
    "#         if abs(corr_subset.iat[i, j]) >= 0.3:\n",
    "#             annot[i, j] = f\"{corr_subset.iat[i, j]:.2f}\" \n",
    "\n",
    "# # create the heatmap\n",
    "# sns.heatmap(data=corr_subset, cmap='YlGnBu', \n",
    "#             fmt=\"\", square=True, \n",
    "#             annot_kws={\"size\": 10})\n",
    "\n",
    "# # Show the plot\n",
    "# plt.title('Pearson Correlation Heatmap')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_vendor_by_age = subset_df.groupby('cust_age')['n_vendor'].mean().reset_index()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.lineplot(data=average_vendor_by_age, x='cust_age', y='n_vendor', marker='o', alpha=0.6)\n",
    "plt.title('Average Vendor Count by Customer Age')\n",
    "plt.xlabel('Customer Age')\n",
    "plt.ylabel('Average Vendor Count')\n",
    "plt.xticks(np.arange(average_vendor_by_age['cust_age'].min(), \n",
    "                       average_vendor_by_age['cust_age'].max() + 1, 3))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
