{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced hour features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the values of the columns as a NumPy array\n",
    "values = data[hour_features].values\n",
    "\n",
    "# Step 2: Use argsort to get the indices that would sort each row\n",
    "sorted_indices = np.argsort(values, axis=1)\n",
    "\n",
    "# Step 3: Get the index of the largest and second-largest column for each row\n",
    "max_col_indices = sorted_indices[:, -1]  # Last column is the highest\n",
    "second_max_col_indices = sorted_indices[:, -2]  # Second to last column is the second-highest\n",
    "\n",
    "# Step 4: Check if the second highest value is zero (i.e., all other values are zero)\n",
    "\n",
    "second_max_values = np.take_along_axis(values, second_max_col_indices[:, None], axis=1).flatten()\n",
    "\n",
    "# Step 5: Map the indices back to column names, or use None if second max is zero\n",
    "data.loc[:, '_1_modal_hr'] = np.array(hour_features)[max_col_indices]\n",
    "data.loc[:, '_2_modal_hr'] = np.where(second_max_values == 0, None, np.array(hour_features)[second_max_col_indices])\n",
    "\n",
    "data.loc[:, '_1_modal_hr'] = data['_1_modal_hr'].apply(lambda x: int(x.split('_')[1]) if isinstance(x, str) else np.nan)\n",
    "data.loc[:, '_2_modal_hr'] = data['_2_modal_hr'].apply(lambda x: int(x.split('_')[1]) if isinstance(x, str) else np.nan)\n",
    "\n",
    "# Step 6: Store actual values corresponding to mode_1 and mode_2\n",
    "data.loc[:, 'n_order_1_modal_hr'] = values[np.arange(values.shape[0]), max_col_indices]\n",
    "data.loc[:, 'n_order_2_modal_hr'] = np.where(second_max_values == 0, 0, values[np.arange(values.shape[0]), second_max_col_indices])\n",
    "\n",
    "# Propensity to consume on hours that are modal\n",
    "data['modal_hr_prop'] = (data.loc[:, 'n_order_1_modal_hr'] + data.loc[:, 'n_order_2_modal_hr']) / data['n_order']\n",
    "\n",
    "# Parsing 1_mode\n",
    "data['_1_modal_hr_rad'] = (data['_1_modal_hr'] * (2 * np.pi / 24)).astype(float)\n",
    "\n",
    "# Calculate the mean of sine and cosine components from both sets of radians\n",
    "mean_sin = np.mean(np.sin(data['_1_modal_hr_rad']))\n",
    "mean_cos = np.mean(np.cos(data['_1_modal_hr_rad']))\n",
    "\n",
    "circular_mean = np.arctan2(mean_sin, mean_cos) % (2 * np.pi)\n",
    "\n",
    "data['position_1_hr_mode'] = data['_1_modal_hr_rad'].apply(lambda x: 1 if (x - circular_mean) > 0 else 0)\n",
    "\n",
    "# Parsing 2_mode\n",
    "data['_2_modal_hr_rad'] = (data['_2_modal_hr'] * (2 * np.pi / 24)).astype(float)\n",
    "\n",
    "# Calculate the mean of sine and cosine components from both sets of radians\n",
    "mean_sin = np.mean(np.sin(data['_2_modal_hr_rad']))\n",
    "mean_cos = np.mean(np.cos(data['_2_modal_hr_rad']))\n",
    "\n",
    "circular_mean = np.arctan2(mean_sin, mean_cos) % (2 * np.pi)\n",
    "\n",
    "data['position_2_hr_mode'] = data['_2_modal_hr_rad'].apply(lambda x: 1 if (x - circular_mean) > 0 else 0)\n",
    "\n",
    "# Parsing the average of both modes\n",
    "data['mean_modal_hr_rad'] = data.apply(\n",
    "    lambda row: np.arctan2(\n",
    "        (np.sin(row['_1_modal_hr_rad']) + np.sin(row['_2_modal_hr_rad'])) / 2,  # mean sine\n",
    "        (np.cos(row['_1_modal_hr_rad']) + np.cos(row['_2_modal_hr_rad'])) / 2   # mean cosine\n",
    "    ) % (2 * np.pi),  # Ensure result is in [0, 2*pi]\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Calculate the mean of sine and cosine components from both sets of radians\n",
    "mean_sin = np.mean(np.sin(data['_1_modal_hr_rad'])) + np.mean(np.sin(data['_2_modal_hr_rad'])) / 2\n",
    "mean_cos = np.mean(np.cos(data['_1_modal_hr_rad'])) + np.mean(np.cos(data['_2_modal_hr_rad'])) / 2\n",
    "\n",
    "circular_mean = np.arctan2(mean_sin, mean_cos) % (2 * np.pi)\n",
    "\n",
    "data['position_mean_hr_mode'] = data['mean_modal_hr_rad'].apply(lambda x: 1 if (x - circular_mean) > 0 else 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced day features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the values of the columns as a NumPy array\n",
    "values = data[day_features].values\n",
    "\n",
    "# Step 2: Use argsort to get the indices that would sort each row\n",
    "sorted_indices = np.argsort(values, axis=1)\n",
    "\n",
    "# Step 3: Get the index of the largest and second-largest column for each row\n",
    "max_col_indices = sorted_indices[:, -1]  # Highest value column\n",
    "second_max_col_indices = sorted_indices[:, -2]  # Second highest column\n",
    "\n",
    "# Step 4: Check if the second highest value is zero\n",
    "second_max_values = np.take_along_axis(values, second_max_col_indices[:, None], axis=1).flatten()\n",
    "\n",
    "# Step 5: Map the indices back to column names, or use None if second max is zero\n",
    "data.loc[:, '_1_modal_day'] = np.array(day_features)[max_col_indices]\n",
    "data.loc[:, '_2_modal_day'] = np.where(second_max_values == 0, None, np.array(day_features)[second_max_col_indices])\n",
    "\n",
    "data.loc[:, '_1_modal_day'] = data['_1_modal_day'].apply(lambda x: int(x.split('_')[1]) if isinstance(x, str) else np.nan)\n",
    "data.loc[:, '_2_modal_day'] = data['_2_modal_day'].apply(lambda x: int(x.split('_')[1]) if isinstance(x, str) else np.nan)\n",
    "\n",
    "# Step 6: Store values for mode_1 and mode_2\n",
    "data.loc[:, 'n_order_1_modal_day'] = values[np.arange(values.shape[0]), max_col_indices]\n",
    "data.loc[:, 'n_order_2_modal_day'] = np.where(second_max_values == 0, 0, values[np.arange(values.shape[0]), second_max_col_indices])\n",
    "\n",
    "# Propensity to consume on modal days\n",
    "data['modal_day_prop'] = (data['n_order_1_modal_day'] + data['n_order_2_modal_day']) / data['n_order']\n",
    "\n",
    "# Parsing 1st mode in radians\n",
    "data['_1_modal_day_rad'] = (data['_1_modal_day'] * (2 * np.pi / 24)).astype(float)\n",
    "\n",
    "# Mean of sine and cosine components for 1st modal day\n",
    "mean_sin = np.mean(np.sin(data['_1_modal_day_rad']))\n",
    "mean_cos = np.mean(np.cos(data['_1_modal_day_rad']))\n",
    "circular_mean = np.arctan2(mean_sin, mean_cos) % (2 * np.pi)\n",
    "\n",
    "data['position_1_day_mode'] = data['_1_modal_day'].apply(lambda x: 1 if (x - circular_mean) > 0 else 0)\n",
    "\n",
    "# Parsing 2nd mode in radians\n",
    "data['_2_modal_day_rad'] = (data['_2_modal_day'] * (2 * np.pi / 24)).astype(float)\n",
    "\n",
    "# Mean of sine and cosine components for 2nd modal day\n",
    "mean_sin = np.mean(np.sin(data['_2_modal_day_rad']))\n",
    "mean_cos = np.mean(np.cos(data['_2_modal_day_rad']))\n",
    "circular_mean = np.arctan2(mean_sin, mean_cos) % (2 * np.pi)\n",
    "\n",
    "data['position_2_day_mode'] = data['_2_modal_day_rad'].apply(lambda x: 1 if (x - circular_mean) > 0 else 0)\n",
    "\n",
    "# Parsing the average of both modes\n",
    "data['mean_modal_day_rad'] = data.apply(\n",
    "    lambda row: np.arctan2(\n",
    "        (np.sin(row['_1_modal_day_rad']) + np.sin(row['_2_modal_day_rad'])) / 2,  # mean sine\n",
    "        (np.cos(row['_1_modal_day_rad']) + np.cos(row['_2_modal_day_rad'])) / 2   # mean cosine\n",
    "    ) % (2 * np.pi),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Mean of sine and cosine components from both sets of radians\n",
    "mean_sin = np.mean(np.sin(data['_1_modal_day_rad'])) + np.mean(np.sin(data['_2_modal_day_rad'])) / 2\n",
    "mean_cos = np.mean(np.cos(data['_1_modal_day_rad'])) + np.mean(np.cos(data['_2_modal_day_rad'])) / 2\n",
    "circular_mean = np.arctan2(mean_sin, mean_cos) % (2 * np.pi)\n",
    "\n",
    "data['position_mean_day_mode'] = data['mean_modal_day_rad'].apply(lambda x: 1 if (x - circular_mean) > 0 else 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asking better questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we would not consider to have done anything that could be called, transforming data into knowledge, for that we need to ask deeper, harder questions. Below is a more or less comprehensive list of the questions that we would like to obtain from our data, that we believe will allow us to understand the drivers of demand, get to know our customer base and general trends about the industry.\n",
    "<br>\n",
    "1. **Who is the customer?**\n",
    "    - How old is he/she?\n",
    "    - How do they pay?\n",
    "    - What do they eat?\n",
    "    - How much do they spend?\n",
    "    - When do they spend it?\n",
    "    - Are they representative of the population?<br>\n",
    "<br>\n",
    "2. **Demand vis. categorical features?** <br><br>\n",
    "3. **Demand and age?**\n",
    "    - Can we construct groups based on age?\n",
    "    - Do age groups capture different behaviours with respect to demand?\n",
    "    - What about demand as a function of categorical variables?\n",
    "    - How does propensity to spend vary with age, are there exceptions, are these exceptions representative?<br>\n",
    "<br>\n",
    "5. **Demand vis. hour, and vis. day of week?**\n",
    "    - Does propensity for a cuisine type vary with HR and DOW values?\n",
    "    - Are orders at different hours more or less sensistive to promotions?\n",
    "    - What about different days of the week?<br>\n",
    "<br>\n",
    "6. **Customer Base evolution in the past 3 months?**\n",
    "    - Are there any differences between the customer base from 30, 60 or 180 days to today?\n",
    "    - Have promotional campaign effective?\n",
    "    - Has the customer base grown or shrunk?\n",
    "    - Has revenue increased or decreased?\n",
    "    - How frequently do customers order on average?\n",
    "    - How many times do customers order per week?<br>\n",
    "<br>\n",
    "8. **To chain or not to chain?**\n",
    "    - Is there a relation between the ratio of chained restaurants and other aspects of demand?\n",
    "    - Does a high ratio imply more demand?\n",
    "    - Is it linked to a particular type of cuisine?\n",
    "    - To a particular schedule?<br>\n",
    "<br>\n",
    "10. **What drives frequent costumers?**\n",
    "    - Who qualifies as a frequent customer? \n",
    "    - Which payment method is prefered by frequent customers?\n",
    "    - Is the number of frequent customers increasing? <br>\n",
    "<br>\n",
    "11. **How does demand vary as a function of type of cuisine?**\n",
    "    - Does cuisine vary by region?\n",
    "    - By age group?\n",
    "    - By customer propensity?\n",
    "    - Is there such a thing as \"Taco Tuesdays\"?<br>\n",
    "<br>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lorenz Curve of accumulated sales as a function of sales percentiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style to 'white'\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Sort the revenue data\n",
    "revenues = data['total_amt'].sort_values()\n",
    "pct_chain = data['pct_chain'].loc[revenues.index]  # Get corresponding values\n",
    "num_customers = len(revenues)\n",
    "\n",
    "# Calculate cumulative revenue\n",
    "cumulative_revenue = revenues.cumsum()\n",
    "\n",
    "# Calculate total revenue and cumulative revenue proportion\n",
    "total_revenue = cumulative_revenue.iloc[-1]\n",
    "cumulative_revenue_proportion = cumulative_revenue / total_revenue\n",
    "\n",
    "# Calculate cumulative customer proportions\n",
    "cumulative_customers_proportion = np.arange(1, num_customers + 1) / num_customers\n",
    "\n",
    "# Prepare the Lorenz curve data\n",
    "cumulative_customers_proportion = np.insert(cumulative_customers_proportion, 0, 0)  # Add (0, 0) as the start point\n",
    "cumulative_revenue_proportion = np.insert(cumulative_revenue_proportion, 0, 0)  # Same as above\n",
    "\n",
    "# Calculate Gini Coefficient\n",
    "A = np.trapz(cumulative_revenue_proportion, cumulative_customers_proportion)\n",
    "Gini_coefficient = (0.5 - A) / 0.5  # Normalizing by the area under the line of equality\n",
    "\n",
    "# Plot the Lorenz Curve with filled area based on 'pct_chain'\n",
    "plt.figure(figsize=(10, 9))\n",
    "\n",
    "# Create a colormap for the first Lorenz curve based on pct_chain\n",
    "cmap = plt.get_cmap('Blues')\n",
    "colors = cmap(pct_chain.sort_values()) # Get colors based on sorted pct_chain values\n",
    "\n",
    "# Fill the area under the Lorenz curve using the colors obtained from pct_chain\n",
    "for i in range(num_customers):\n",
    "    plt.fill_between(\n",
    "        cumulative_customers_proportion[i:i + 2]\n",
    "        , cumulative_revenue_proportion[i:i + 2]\n",
    "        , color=colors[i]\n",
    "        , alpha=0.9\n",
    "    )\n",
    "\n",
    "# Plotting the Lorenz curve\n",
    "plt.plot(\n",
    "        cumulative_customers_proportion\n",
    "        , cumulative_revenue_proportion\n",
    "        , color='Black'\n",
    "        , linestyle='-.'\n",
    "        , linewidth=2\n",
    "        , label=f\"Lorenz Curve (Gini: {Gini_coefficient:.2f})\"\n",
    "    )\n",
    "\n",
    "# Ensure graph starts at 0 and ends at 1, for both x and y.\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Plotting the line of equality\n",
    "plt.plot(\n",
    "    [0, 1]\n",
    "    , [0, 1]\n",
    "    , color='crimson'\n",
    "    , linestyle='--',\n",
    "    label='Line of Equality'\n",
    ")\n",
    "\n",
    "# Setting title and labels for graph\n",
    "plt.title('Cummulative Customer Revenue with % in chain purchase gradient', weight='bold',pad=20)\n",
    "plt.xlabel('Cumulative Proportion of Customers')\n",
    "plt.ylabel('Cumulative Proportion of Revenue')\n",
    "plt.legend(loc='upper left')  # Set a specific location for the legend\n",
    "\n",
    "# Create a color bar for the pct_chain gradient\n",
    "norm = plt.Normalize(vmin=pct_chain.min(), vmax=pct_chain.max()) # Create a normalized scale based on pct_chain values\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm) # Map these values to a scalar map\n",
    "\n",
    "# Create a color bar with extra padding\n",
    "cbar = plt.colorbar(sm, ax=plt.gca(), orientation='vertical', pad=0.05) \n",
    "cbar.set_label('Percentile of % purchases in chained restaurant', labelpad=25)\n",
    "\n",
    "# Set the ticks at specific data values (min, 25th percentile, etc.)\n",
    "cbar.ax.set_yticks([\n",
    "    pct_chain.min(),\n",
    "    pct_chain.quantile(0.25),\n",
    "    pct_chain.median(),\n",
    "    pct_chain.quantile(0.55),\n",
    "    pct_chain.quantile(0.59),\n",
    "    pct_chain.max()\n",
    "])\n",
    "\n",
    "# Set the corresponding custom labels\n",
    "cbar.ax.set_yticklabels([\n",
    "    f\"{round(pct_chain.min(), 2)}\",\n",
    "    f\"{round(pct_chain.quantile(0.25), 2)}\",\n",
    "    f\"{round(pct_chain.median(), 2)}\",\n",
    "    f\"{round(pct_chain.quantile(0.55), 2)}\",\n",
    "    f\"{round(pct_chain.quantile(0.59), 2)}\",\n",
    "    f\"{round(pct_chain.max(), 2)}\"\n",
    "])\n",
    "\n",
    "\n",
    "quantile_labels = ['0', '25', '50', '55', '59', '100'] # Add quantile labels on the left side of the colorbar\n",
    "tick_positions = cbar.ax.get_yticks()  # Get the positions of the ticks\n",
    "\n",
    "for i, label in enumerate(quantile_labels):\n",
    "    cbar.ax.text(\n",
    "        -0.1\n",
    "        , tick_positions[i]\n",
    "        , label\n",
    "        , ha='right'\n",
    "        , va='center'\n",
    "        , fontstyle='italic'\n",
    "        , fontsize=7\n",
    "        , color='black',\n",
    "        bbox=dict(facecolor='white', alpha=0, edgecolor='none', boxstyle='round,pad=0.5')\n",
    "    ) # Set text labels on the left side of the gradient\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this Lorenz Curve instead represented income inequality, it would be 13th on the list according to Wikipedia a.k.a the Democratic Republic of the Congo. We point out that almost 80% of total revenue is coming the top 40th percentile of the customer base.\n",
    "\n",
    "In a flicker of inspiration we decided to plot the gradient of % in chain purchases under the Lorents curve of per customer revenue, which incidentily by the laws of calculus tells us the exact ammount spent on chained restaurants, but more interestingly it shows a clear threshold, at the quantile we had mentioned previously. Note then that, what we are seeing is that the top 40th percentile of customers, which are responsible for those 80% of revenue, order all of their meals from chained restaurants. \n",
    "\n",
    "This is too big a finding not to consider studying populations resulting from this rule in isolation. We can deduce that in some way shape or form, the top 40th consumers are those whose routine revolves around a predictable demand for ABCDEats affiliates products, and the other group represent a much less homogenous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Sales as function of Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Set Seaborn style to 'white'\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Sort the data by age\n",
    "sorted_data = data.sort_values('cust_age')\n",
    "ages = sorted_data['cust_age'].values  # Use age as the primary variable\n",
    "sales = sorted_data['total_amt'].values  # Corresponding sales based on sorted ages\n",
    "num_customers = len(sales)\n",
    "\n",
    "# Calculate cumulative sales\n",
    "cumulative_sales = sales.cumsum()  # Correct calculation of cumulative sales\n",
    "\n",
    "# Create a figure with two subplots (one for the cumulative sales curve and one for the customer distribution)\n",
    "plt.figure(figsize=(10, 14))\n",
    "\n",
    "# First subplot: Plotting the accumulation curve\n",
    "plt.subplot(2, 1, 1)  # Two rows, one column, first plot\n",
    "plt.plot(\n",
    "    ages,  # Use ages for x-axis\n",
    "    cumulative_sales,  # Cumulative sales for the y-axis\n",
    "    color='Black',\n",
    "    linestyle='-',\n",
    "    linewidth=2,\n",
    "    label='Accumulation Curve'\n",
    ")\n",
    "\n",
    "# Plotting the line of equality\n",
    "plt.plot(\n",
    "    [ages.min(), ages.max()],  # X values for the line of equality\n",
    "    [0, cumulative_sales[-1]],  # Y values from 0 to total cumulative sales\n",
    "    color='crimson',\n",
    "    linestyle='--',\n",
    "    label='Line of Equality'\n",
    ")\n",
    "\n",
    "# Total sales amount\n",
    "total_sales = cumulative_sales[-1]  \n",
    "\n",
    "# Add horizontal lines at every 5% of the total sales\n",
    "for i in range(1, 21):  # 1 to 20 for 5% increments\n",
    "    plt.plot([ages.min(), ages.max()], [i * 0.05 * total_sales, i * 0.05 * total_sales], color='black', linestyle='--', linewidth=0.7, alpha=0.5)\n",
    "\n",
    "# Add vertical lines at 80%, 90%, and 95%\n",
    "percentiles = [0.80, 0.90, 0.95]\n",
    "age_percentile_values = []  # List to store age values for percentiles\n",
    "cumulative_sales_values = []  # List to store cumulative sales values for percentiles\n",
    "\n",
    "for p in percentiles:\n",
    "    age_at_percentile = np.percentile(ages, p * 100)  # Get the corresponding age for the percentile\n",
    "    age_percentile_values.append(age_at_percentile)  # Store the value\n",
    "    \n",
    "    # Calculate the y-value for the intersection with the accumulation curve\n",
    "    sales_at_percentile_index = int(num_customers * p)  # Get index for the current percentile\n",
    "    y_value = cumulative_sales[sales_at_percentile_index - 1]  # Cumulative sales at this percentile\n",
    "    \n",
    "    # Plot vertical line that ends at the intersection point\n",
    "    plt.plot([age_at_percentile, age_at_percentile], [0, y_value], color='blue', linestyle='--', linewidth=1)\n",
    "    \n",
    "    # Plot horizontal line that ends at the intersection point\n",
    "    plt.plot([ages.min(), age_at_percentile], [y_value, y_value], color='blue', linestyle='--', linewidth=1)\n",
    "\n",
    "    # Store cumulative sales for this percentile\n",
    "    cumulative_sales_values.append(y_value)  # Store cumulative sales for this percentile\n",
    "\n",
    "# Add the total sales (100th percentile) to the cumulative sales values\n",
    "cumulative_sales_values.append(total_sales)\n",
    "\n",
    "# Set tick labels for the respective age percentiles on the x-axis\n",
    "xticks_labels = [\n",
    "    f'Min: {int(ages.min())}',  # Add min age label\n",
    "    f'80%: {int(age_percentile_values[0])}',\n",
    "    f'90%: {int(age_percentile_values[1])}',\n",
    "    f'95%: {int(age_percentile_values[2])}',\n",
    "    f'Max: {int(ages.max())}'  # Add max age label\n",
    "]\n",
    "\n",
    "plt.xticks(\n",
    "    [ages.min()] + list(age_percentile_values) + [ages.max()],  # Add min age to ticks\n",
    "    xticks_labels,\n",
    "    rotation=45, ha='right', fontsize=11, color='black'  # Rotate and set color for better visibility\n",
    ")\n",
    "\n",
    "# Format y-ticks for cumulative sales in Euros\n",
    "def euro_formatter(x, pos):\n",
    "    return f'€{int(x):,}'\n",
    "\n",
    "plt.yticks(cumulative_sales_values, [f'80%: €{int(cumulative_sales_values[0])}', \n",
    "                                     f'90%: €{int(cumulative_sales_values[1])}', \n",
    "                                     f'95%: €{int(cumulative_sales_values[2])}', \n",
    "                                     f'100%: €{int(cumulative_sales_values[3])}'],\n",
    "           fontsize=11, color='black')  # Set font size and color for y-ticks\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(euro_formatter))\n",
    "\n",
    "# Adjust the position and rotation of y-tick labels\n",
    "for label in plt.gca().get_yticklabels():\n",
    "    label.set_rotation(45)  # Rotate y-tick labels to 45 degrees\n",
    "    label.set_y(label.get_position()[1] - 0.02)  # Pad down slightly\n",
    "\n",
    "# Setting title and labels for the first plot\n",
    "plt.title('Cumulative Sales as a Function of Age', weight='bold', pad=20, fontsize=20)\n",
    "plt.xlabel('Age', fontsize=14, fontweight='bold', labelpad=-27)  # Bold x-label with padding\n",
    "plt.ylabel('Cumulative Sales', fontsize=14, fontweight='bold', labelpad=-35)  # Bold y-label with padding\n",
    "plt.ylim(0, total_sales)  # Ensure y-axis starts at 0 and ends slightly above max cumulative sales\n",
    "plt.xlim(min(ages), max(ages))\n",
    "\n",
    "plt.legend(loc='lower right')  # Set a specific location for the legend to bottom right\n",
    "\n",
    "# Second subplot: Customer distribution by age\n",
    "plt.subplot(2, 1, 2)  # Second plot (2 rows, 1 column, second plot)\n",
    "\n",
    "# Plot histogram to show customer distribution by age\n",
    "plt.hist(ages, bins=20, color='grey', alpha=0.7)  # Use a histogram to show the distribution\n",
    "\n",
    "# Set title and labels for the second plot\n",
    "plt.title('Customer Distribution by Age', weight='bold', fontsize=16)\n",
    "plt.xlabel('Age', fontsize=14, fontweight='bold', labelpad=-5)  # Bold x-label with padding\n",
    "plt.ylabel('Number of Customers', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Adjust layout to ensure the two plots fit nicely without overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show both plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Sales as functioons of Customer Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style to 'white'\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Ensure 'cust_region' is treated as an ordered categorical\n",
    "data['cust_region'] = pd.Categorical(data['cust_region']).as_ordered()\n",
    "\n",
    "# Group by customer region and sum up sales for each region\n",
    "grouped_data = data.groupby('cust_region', observed=True)['total_amt'].sum().reset_index()\n",
    "\n",
    "# Sort the grouped data by total sales (descending order for highest sales first)\n",
    "sorted_data = grouped_data.sort_values('total_amt', ascending=False)\n",
    "\n",
    "# Get mapped region labels\n",
    "mapped_region_labels = list(get_mapping(_dict=_region_dict))  # Get region labels from mapping\n",
    "region_sales = sorted_data['total_amt'].values  # Total sales for each region\n",
    "\n",
    "# Calculate cumulative sales by region\n",
    "cumulative_sales = region_sales.cumsum()  # Cumulative sales per region\n",
    "\n",
    "# Ensure the plot starts at 0\n",
    "cumulative_sales = np.insert(cumulative_sales, 0, 0)  # Insert 0 at the start of the cumulative sales\n",
    "\n",
    "# Calculate cumulative percentages\n",
    "total_sales = region_sales.sum()  # Total sales for percentage calculations\n",
    "cumulative_percentages = (cumulative_sales / total_sales) * 100  # Cumulative percentage calculations\n",
    "\n",
    "# Plotting the accumulation curve by region with dots\n",
    "plt.figure(figsize=(10, 12))  # Increase width by 2 points, adjust height as needed\n",
    "\n",
    "# Create a scatter plot for cumulative sales\n",
    "plt.subplot(2, 1, 1)  # First subplot for the scatter plot\n",
    "plt.scatter(\n",
    "    mapped_region_labels,  # Use region names for x-axis\n",
    "    cumulative_sales[1:],  # Cumulative sales for the y-axis (exclude the first 0)\n",
    "    color='Black',\n",
    "    label='Cumulative Sales Points',\n",
    "    s=100  # Size of the dots\n",
    ")\n",
    "\n",
    "# Optional: Add a line connecting the dots\n",
    "plt.plot(\n",
    "    mapped_region_labels,  # Use region names for x-axis\n",
    "    cumulative_sales[1:],  # Cumulative sales for the y-axis (exclude the first 0)\n",
    "    color='gray',  # Line color\n",
    "    linestyle='--',  # Line style\n",
    "    linewidth=1,  # Line width\n",
    "    alpha=0.5  # Line transparency\n",
    ")\n",
    "\n",
    "# Add cumulative percentage annotations to the scatter plot\n",
    "for i, cum_percent in enumerate(cumulative_percentages[1:]):  # Skip the first value (0)\n",
    "    plt.annotate(f\"{cum_percent:.1f}%\", \n",
    "                 (mapped_region_labels[i], cumulative_sales[i + 1]),  # Adjust to use cumulative_sales for y-value\n",
    "                 textcoords=\"offset points\", \n",
    "                 xytext=(0, -15),  # Move text down 15 points\n",
    "                 ha='center', \n",
    "                 fontsize=10)\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Cumulative Sales by Region', weight='bold', fontsize=18)\n",
    "plt.xlabel('Region', fontsize=14)\n",
    "plt.ylabel('Accumulated Sales (€)', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Format y-axis to show in dollars\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "def dollar_formatter(x, _):\n",
    "    return f'${x:,.0f}'  # Format the number as dollars\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(dollar_formatter))\n",
    "\n",
    "# Ensure y-axis starts at 0\n",
    "plt.ylim(bottom=0)\n",
    "\n",
    "# Show plot for scatter\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "\n",
    "# Create a bar plot beneath the scatter plot\n",
    "ax2 = plt.subplot(2, 1, 2)  # Second subplot for the bar plot\n",
    "ax2.bar(mapped_region_labels, region_sales, color='grey', alpha=0.6, label='Sales by Region')\n",
    "ax2.set_ylabel('Sales (€)', fontsize=14)\n",
    "\n",
    "# Add annotations for proportional sales on the bar plot\n",
    "for i, sale in enumerate(region_sales):\n",
    "    # Calculate the proportional sales as a percentage of total sales\n",
    "    proportional_sales = (sale / total_sales) * 100\n",
    "    ax2.annotate(f\"{proportional_sales:.1f}%\", \n",
    "                 (mapped_region_labels[i], sale), \n",
    "                 textcoords=\"offset points\", \n",
    "                 xytext=(0, 5),  # Move text up by 5 points\n",
    "                 ha='center', \n",
    "                 fontsize=10)\n",
    "\n",
    "# Set the legend for the bar plot\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.title('Sales Distribution by Region', weight='bold', fontsize=18)  # Title for the bar plot\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Adjust the height of the bottom plot\n",
    "plt.subplots_adjust(hspace=0.3)  # Increase space between plots if needed\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to ensure everything fits\n",
    "plt.show()  # Show all plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Entropy Discretization Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_bins(dataframe, target, binner):\n",
    "\n",
    "    # read dataset\n",
    "    X, y = dataframe[[target]].values, dataframe[[binner]].values\n",
    "    numeric_features = np.arange(X.shape[1])  # This feature will be discretized\n",
    "    \n",
    "    # Split between training and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "    \n",
    "    #Initialize discretizer object and fit to training data\n",
    "    discretizer = MDLP_Discretizer(features=numeric_features)\n",
    "    discretizer.fit(X_train, y_train)\n",
    "    \n",
    "    X_train_discretized = discretizer.transform(X_train)\n",
    "    \n",
    "    # apply same discretization to test set\n",
    "    X_test_discretized = discretizer.transform(X_test)\n",
    "\n",
    "    # Easiest is to manually adjust the bins, \n",
    "    while True:\n",
    "        try:\n",
    "            _bins = list(\n",
    "                set(\n",
    "                    np.round(\n",
    "                        [\n",
    "                            float(x) for x in discretizer._cuts[0]\n",
    "                        ] \n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            _bins = [0] + sorted(_bins) + [9999]\n",
    "             \n",
    "            dataframe[f'{target}_bin_{binner}'] = (\n",
    "                pd.cut(\n",
    "                    dataframe[target]\n",
    "                    , bins= _bins\n",
    "                    , labels=False\n",
    "                    , include_lowest=True\n",
    "                    , right=True\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            return (f'{target}_bin_{binner}', (target, _bins))\n",
    "            \n",
    "        except Exception as e:\n",
    "            input(f'Error was {e}: press any to cry.')\n",
    "            pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_bins(data, target_feature, feature_lists):\n",
    "    \"\"\"\n",
    "    Processes binning for the given feature lists and updates the binning dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        data (DataFrame): The dataset to process.\n",
    "        target_feature (str): The target feature for binning.\n",
    "        feature_lists (list): A list of lists, where each sub-list contains features to process.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated binning dictionary.\n",
    "    \"\"\"\n",
    "    binning_dict = {}\n",
    "    get_bins_func = partial(_get_bins, data, target_feature)\n",
    "\n",
    "    for features in feature_lists:\n",
    "        for feature in features:\n",
    "            try:\n",
    "                key, value = get_bins_func(feature)\n",
    "                binning_dict[key] = value\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing feature '{feature}': {e}\")\n",
    "\n",
    "    return binning_dict\n",
    "\n",
    "\n",
    "def set_dicts(data, extende_r, extende_d):\n",
    "    for key, value in extende_r.items():\n",
    "        data[key] = (\n",
    "            pd.cut(\n",
    "                data[value[0]]\n",
    "                , bins= value[1]\n",
    "                , labels=False\n",
    "                , include_lowest=True\n",
    "                , right=True\n",
    "            )\n",
    "        )\n",
    "    return extende_d.extend(extende_r.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Correlator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polynomial(data, metric_features, selected_target, degree=2):\n",
    "    \"\"\"\n",
    "    Generates polynomial features from the given metric features, calculates their correlations\n",
    "    with the target variable, and identifies the features with the highest Pearson and Spearman correlations.\n",
    "\n",
    "    Args:\n",
    "    data (pd.DataFrame): The input DataFrame containing the features and target.\n",
    "    metric_features (list): List of metric features/columns.\n",
    "    selected_target (str): The target variable/column to correlate with.\n",
    "    degree (int): The degree of the polynomial features to generate. Default is 2.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Feature with the highest Pearson correlation and its value,\n",
    "           Feature with the highest Spearman correlation and its value.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Drop any features that have the target in their name\n",
    "    features = [feature for feature in metric_features if feature not in selected_target]\n",
    "\n",
    "    # Generate polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=True)\n",
    "    X_poly = poly.fit_transform(data[features])\n",
    "\n",
    "    # Create a DataFrame for polynomial features\n",
    "    poly_features = pd.DataFrame(X_poly, columns=poly.get_feature_names_out(features))\n",
    "\n",
    "    # Calculate correlations with the target variable\n",
    "    correlations = {}\n",
    "    for col in poly_features.columns:\n",
    "        pearson_corr, _ = pearsonr(poly_features[col], data[selected_target])\n",
    "        spearman_corr, _ = spearmanr(poly_features[col], data[selected_target])\n",
    "        correlations[col] = {\n",
    "            'pearson': abs(pearson_corr),\n",
    "            'spearman': abs(spearman_corr)\n",
    "        }\n",
    "\n",
    "    # Get the feature with the highest Pearson correlation\n",
    "    max_pearson_feature = max(correlations, key=lambda x: correlations[x]['pearson'])\n",
    "    max_pearson_corr = correlations[max_pearson_feature]['pearson']\n",
    "\n",
    "    # Get the feature with the highest Spearman correlation\n",
    "    max_spearman_feature = max(correlations, key=lambda x: correlations[x]['spearman'])\n",
    "    max_spearman_corr = correlations[max_spearman_feature]['spearman']\n",
    "\n",
    "    return (max_pearson_feature, max_pearson_corr), (max_spearman_feature, max_spearman_corr)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
