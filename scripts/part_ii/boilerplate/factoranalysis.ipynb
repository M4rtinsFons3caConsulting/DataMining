{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7819284c-d6ca-43cd-aeec-fde7fab11d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "class PolynomialFactorAnalysis:\n",
    "    \"\"\"\n",
    "    Polynomial Factor Analysis is an extension of Factor Analysis that takes into consideration feature polynomial to capture non-linearities in the latent space, similarly to how Linear Regression Models use polinomials to fit the regressors on the regressand.\n",
    "    \"\"\"\n",
    "    def __init__(self, degree=2, n_components=2, tol=0.001, max_iter=10000, \n",
    "                 noise_variance_init=None, svd_method='lapack', iterated_power=3, \n",
    "                 rotation='varimax', random_state=0, interaction_only=False, include_bias=True):\n",
    "        \"\"\"\n",
    "        Initialize Polynomial Factor Analysis (PFA).\n",
    "        Parameters:\n",
    "        degree (int): The degree of the polynomial features transformation. \n",
    "                      It controls the highest degree of interaction for the polynomial features.\n",
    "                      Default is 2 for quadratic features.\n",
    "\n",
    "        n_components (int): The number of components (latent factors) to extract using Factor Analysis. \n",
    "                             This corresponds to the number of latent factors in the model. \n",
    "                             Default is 2.\n",
    "\n",
    "        tol (float): The convergence tolerance for the Factor Analysis optimization algorithm. \n",
    "                     The algorithm will stop when the change in log-likelihood is less than this value. \n",
    "                     Default is 0.01.\n",
    "\n",
    "        max_iter (int): The maximum number of iterations to perform in the Factor Analysis optimization.\n",
    "                        Default is 1000.\n",
    "\n",
    "        noise_variance_init (array-like, optional): The initial guess for the noise variance for each feature.\n",
    "                                                    Default is None, which lets the model estimate it automatically.\n",
    "\n",
    "        svd_method (str): The method to use for Singular Value Decomposition in Factor Analysis. \n",
    "                          Options are 'randomized' or 'lapack'. Default is 'randomized'.\n",
    "\n",
    "        iterated_power (int): The number of iterations for the power method in the randomized SVD algorithm. \n",
    "                               Default is 3.\n",
    "\n",
    "        rotation (str or None): The rotation method to apply to the latent factors. \n",
    "                                 Options include 'varimax' or 'quartimax'. Default is None, meaning no rotation.\n",
    "\n",
    "        random_state (int or None): The seed for random number generation. Default is 0. \n",
    "                                    If None, the random seed will be chosen by the system.\n",
    "\n",
    "        interaction_only (bool): If True, only interaction features (no polynomial terms like x^2, x^3) \n",
    "                                  will be included in the transformation. Default is False.\n",
    "\n",
    "        include_bias (bool): If True, the transformed features will include a bias term (column of ones). \n",
    "                             Default is True.\n",
    "        Attributes:\n",
    "        poly (PolynomialFeatures): The polynomial feature transformer.\n",
    "        fa (FactorAnalysis): The Factor Analysis model.\n",
    "        \"\"\"\n",
    "        # Initialize PolynomialFeatures and FactorAnalysis\n",
    "        self.degree = degree\n",
    "        self.n_components = n_components\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.noise_variance_init = noise_variance_init\n",
    "        self.svd_method = svd_method\n",
    "        self.iterated_power = iterated_power\n",
    "        self.rotation = rotation\n",
    "        self.random_state = random_state\n",
    "        self.interaction_only = interaction_only\n",
    "        self.include_bias = include_bias\n",
    "\n",
    "        # Initialize PolynomialFeatures for polynomial expansion\n",
    "        self.poly = PolynomialFeatures(degree=self.degree,\n",
    "                                       interaction_only=self.interaction_only,\n",
    "                                       include_bias=self.include_bias)\n",
    "\n",
    "        # Initialize FactorAnalysis\n",
    "        self.fa = FactorAnalysis(n_components=self.n_components,\n",
    "                                 tol=self.tol,\n",
    "                                 max_iter=self.max_iter,\n",
    "                                 noise_variance_init=self.noise_variance_init,\n",
    "                                 svd_method=self.svd_method,\n",
    "                                 iterated_power=self.iterated_power,\n",
    "                                 rotation=self.rotation,\n",
    "                                 random_state=self.random_state)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the Polynomial Factor Analysis model on the input data X.\n",
    "\n",
    "        Parameters:\n",
    "        X (array-like): The input data, where each row represents an observation and each column a feature.\n",
    "\n",
    "        \"\"\"\n",
    "        # Apply polynomial feature transformation\n",
    "        X_poly = self.poly.fit_transform(X)\n",
    "\n",
    "        # Fit the Factor Analysis model to the polynomial transformed data\n",
    "        self.fa.fit(X_poly)\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the data X using the fitted Polynomial Factor Analysis model.\n",
    "\n",
    "        Parameters:\n",
    "        X (array-like): The input data to transform.\n",
    "\n",
    "        Returns:\n",
    "        Transformed data (latent factors)\n",
    "        \"\"\"\n",
    "        # Apply polynomial feature transformation\n",
    "        X_poly = self.poly.transform(X)\n",
    "\n",
    "        # Apply Factor Analysis to the polynomial transformed data\n",
    "        return self.fa.transform(X_poly)\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Fit and transform the data using the Polynomial Factor Analysis model.\n",
    "\n",
    "        Parameters:\n",
    "        X (array-like): The input data.\n",
    "\n",
    "        Returns:\n",
    "        Transformed data (latent factors)\n",
    "        \"\"\"\n",
    "        # Apply polynomial feature transformation\n",
    "        X_poly = self.poly.fit_transform(X)\n",
    "\n",
    "        # Apply Factor Analysis to the polynomial transformed data\n",
    "        return self.fa.fit_transform(X_poly)\n",
    "\n",
    "    def get_results(self, X):\n",
    "        \"\"\"\n",
    "        Return AIC, BIC, and explained variance as a tuple.\n",
    "        \n",
    "        Parameters:\n",
    "        X (array-like): The input data to calculate the results for.\n",
    "        \n",
    "        Returns:\n",
    "        tuple: (AIC, BIC, explained_variance)\n",
    "        \"\"\"\n",
    "        # Get the number of data points and features\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Log marginal likelihood\n",
    "        log_marginal_likelihood = self.fa.log_marginal_likelihood_\n",
    "\n",
    "        # Number of parameters: components (factor loadings) + noise variances\n",
    "        n_parameters = self.n_components * n_features + n_features\n",
    "\n",
    "        # Calculate AIC (Akaike's Information Criterion)\n",
    "        AIC = 2 * n_parameters - 2 * log_marginal_likelihood\n",
    "        \n",
    "        # Calculate BIC (Bayesian Information Criterion)\n",
    "        BIC = n_parameters * np.log(n_samples) - 2 * log_marginal_likelihood\n",
    "\n",
    "        # Get the explained variance of the latent factors\n",
    "        explained_variance = self.fa.explained_variance_\n",
    "\n",
    "        # Return AIC, BIC, and explained variance as a tuple\n",
    "        return (AIC, BIC, explained_variance, self.fa.components_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc9d17f-c1f4-4077-97c5-f1f2ea75662c",
   "metadata": {},
   "source": [
    "#### Searching for optima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0cf627-5e1b-41f2-b569-85191894ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "\n",
    "# Sample data for fitting the model (replace with your actual dataset)\n",
    "X = np.random.randn(100, 5)  # 100 samples, 5 features\n",
    "\n",
    "# Initialize a list to store results\n",
    "results_list = []\n",
    "\n",
    "# Define the ranges for degrees and number of components\n",
    "degree_range = [1, 2, 3]  # Example degrees\n",
    "n_components_range = [1, 2, 3]  # Example components\n",
    "\n",
    "# Iterate through all combinations of degree and n_components\n",
    "for degree, n_components in product(degree_range, n_components_range):\n",
    "    # Initialize and fit the PolynomialFactorAnalysis model\n",
    "    pfa = PolynomialFactorAnalysis(degree=degree, n_components=n_components)\n",
    "    pfa.fit(X)\n",
    "    \n",
    "    # Get results (AIC, BIC, and explained variance)\n",
    "    AIC, BIC, explained_variance = pfa.get_results(X)\n",
    "    \n",
    "    # Store results as a tuple (degree, n_components, AIC, BIC, explained_variance)\n",
    "    results_list.append((degree, n_components, AIC, BIC, explained_variance))\n",
    "\n",
    "# Convert the results list into a DataFrame\n",
    "results_df = pd.DataFrame(results_list, columns=[\"degree\", \"n_components\", \"AIC\", \"BIC\", \"explained_variance\"])\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cff2c88-2dbd-4794-b7fe-596976d74f7b",
   "metadata": {},
   "source": [
    "#### Scree Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c05b18-c7c2-4519-b1ae-7de71718f35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting the scree plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.lineplot(x='n_components', y='explained_variance', data=results_df, marker='o', color='blue')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Scree Plot\", fontsize=16)\n",
    "plt.xlabel(\"Number of Components\", fontsize=14)\n",
    "plt.ylabel(\"Explained Variance\", fontsize=14)\n",
    "plt.xticks(df['n_components'])  # Ensure each component is labeled\n",
    "plt.grid(True)\n",
    "\n",
    "# Optionally: Mark the \"elbow\" if you want\n",
    "# For example, you could highlight the \"elbow\" based on visual inspection or a rule (like variance > 0.1)\n",
    "elbow_point = df[df['explained_variance'] > 0.1].iloc[-1]  # Example rule to mark elbow\n",
    "plt.scatter(elbow_point['n_components'], elbow_point['explained_variance'], color='red', zorder=5, label=\"Elbow Point\")\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ecf016-3c72-4d54-bed4-cd3d0689a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the PolynomialFactorAnalysis model (with desired degree and components)\n",
    "pfa_model = PolynomialFactorAnalysis(degree=2, n_components=3)\n",
    "\n",
    "# Fit the PFA model on the training data\n",
    "pfa_model.fit(X_train)\n",
    "\n",
    "# Make a copy of the test data (ground truth)\n",
    "X_test_original = X_test.copy()\n",
    "\n",
    "# Transform the test data using the fitted model\n",
    "X_test_transformed = pfa_model.transform(X_test)\n",
    "\n",
    "# Compare the transformed test data to the original test data\n",
    "# Calculate Mean Squared Error (MSE) to assess how far the transformation has deviated\n",
    "mse = mean_squared_error(X_test_original, X_test_transformed)\n",
    "\n",
    "# Optionally, you can calculate other metrics like Euclidean distance, Cosine similarity, etc.\n",
    "# For Euclidean distance, you can use numpy:\n",
    "euclidean_distance = np.linalg.norm(X_test_original - X_test_transformed)\n",
    "\n",
    "# If you want to use Cosine similarity, you can calculate that as well:\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_sim = cosine_similarity(X_test_original, X_test_transformed)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Euclidean Distance: {euclidean_distance}\")\n",
    "print(f\"Cosine Similarity: {cosine_sim}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
