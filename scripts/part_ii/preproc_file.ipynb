{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports, Options and Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports list\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style is importantuntitled:Untitled-1.ipynb?jupyter-notebook\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Ensuring pandas always prints all columns and rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_seq_items', None)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/shadybea/OneDrive/General/Data Mining/Project/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the csv data\n",
    "data = pd.read_csv(f'{path}DM2425_ABCDEats_DATASET.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "_rename_dict = {\n",
    "        'customer_region' : 'cust_region'\n",
    "        , 'payment_method' : 'pay_method'\n",
    "        , 'customer_age' : 'cust_age'\n",
    "        , 'vendor_count' : 'n_vendor'\n",
    "        , 'product_count' : 'n_product'\n",
    "        , 'n_order' : 'n_order'\n",
    "        , 'is_chain' : 'n_chain'\n",
    "        , 'CUI_American' : 'american'\n",
    "        , 'CUI_Asian' : 'asian'\n",
    "        , 'CUI_Beverages' : 'beverages'\n",
    "        , 'CUI_Cafe' : 'cafe'\n",
    "        , 'CUI_Chicken Dishes' : 'chicken_dishes'\n",
    "        , 'CUI_Chinese' : 'chinese'\n",
    "        , 'CUI_Desserts' : 'desserts'\n",
    "        , 'CUI_Healthy' : 'healthy'\n",
    "        , 'CUI_Indian' : 'indian'\n",
    "        , 'CUI_Italian' : 'italian'\n",
    "        , 'CUI_Japanese' : 'japanese'\n",
    "        , 'CUI_Noodle Dishes' : 'noodle_dishes'\n",
    "        , 'CUI_OTHER' : 'other'\n",
    "        , 'CUI_Street Food / Snacks' : 'street_food_snacks'\n",
    "        , 'CUI_Thai' : 'thai'\n",
    "}\n",
    "\n",
    "# Rename the columns for easier reference\n",
    "data.rename(columns=_rename_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the integer value of the customer hex values, the index. \n",
    "data['customer_id'] = data['customer_id'].apply(lambda x: int(x, 16))\n",
    "\n",
    "# Set 'customer_id' as the index\n",
    "data = data[~data['customer_id'].duplicated()].set_index('customer_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values for first_order\n",
    "data.loc[data['first_order'].isna(), 'first_order'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values for HR_0\n",
    "sum_week = data[[f\"DOW_{n}\" for n in range(7)]].sum(axis=1)\n",
    "sum_day = data[[f\"HR_{n}\" for n in range(24)]].sum(axis=1)\n",
    "\n",
    "data.loc[data['HR_0'].isna(), 'HR_0'] = (sum_week - sum_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Inconsistencies\n",
    "\n",
    "### 2.2.1. Duplicate Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find 94 such rows, regarding 47 entries; and simply drop the repeats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Inconsistent Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# has at least one vendor\n",
    "has_vendor = data['n_vendor'] != 0 \n",
    "\n",
    "# has at least one product\n",
    "has_product = data['n_product'] != 0 \n",
    "\n",
    "# purchase must have been made on a valid dow\n",
    "some_day = (data[[f\"DOW_{n}\" for n in range(7)]] != 0).any(axis = 1) \n",
    "\n",
    "# purchase must have been made at a valid hour\n",
    "some_hour = (data[[f\"HR_{n}\" for n in range(24)]] != 0).any(axis = 1)  \n",
    "\n",
    "# some type of cuisine must have been ordered\n",
    "some_food = (data[data.columns[9:24]] != 0).any(axis = 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(has_vendor & has_product & some_day & some_hour & some_food)]  # And we drop these values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. Weird Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer Region\n",
    "data.loc[data['cust_region'] == '-', 'cust_region'] = '8670'\n",
    "data.loc[data['cust_region'].isin(['2440', '2490']), 'cust_region'] = '2400'\n",
    "\n",
    "# Add the feature Customer CIty\n",
    "data['cust_city'] = data['cust_region'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last Promo\n",
    "data.loc[data['last_promo'] == '-', 'last_promo'] = 'NO_PROMO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidying up datatypes\n",
    "for col in data.iloc[:, 0:9]:\n",
    "    if col in ['last_promo', 'pay_method']:\n",
    "        data[col] = data[col].astype(object)\n",
    "    else:\n",
    "        data[col] = data[col].astype('Int64')\n",
    "\n",
    "for col in data.iloc[:, 9:24]:\n",
    "    data[col] = data[col].astype(float)\n",
    "\n",
    "for col in data.iloc[:, 24:]:\n",
    "    data[col] = data[col].astype('Int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "We create lists of features for easy reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables\n",
    "non_metric_features = ['cust_region', 'last_promo', 'pay_method', 'cust_city']\n",
    "\n",
    "# Hour of day variables\n",
    "hour_features = data.columns[31:55].to_list()\n",
    "\n",
    "# Day of week variables\n",
    "day_features = data.columns[24:31].to_list()\n",
    "\n",
    "# Cuisine features\n",
    "cuisine_features = data.columns[9:24].to_list()\n",
    "\n",
    "# Metric variables, that are not above\n",
    "metric_features = data.columns.drop(non_metric_features).drop(hour_features).drop(day_features).drop(cuisine_features).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total amount spent by customer on all types of cuisine\n",
    "data['total_amt'] = data[cuisine_features].sum(axis=1)\n",
    "\n",
    "# Number of orders made by the customer\n",
    "data['n_order'] = data[day_features].sum(axis=1)\n",
    "\n",
    "# Amount spent on average per product\n",
    "data['avg_amt_per_product'] = data['total_amt'] / data['n_product']\n",
    "\n",
    "# Amount spent on average per order\n",
    "data['avg_amt_per_order'] = data['total_amt'] / data['n_order']\n",
    "\n",
    "# Amount spent on average per vendor\n",
    "data['avg_amt_per_vendor'] = data['total_amt'] / data['n_vendor']\n",
    "\n",
    "# Total days as customer\n",
    "data['days_cust'] = data['last_order'] - data['first_order']\n",
    "\n",
    "# Average days between orders\n",
    "data['avg_days_to_order'] = data['days_cust'] / data['n_order']\n",
    "\n",
    "# Days the customer is due, according to their average days between orders\n",
    "data['days_due'] = 90 - data['last_order'] + data['avg_days_to_order']\n",
    "\n",
    "# Percentage of orders placed to restaurants that are part of a chain\n",
    "data['per_chain_order'] = data['n_chain'] / data['n_order']\n",
    "\n",
    "# And we add these tese features to the metric features list.\n",
    "metric_features.extend(\n",
    "    [\n",
    "        'n_order'\n",
    "        , 'per_chain_order'\n",
    "        ,'total_amt'\n",
    "        , 'avg_amt_per_order'\n",
    "        , 'avg_amt_per_product'\n",
    "        , 'avg_amt_per_vendor'\n",
    "        , 'days_cust'\n",
    "        , 'avg_days_to_order'\n",
    "        , 'days_due'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask to check if each day column is populated\n",
    "mask = data[[f'DOW_{i}' for i in range(7)]] > 0\n",
    "\n",
    "# Sum over the mask to get the count of days with purchases for each row\n",
    "data.loc[:, 'n_days_week'] = mask.sum(axis=1)\n",
    "\n",
    "# Updating the list of metric features\n",
    "metric_features.append('n_days_week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask to check if each hour column is populated\n",
    "mask = data[hour_features] > 0\n",
    "\n",
    "# Sum over the mask to get the count of hours with purchases for each row\n",
    "data.loc[:, 'n_times_day'] = mask.sum(axis=1)\n",
    "\n",
    "# Updating the list of metric features\n",
    "metric_features.append('n_times_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag customers who have purchased in more than one day\n",
    "data['regular'] = (data['days_cust'] > 1)\n",
    "\n",
    "non_metric_features.append('regular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask where values are greater than zero (indicating an order)\n",
    "mask = data[cuisine_features] > 0\n",
    "\n",
    "# # Use mask to get the ordered cuisines for each row\n",
    "# data.loc[:, 'ordered_cuisines'] = mask.apply(lambda row: [cuisine for cuisine, ordered in row.items() if ordered], axis=1)\n",
    "\n",
    "# # Updating the non_metric_features_list\n",
    "# non_metric_features.append('ordered_cuisines')\n",
    "\n",
    "# Use mask to get the number of cuisines for each row\n",
    "data.loc[:, 'n_cuisines'] = mask.sum(axis=1)\n",
    "\n",
    "# Updating the metric_features_list\n",
    "metric_features.append('n_cuisines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping specified columns and getting remaining columns as a list\n",
    "targets = data.drop(columns=[\n",
    "    'cust_age'\n",
    "    , 'first_order'\n",
    "    , 'last_order'\n",
    "    , 'days_cust'\n",
    "    , 'days_due'\n",
    "    , 'avg_days_to_order'\n",
    "    , 'per_chain_order'\n",
    "    , 'cust_region'\n",
    "    , 'cust_city'\n",
    "    , 'last_promo'\n",
    "    , 'pay_method'\n",
    "    # , 'ordered_cuisines'\n",
    "    , 'n_cuisines'\n",
    "    , 'regular'\n",
    "] + hour_features + day_features).columns.tolist()\n",
    "\n",
    "# Initialize an empty DataFrame to store log-transformed columns\n",
    "log_transformed = pd.DataFrame()\n",
    "\n",
    "# Apply log1p to each column in targets and add it to log_transformed with the prefix 'log_'\n",
    "for col in targets:\n",
    "    log_transformed[f\"log_{col}\"] = np.log1p(data[col])\n",
    "\n",
    "# We create a list of log_features to assist us in our exploration\n",
    "log_features = log_transformed.columns.tolist()\n",
    "\n",
    "# Concatenate the original DataFrame with the new log-transformed DataFrame\n",
    "data = pd.concat([data, log_transformed], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of customers flagged as foodie: 2852\n",
      "Number of customers flagged as gluttonous: 645\n",
      "Number of customers flagged as loyal: 454\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionaries for feature groups with flags and relevant columns\n",
    "feature_groups = {\n",
    "    'foodie': ['n_vendor', 'n_product', 'n_order', 'n_cuisines'],\n",
    "    'gluttonous': ['avg_amt_per_order', 'total_amt', 'n_chain'],\n",
    "    'loyal': ['avg_amt_per_vendor'] + cuisine_features\n",
    "}\n",
    "\n",
    "\n",
    "# Create columns to hold the flags for each feature group\n",
    "data['foodie_flag'] = 0\n",
    "data['gluttonous_flag'] = 0\n",
    "data['loyal_flag'] = 0\n",
    "\n",
    "# Function to calculate IQR bounds\n",
    "def calculate_bounds(series):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# Assign flags for each feature group\n",
    "for group, features in feature_groups.items():\n",
    "    for feature in features:\n",
    "        log_feature = f\"log_{feature}\"\n",
    "        \n",
    "        if feature == 'n_cuisines':\n",
    "            log_feature = feature\n",
    "        \n",
    "        lower_bound, upper_bound = calculate_bounds(data.loc[(data['regular'] == 1) & (data[feature] > 0), log_feature])\n",
    "        \n",
    "        # Mark outliers for each group\n",
    "        if group == 'foodie':\n",
    "            data.loc[data['regular'] == 1, 'foodie_flag'] |= (\n",
    "                data.loc[data['regular'] == 1, log_feature] > upper_bound\n",
    "            ).astype(int)\n",
    "        elif group == 'gluttonous':\n",
    "            data.loc[data['regular'] == 1, 'gluttonous_flag'] |= (\n",
    "                data.loc[data['regular'] == 1, log_feature] > upper_bound\n",
    "            ).astype(int)\n",
    "        elif group == 'loyal':\n",
    "            data.loc[data['regular'] == 1, 'loyal_flag'] |= (\n",
    "                data.loc[data['regular'] == 1, log_feature] > upper_bound\n",
    "            ).astype(int)\n",
    "\n",
    "# Display results\n",
    "for group in ['foodie_flag', 'gluttonous_flag', 'loyal_flag']:\n",
    "    print(f\"Number of customers flagged as {group.split('_')[0]}:\", data[group].sum())\n",
    "\n",
    "non_metric_features.extend([\n",
    "    'foodie_flag'\n",
    "    ,'gluttonous_flag'\n",
    "    ,'loyal_flag'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n(row, col_list, n):\n",
    "    # Sort the specified columns in descending order\n",
    "    sorted_row = row[col_list].sort_values(ascending=False)\n",
    "\n",
    "    # Get the unique sorted values\n",
    "    unique_sorted_values = sorted_row.unique()\n",
    "\n",
    "    # Ensure there are enough unique values to determine the n-th largest\n",
    "    if len(unique_sorted_values) >= n:\n",
    "        nth_value = unique_sorted_values[n - 1]  # Get the n-th largest unique value\n",
    "\n",
    "        # If the n-th value is 0, return None\n",
    "        if nth_value == 0:\n",
    "            return None\n",
    "        \n",
    "        # If n > 1, check for uniqueness against the (n-1)-th largest\n",
    "        if n > 1:\n",
    "            prev_value = unique_sorted_values[n - 2]  # (n-1)-th largest unique value\n",
    "            # If nth_value is equal to the (n-1)-th value, we don't want to return it\n",
    "            if nth_value == prev_value:\n",
    "                return None\n",
    "        \n",
    "        # Return the index of the n-th largest value\n",
    "        return sorted_row[sorted_row == nth_value].index[0]\n",
    "\n",
    "    # Return None if conditions are not met\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['top_cuisine'] = data.apply(top_n, col_list=cuisine_features, n=1, axis=1)\n",
    "\n",
    "non_metric_features.append('top_cuisine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average amount spent per day as customer\n",
    "data['avg_amt_per_day'] = np.round(data['total_amt'] / data['days_cust'], 4)\n",
    "\n",
    "# Average number of products ordered per day as customer\n",
    "data['avg_product_per_day'] = np.round(data['n_product'] / data['days_cust'], 4)\n",
    "\n",
    "# Average number of orders per day as customer\n",
    "data['avg_order_per_day'] = np.round(data['n_order'] / data['days_cust'], 4)\n",
    "\n",
    "metric_features.extend([\n",
    "    'avg_amt_per_day'\n",
    "    ,'avg_product_per_day'\n",
    "    ,'avg_order_per_day'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the preprocessing efforts, there are some customers that can be perceived as outliers, namely the ones that were flagged as part of a group or non-regulars. Therefore, we create an auxiliary dataframe excluding these, which we will use in a fashion resembling a train-test split, where these are the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with only customers which are regular and do not belong to another group\n",
    "aux_df = data[data['regular'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Outliers\n",
    "\n",
    "Based on the boxplots of the EDA part of the project, we can filter our dataset using the following conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_rows = data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_percentile_bounds(reference_data, filters):\n",
    "    \"\"\"\n",
    "    Compute percentile bounds for the given filters using the reference dataset.\n",
    "\n",
    "    :param reference_data: DataFrame used to compute the percentiles.\n",
    "    :param filters: Dictionary of column names and (lower_percentile, upper_percentile).\n",
    "    :return: Dictionary with column names and the corresponding (lower_bound, upper_bound).\n",
    "    \"\"\"\n",
    "    bounds = {}\n",
    "    \n",
    "    for col, percentiles in filters.items():\n",
    "        if col not in reference_data.columns:\n",
    "            raise ValueError(f\"Column '{col}' not found in the reference DataFrame.\")\n",
    "\n",
    "        lower_percentile, upper_percentile = percentiles\n",
    "        lower_bound = reference_da.ta[col].quantile(lower_percentile) if lower_percentile is not None else None\n",
    "        upper_bound = reference_data[col].quantile(upper_percentile) if upper_percentile is not None else None\n",
    "        bounds[col] = (lower_bound, upper_bound)\n",
    "\n",
    "    return bounds\n",
    "\n",
    "def filter_by_bounds(data, bounds):\n",
    "    \"\"\"\n",
    "    Filter the data using precomputed bounds.\n",
    "\n",
    "    :param data: DataFrame to be filtered.\n",
    "    :param bounds: Dictionary with column names and the corresponding (lower_bound, upper_bound).\n",
    "    :return: Filtered DataFrame.\n",
    "    \"\"\"\n",
    "    conditions = []\n",
    "\n",
    "    for col, (lower_bound, upper_bound) in bounds.items():\n",
    "        if col not in data.columns:\n",
    "            raise ValueError(f\"Column '{col}' not found in the DataFrame.\")\n",
    "\n",
    "        if lower_bound is not None:\n",
    "            conditions.append(data[col] >= lower_bound)\n",
    "        if upper_bound is not None:\n",
    "            conditions.append(data[col] <= upper_bound)\n",
    "\n",
    "    if conditions:\n",
    "        combined_condition = conditions[0]\n",
    "        for condition in conditions[1:]:\n",
    "            combined_condition &= condition\n",
    "        return data[combined_condition]\n",
    "\n",
    "    return data\n",
    "\n",
    "# Example Usage:\n",
    "# Compute bounds using dataset a\n",
    "defined_filters = {\n",
    "    **{col: (None, 0.999) for col in aux_df.columns if 'log' in col}\n",
    "}\n",
    "percentile_bounds = compute_percentile_bounds(aux_df, defined_filters)\n",
    "\n",
    "# Filter dataset b using the computed bounds\n",
    "data = filter_by_bounds(data, percentile_bounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of data kept after removing outliers: 94.99\n"
     ]
    }
   ],
   "source": [
    "print('Percentage of data kept after removing outliers:', 100 * np.round(data.shape[0] / og_rows, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us recheck the boxplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate dataframe with only customers which are regular and do not belong to another group\n",
    "aux_df = data[data['regular'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are missing values for 'cust_age', which will be imputed using a strategy to be determined next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['cust_age'].isna(), 'cust_age'] = aux_df['cust_age'].mean().astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Age Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating age buckets\n",
    "data['age_bucket'] = np.where(\n",
    "    data['cust_age'] < 25, '15-24', np.where(\n",
    "        data['cust_age'] < 35, '25-34', np.where(\n",
    "            data['cust_age'] < 45, '35-44', np.where(\n",
    "                data['cust_age'] < 55, '45-54', np.where(\n",
    "                    data['cust_age'] < 65, '55-64', '65+'\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "non_metric_features.insert(4, 'age_bucket')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the auxiliary dataframe\n",
    "aux_df = data[data['regular'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse_output=False).set_output(transform='pandas')\n",
    "ohe_df = ohe.fit_transform(data[non_metric_features[:5]])\n",
    "\n",
    "ohe_feat_names = ohe.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data.drop(columns=non_metric_features[:5]), ohe_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "regulars = data[data['regular'] == 1].copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Data Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler().set_output(transform='pandas')\n",
    "regulars = pd.concat([\n",
    "    scaler.fit_transform(regulars.drop(columns=list(ohe_feat_names) + non_metric_features[5:]))\n",
    "    ,regulars[list(ohe_feat_names) + non_metric_features[5:]]\n",
    "], axis=1)\n",
    "\n",
    "# Save the scaler to a file\n",
    "joblib.dump(scaler, 'std_scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Multidimensional Outliers\n",
    "\n",
    "Now, Local Outlier Factor can be applied to remove the multidimensional outliers, since the data is scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "regulars_og = regulars.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_std = LocalOutlierFactor(n_neighbors=5)\n",
    "\n",
    "outlier_predictions = lof_std.fit_predict(regulars.drop(columns=list(ohe_feat_names) + non_metric_features[5:]))\n",
    "\n",
    "# Mark outliers\n",
    "regulars['Outlier'] = np.where(outlier_predictions == -1, True, False)\n",
    "\n",
    "# Remove outliers\n",
    "regulars = regulars[regulars['Outlier'] == False].drop(columns=['Outlier'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform PCA with the variables avg_amt_per_day, avg_product_per_day, avg_order_per_day, n_product and n_order - in order to reduce dimensionality and assess if the resulting principal components are of any help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(random_state=20)\n",
    "pca_feat = pca.fit_transform(regulars[['avg_amt_per_day', 'avg_product_per_day', 'avg_order_per_day', 'n_product', 'n_order']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eigenvalue</th>\n",
       "      <th>Difference</th>\n",
       "      <th>Proportion</th>\n",
       "      <th>Cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.587732</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.554484</td>\n",
       "      <td>0.554484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.782654</td>\n",
       "      <td>-0.805078</td>\n",
       "      <td>0.381977</td>\n",
       "      <td>0.936461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.234387</td>\n",
       "      <td>-1.548267</td>\n",
       "      <td>0.050223</td>\n",
       "      <td>0.986684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.043748</td>\n",
       "      <td>-0.190639</td>\n",
       "      <td>0.009374</td>\n",
       "      <td>0.996058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.018395</td>\n",
       "      <td>-0.025352</td>\n",
       "      <td>0.003942</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Eigenvalue  Difference  Proportion  Cumulative\n",
       "1    2.587732    0.000000    0.554484    0.554484\n",
       "2    1.782654   -0.805078    0.381977    0.936461\n",
       "3    0.234387   -1.548267    0.050223    0.986684\n",
       "4    0.043748   -0.190639    0.009374    0.996058\n",
       "5    0.018395   -0.025352    0.003942    1.000000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the eigenvalues (explained variance)\n",
    "explained_variance = pca.explained_variance_\n",
    "\n",
    "# Get the explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# get the cumulative explained variance ratio \n",
    "cumulative_explained_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Combine into a dataframe\n",
    "pca_results = pd.DataFrame(\n",
    "    {\n",
    "        \"Eigenvalue\": explained_variance,\n",
    "        \"Difference\": np.insert(np.diff(explained_variance), 0, 0),\n",
    "        \"Proportion\": explained_variance_ratio,\n",
    "        \"Cumulative\": cumulative_explained_variance_ratio\n",
    "    },\n",
    "        index=range(1, pca.n_components_ + 1)\n",
    ")\n",
    "\n",
    "pca_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA again with the number of principal components you want to retain\n",
    "pca = PCA(n_components=3, random_state=20)\n",
    "pca_feat = pca.fit_transform(regulars[['avg_amt_per_day', 'avg_product_per_day', 'avg_order_per_day', 'n_product', 'n_order']])\n",
    "pca_feat_names = [f\"PC{i}\" for i in range(pca.n_components_)]\n",
    "\n",
    "# remember index=df_pca.index\n",
    "pca_df = pd.DataFrame(pca_feat, index=regulars.index, columns=pca_feat_names)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassigning df to contain pca variables\n",
    "df_pca = pd.concat([regulars, pca_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = df_pca[['avg_amt_per_day', 'avg_product_per_day', 'avg_order_per_day', 'n_product', 'n_order'] + pca_feat_names].corr().loc[['avg_amt_per_day', 'avg_product_per_day', 'avg_order_per_day', 'n_product', 'n_order'], pca_feat_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca.drop(columns='PC2', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca.rename(columns={'PC0': 'transaction_volume', 'PC1': 'interaction_rate'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "regulars = df_pca.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(f'{path}all_customers_preproc.csv', index=True)\n",
    "regulars.to_csv(f'{path}regulars_preproc.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
